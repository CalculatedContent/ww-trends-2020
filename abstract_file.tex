
In many applications,
% of machine learning and artificial intelligence, 
one must work with models that have been trained by someone else.
For such \emph{pretrained models}, one typically does not have access to training data or test data, and one does not know the details of how the models were built, the specifics of the data that were used to train the model, what was the loss function or hyperparameter values, how precisely the model was regularized, etc.
Here, we develop and evaluate quality metrics for pretrained neural network models at scale.
%%Our metrics are drawn from traditional statistical learning theory (e.g., norm-based capacity control metrics) as well as Heavy-Tailed Random Matrix Theory (HT-RMT), as used in the recently-developed Theory of Heavy-Tailed Self Regularization (e.g., fitted power law metrics used to characterize the degree of strong correlations in trained models).
Most promising are metrics drawn from traditional statistical learning theory (e.g., norm-based capacity control metrics) as well as metrics (e.g., fitted power law metrics used to characterize the degree of strong correlations in trained models) derived from the recently-developed Theory of Heavy-Tailed Self Regularization (HT-SR).
Using the publicly-available \emph{WeightWatcher} tool, we analyze hundreds of publicly-available pretrained models, including older and current state-of-the-art models in computer vision and natural language processing.
We find that norm-based metrics do a reasonably good job at predicting quality trends in well-trained models, i.e., they can be used to discriminate between ``good-better-best,'' but that HT-SR metrics do much better.
We also find that, for models that are not well-trained (which, arguably is the point of needing metrics to evaluate the quality of pretrained models), norm-based metrics can qualitatively fail, while HT-SR metrics still go a very good job at predicting trends in model quality.
HT-SR metrics can also be used to characterize fine-scale properties of models, such as layer-wise \emph{correlation flow} and XXX SOMETHING ELSE.

