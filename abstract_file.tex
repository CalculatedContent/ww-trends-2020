
In many practical AI applications, one works with deep neural network (DNN) models trained by someone else.
For such \emph{pretrained models}, one typically does not have access to training data or test data, and, moreover,  does not know the details of how the models were built, the specifics of the data that were used to train the model, what was the loss function or hyperparameter values, how precisely the model was regularized, etc.
Here, we present and evaluate quality metrics for pretrained neural network models at scale.
%%Our metrics are drawn from traditional statistical learning theory (e.g., norm-based capacity control metrics) as well as Heavy-Tailed Random Matrix Theory (HT-RMT), as used in the recently-developed Theory of Heavy-Tailed Self Regularization (e.g., fitted power law metrics used to characterize the degree of strong correlations in trained models).
%%The most promising are metrics drawn from traditional statistical learning theory (e.g., norm-based capacity control metrics) as well as metrics (e.g., fitted power law metrics used to characterize the degree of strong correlations in a system) derived from the recently-developed Theory of Heavy-Tailed Self Regularization (HT-SR).
The most promising are norm-based metrics (such as those used to provide capacity control in traditional statistical learning theory) and metrics based on fitting eigenvalue distributions to truncated power law (PL) distributions (which derive from statistical mechanics, in particular the recently-developed Theory of Heavy-Tailed Self Regularization, and characterize the strength of correlations in a DNN). 
%
Using the publicly-available \emph{WeightWatcher} tool, we analyze hundreds of publicly-available pretrained models, including older and current state-of-the-art models in computer vision (ZCV) and natural language processing (NLP)
We find that norm-based metrics do a reasonably good job at predicting quality trends in well-trained models, i.e., they can discriminate amoing ``good-better-best''  models.
On the other hand, for poorly trained models--which, arguably why we need quality metrics-- we want to distinguish which and when norm-based metrics can distiungush ''good-vs-bad'' models.
We also find that PL-based metrics do much better, quantitatively better, at discriminating aming ''good-better-best'' models, and qualitatively better at discriminating ''good-vs-bad'' models.
PL-based metrics can also be used to characterize fine-scale properties of models, e.g., understanding layer-wise \emph{correlation flow}, and evaluate post-training modifications such as model distillation, increasing the data set size, and other model improvements.

