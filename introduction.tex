
\section{Introduction}
\label{sxn:intro}

A common problem in machine learning (ML) 
%and artificial intelligence (AI) 
is to evaluate the quality of a given model.
A popular way to accomplish this
%, in particular in academic environments, 
is to train a model and then evaluate its training and/or testing error.
There are many problems with this approach.
Well-known problems with just examining training/testing curves include that 
they give very limited insight into the overall properties of the model, 
they do not take into account the (often extremely large human and CPU/GPU) time for hyperparameter fiddling,
they typically do not correlate with other properties of interest such as robustness or fairness or interpretability, 
%XXX SOMETHING ELSE, 
and so on.
A somewhat less well-known problem, but one that is increasingly important (in particular in industrial-scale ML---where the \emph{users} of models are not the \emph{developers} of the models) is that one may access to neither the training data nor the testing data.
Instead, one may simply be given a model that has already been trained---we will call such an already-trained model a \emph{pre-trained model}---and be told to use it.

Na\"{\i}vely---but in our experience commonly, among both ML practitioners and ML theorists---if one does not have access to training or testing data, then one can say absolutely nothing about the quality of a ML model.
This may be true in worst-case theory, but ML models are used in practice, and there is a need for theory to guide that practice.
Moreover, if ML is to become an industrial process, then the process will become siloed: some groups will gather data, other groups will develop models, and still other groups will use those models.
The users of models can not be expected to know the precise details of how the models were built, the specifics of the data that were used to train the model, what the loss of values of the hyperparameters were, how precisely the model was regularized, etc.
%
Having metrics to evaluate the quality of a ML model in the absence of training and testing data and without any detailed knowledge of the training and testing process---indeed, having theory for pre-trained models, to predict how, when, and why such models can be expected to perform well or poorly---is clearly of interest.

In this paper, we present and apply techniques to evaluate the generalization properties of large-scale state-of-the-art pre-trained neural network (NN) models.%
\footnote{We reiterate: One could use these techniques to improve training, and we have been asked about that, but we are not interested in that here. Our main goal here is to use these techniques to evaluate properties of state-of-the-art pre-trained NN models.}
To do so, we consider a large suite of publicly-available models from computer vision (CV) and natural language processing (NLP).
%
By now, there are many such state-of-the-art models that are publicly-available, e.g., 
there are now hundreds of pre-trained models in CV ($\ge 500$) and NLP ($\approx 100$).%
\footnote{When we began this work in 2018, there were fewer than tens of such models; now in 2020, there are hundreds of such models; and we expect that in a year or two there will be an order of magnitude or more of such models.}
These provide a large corpus of models that by some community standard are state-of-the-art.%
\footnote{Clearly, there is a selection bias or survivorship bias here---people tend not to make publicly-available their poorly-performing models---but these models are things in the world that (like social networks or the internet) can be analyzed for their properties.}
XXX.  MORE DETAILS.
Importantly, for all of these models, we have no access to training data or testing data.

XXX.  LIST PLACES WHERE THEY ARE AVAIALBLE, HERE OR IN NEXT SECTION.

In more detail, our main contributions are the following.
\begin{itemize}
\item XXX TECHNICAL THING 1
\item XXX TECHNICAL THING 2
\item XXX TECHNICAL THING 3
\end{itemize}
Perhaps our most improtant contribution, however, is just doing things different, etc. ...

