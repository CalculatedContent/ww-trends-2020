
\section{Introduction}
\label{sxn:intro}

A common problem in machine learning (ML) 
%and artificial intelligence (AI) 
is to evaluate the quality of a given model.
A popular way to accomplish this
%, in particular in academic environments, 
is to train a model and then evaluate its training and/or testing error.
There are many problems with this approach.
Well-known problems with just examining training/testing curves include that 
they give very limited insight into the overall properties of the model, 
they do not take into account the (often extremely large human and CPU/GPU) time for hyperparameter fiddling,
they typically do not correlate with other properties of interest such as robustness or fairness or interpretability, 
and so on.
A somewhat less well-known problem, but one that is increasingly important (in particular in industrial-scale ML---where the \emph{users} of models are not the \emph{developers} of the models) is that one may have access to neither the training data nor the testing data.
Instead, one may simply be given a model that has already been trained---a \emph{pretrained model}---and be told to use it.

Na\"{\i}vely---but in our experience commonly, among both ML practitioners and ML theorists---if one does not have access to training or testing data, then one can say absolutely nothing about the quality of a ML model.
This may be true in worst-case theory, but ML models are used in practice, and there is a need for a \emph{practical theory} to guide that practice.
Moreover, if ML is to become an industrial process, then that process will become siloed: some groups will gather data, other groups will develop models, and still other groups will use those models.
The users of models can not be expected to know the precise details of how the models were built, the specifics of the data that were used to train the model, what was the loss function or hyperparameter values, how precisely the model was regularized, etc.
%
Moreover, in industry, one faces unique practical problems such as: do we have enough data for this model? 
Methods that are developed and evaluated on any well-defined publicly-available coprus of data, no matter how large or diverse or interesting, are clearly not going to be well-suited to address problems such as this.
It is thus of great practical interest to have metrics to evaluate the quality of a ML model in the absence of training and testing data and without any detailed knowledge of the training and testing process.  
We seek a practical theory for pretrained models which can predict how, when, and why such models can be expected to perform well or poorly.

In this paper, we present and evaluate quality metrics for pretrained neural network (NN) and deep neural network (DNN) models at scale.
To do so, we consider a large suite of hundreds of publicly-available models, mostly from computer vision (CV) and natural language processing (NLP).
%
By now, there are many such state-of-the-art models that are publicly-available, e.g., 
there are now hundreds of pretrained models in CV ($\ge 500$) and NLP ($\approx 100$).%
\footnote{When we began this work in 2018, there were fewer than tens of such models; now in 2020, there are hundreds of such models; and we expect that in a year or two there will be an order of magnitude or more of such models.}
These provide a large corpus of models that by some community standard are state-of-the-art.%
\footnote{Clearly, there is a selection bias or survivorship bias here---people tend not to make publicly-available their poorly-performing models---but these models are things in the world that (like social networks or the internet) can be analyzed for their properties.}
They include 
\michael{XXX, XXX and XXX.  MORE DETAILS.}
\michael{XXX.  LIST PLACES WHERE THEY ARE AVAIALBLE, HERE OR IN NEXT SECTION.}
Importantly, all of these models have been trained by someone else and have been viewed to be of sufficient interest/quality to be made publicly-available; and, for all of these models, we have no access to training data or testing data, and we have no knowledge of the training/testing protocols. 

The \emph{quality metrics} we consider are based on the spectral properties of the layer weight matrics.
In particular, they are based on the norms of weight matrices (such norms have been used in traditional statistical learning theory to bound capacity and construct regularizers) and parameters of power law (PL) fits of the eigenvalues of weight matrices (such PL fits are based on statistical mechanics approaches to NNs).
Note that, while we use traditional norm-based and PL-based metrics, our goals are different.
In particular, unlike more common ML approaches, \emph{we do not seek a bound on the generalization} (e.g., by evaluating training/test error during training), \emph{we do not seek a new regularizer}, and \emph{we do not aim to evaluate a single model} (e.g., hyperparmeter optimization).% 
\footnote{One could of course use these techniques to improve training, and we have been asked about that, but we are not interested in that here. Our main goal here is to use these techniques to evaluate properties of state-of-the-art pretrained NN models.}
Instead, we want to examine different models across common architecture series, and we want to compare models between different architectures themselves, and in both cases \emph{we aim to predict trends in the quality of pre-trained models without access to training or testing data}.  


In more detail, our main contributions are the following.
%\begin{itemize}
%\item 
First, 
motivated by practical problems, we formulate the question ``Can one predict trends in the quality of state-of-the-art neural networks without access to training or testing data?''
%\item
Second, 
to answer this question, we analyze hundreds of publicly-available pretrained models, including older and current state-of-the-art models in CV and NLP.
%\item 
Third, 
we find that norm-based metrics do a reasonably good job at predicting quality trends in well-trained models, i.e., they can be used to discriminate between ``good-better-best'' models, but they can qualitatively fail for models that may not be well-trained, i.e., to distinguish ``good-vs-bad'' models.
%\item 
Fourth, 
we find that PL-based metrics do much better, quantitatively better at discriminating good-better-best and qualitatively better at discriminating good-vs-bad.
%\item 
Finally, 
we find that PL-based metrics can also be used to characterize fine-scale properties of models, including understanding layer-wise \emph{correlation flow}, and evaluating post-training modifications such as model distillation and model improvement.
%\end{itemize}


\paragraph{Organization of this paper.}

We start in Section~\ref{sxn:background} and Section~\ref{sxn:methods} with background and an overview of our general approach.
In Section \ref{sxn:cv}, we study three well-known widely-available DNN CV architectures (the VGG, ResNet, and DenseNet series of models); and we provide an illustration of our basic methodology, both to evaluate the different metrics against reported test accuracies and to use quality metrics to understand model properties.
Then, in Section~\ref{sxn:nlp}, we look at several variations of a popular NLP DNN architecture (the OpenAI GPT and GPT2 models); and we show how model quality and properties vary between several variants of GPT and GPT2, including how metrics behave similarly and differently.
Then, in Section \ref{sxn:all_cv_models}, we present results based on an analysis of hundreds of pretraind DNN CV models, showing how well each metric is correlated with the reported test accuracies, and how the Alpha-Norm metric(s) perform remarkably well.
Finally, in Section~\ref{sxn:conc}, we provide a brief discussion and conclusion.


\paragraph{The WeightWatcher Tool.}

All of our computations were performed with the publicly-available \emph{WeightWatcher} tool (version 0.2.7)~\cite{weightwatcher_package};
and, in order to make our results fully reproducible, we provide Jupyter notebooks in the github repo for this paper~\cite{repo}.
\michael{LINK TO: Google Colab, IS THIS THAT REPO.}



