{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel / Nervana Systems Distiller Models\n",
    "\n",
    "https://nervanasystems.github.io/distiller/model_zoo.html#distiller-model-zoo\n",
    "\n",
    "Code adapted from:\n",
    "\n",
    "https://github.com/NervanaSystems/distiller/blob/master/jupyter/alexnet_insights.ipynb\n",
    "\n",
    "\n",
    "### Learning Structured Sparsity in Deep Neural Networks  (SSL)\n",
    "\n",
    "  ResNet20 models\n",
    "   \n",
    "###   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T02:23:08.286445Z",
     "start_time": "2019-03-06T02:23:08.276479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress the powerlaw package warnings\n",
    "# \"powerlaw.py:700: RuntimeWarning: divide by zero encountered in true_divide\"\n",
    "# \"powerlaw.py:700: RuntimeWarning: invalid value encountered in true_divide\"\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T00:33:16.711740Z",
     "start_time": "2019-03-06T00:33:06.655926Z"
    }
   },
   "outputs": [],
   "source": [
    "import weightwatcher as ww\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Load some common jupyter code\n",
    "%run './distiller_jupyter_helpers.ipynb'\n",
    "from distiller.models import create_model\n",
    "from distiller.apputils import *\n",
    "import qgrid\n",
    "\n",
    "from ipywidgets import *\n",
    "from bqplot import *\n",
    "import bqplot.pyplot as bqplt\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTILLER_DIR = \"/Users/charleshmartin/work/distiller/\"\n",
    "SSL_CHECKPOINTS_DIR = DISTILLER_DIR+\"examples/ssl/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_trained_4D_regularized_5Lremoved.pth.tar\r\n",
      "checkpoint_trained_4D_regularized_5Lremoved_finetuned.pth.tar\r\n",
      "checkpoint_trained_ch_regularized_dense.pth.tar\r\n",
      "checkpoint_trained_channel_regularized_resnet20.pth.tar\r\n",
      "checkpoint_trained_channel_regularized_resnet20_finetuned.pth.tar\r\n",
      "checkpoint_trained_dense.pth.tar\r\n"
     ]
    }
   ],
   "source": [
    "ls $CHECKPOINTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the checkpoint captured after one pruning event, and fine-tuning for one epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T02:04:33.004059Z",
     "start_time": "2019-03-06T02:04:32.993319Z"
    }
   },
   "outputs": [],
   "source": [
    "cpfiles = {\n",
    "    'checkpoint_trained_4D_regularized_5Lremoved.pth.tar': 90.620,\n",
    "    'checkpoint_trained_4D_regularized_5Lremoved_finetuned.pth.tar': 94.240,\n",
    "    'checkpoint_trained_ch_regularized_dense.pth.tar': 91.700,\n",
    "    'checkpoint_trained_channel_regularized_resnet20.pth.tar': 91.420,\n",
    "    'checkpoint_trained_channel_regularized_resnet20_finetuned.pth.tar': 91.420,\n",
    "    'checkpoint_trained_dense.pth.tar': 92.540,\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T02:25:24.713288Z",
     "start_time": "2019-03-06T02:23:15.624381Z"
    }
   },
   "source": [
    "results = {}\n",
    "\n",
    "for file, accuracy in cpfiles.items():\n",
    "    logger.info(file)\n",
    "    checkpoint_file = SSL_CHECKPOINTS_DIR+file\n",
    "\n",
    "    try:\n",
    "        resnet20_model = create_model(False, 'cifar10', 'resnet20_cifar', parallel=True)\n",
    "        load_checkpoint(resnet20_model, checkpoint_file);\n",
    "        watcher = ww.WeightWatcher(model=resnet20_model, logger=logger)\n",
    "        watcher.analyze(compute_alphas=True)\n",
    "        summary = watcher.get_summary()\n",
    "        summary['accuracy'] = accuracy\n",
    "        results[file] = summary\n",
    "    except Exception as e:\n",
    "        print(\"Did you forget to download the checkpoint file?\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint_trained_4D_regularized_5Lremoved.pth.tar'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T01:45:39.905509Z",
     "start_time": "2019-03-06T01:45:39.771241Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==> using cifar10 dataset\n",
      "INFO:root:=> creating resnet20_cifar model for CIFAR10\n",
      "INFO:root:=> loading checkpoint /Users/charleshmartin/work/distiller/examples/ssl/checkpoints/checkpoint_trained_4D_regularized_5Lremoved.pth.tar\n",
      "INFO:root:   best top@1: 90.620\n",
      "INFO:root:Loaded compression schedule from checkpoint (epoch 179)\n",
      "INFO:root:=> loaded checkpoint '/Users/charleshmartin/work/distiller/examples/ssl/checkpoints/checkpoint_trained_4D_regularized_5Lremoved.pth.tar' (epoch 179)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/charleshmartin/work/distiller/examples/ssl/checkpoints/checkpoint_trained_4D_regularized_5Lremoved.pth.tar\n"
     ]
    }
   ],
   "source": [
    "resnet20_model = create_model(False, 'cifar10', 'resnet20_cifar', parallel=True)\n",
    "checkpoint_file = SSL_CHECKPOINTS_DIR+list(cpfiles.keys())[0]\n",
    "print(checkpoint_file)\n",
    "\n",
    "try:\n",
    "    load_checkpoint(resnet20_model, checkpoint_file);\n",
    "except Exception as e:\n",
    "    print(\"Did you forget to download the checkpoint file?\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T02:25:36.491977Z",
     "start_time": "2019-03-06T02:25:36.471184Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-12 18:26:49,682 INFO \n",
      "WeightWatcher v0.1.2 by Calculation Consulting\n",
      "Analyze weight matrices of Deep Neural Networks\n",
      "https://calculationconsulting.com/\n",
      "python      version 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "numpy       version 1.15.3\n",
      "tensforflow version 1.10.1\n",
      "keras       version 2.2.2\n",
      "INFO:weightwatcher.weightwatcher:\n",
      "WeightWatcher v0.1.2 by Calculation Consulting\n",
      "Analyze weight matrices of Deep Neural Networks\n",
      "https://calculationconsulting.com/\n",
      "python      version 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "numpy       version 1.15.3\n",
      "tensforflow version 1.10.1\n",
      "keras       version 2.2.2\n"
     ]
    }
   ],
   "source": [
    "ww = ww.WeightWatcher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show that 5L removed layers are not analyzed\n",
    "\n",
    "see: https://nervanasystems.github.io/distiller/model_zoo.html#distiller-model-zoo\n",
    "\n",
    "<pre>\n",
    "Removing layer: module.layer1.0.conv1 [layer=0 block=0 conv=0]\n",
    "Removing layer: module.layer1.0.conv2 [layer=0 block=0 conv=1]\n",
    "Removing layer: module.layer1.1.conv1 [layer=0 block=1 conv=0]\n",
    "Removing layer: module.layer1.1.conv2 [layer=0 block=1 conv=1]\n",
    "Removing layer: module.layer2.2.conv2 [layer=1 block=2 conv=1]\n",
    "\n",
    "</pre>\n",
    "\n",
    "I think most of these are so small that the WW does not consider them anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-12 18:27:57,533 INFO Analyzing model\n",
      "INFO:weightwatcher.weightwatcher:Analyzing model\n",
      "2019-04-12 18:27:57,546 INFO ### Printing results ###\n",
      "INFO:weightwatcher.weightwatcher:### Printing results ###\n",
      "2019-04-12 18:27:57,548 INFO LogNorm: min: -0.13694187998771667, max: 0.8701463937759399, avg: 0.5175516605377197\n",
      "INFO:weightwatcher.weightwatcher:LogNorm: min: -0.13694187998771667, max: 0.8701463937759399, avg: 0.5175516605377197\n",
      "2019-04-12 18:27:57,550 INFO LogNorm compound: min: -0.08011464857392842, max: 0.8399592306878831, avg: 0.5175515964627266\n",
      "INFO:weightwatcher.weightwatcher:LogNorm compound: min: -0.08011464857392842, max: 0.8399592306878831, avg: 0.5175515964627266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'id': 0, 'type': ResNetCifar(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
       "    (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 1: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 1/9 (3,16): Skipping: too small (<50)'},\n",
       "  1: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 2/9 (3,16): Skipping: too small (<50)'},\n",
       "  2: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 3/9 (3,16): Skipping: too small (<50)'},\n",
       "  3: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 4/9 (3,16): Skipping: too small (<50)'},\n",
       "  4: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 5/9 (3,16): Skipping: too small (<50)'},\n",
       "  5: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 6/9 (3,16): Skipping: too small (<50)'},\n",
       "  6: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 7/9 (3,16): Skipping: too small (<50)'},\n",
       "  7: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 8/9 (3,16): Skipping: too small (<50)'},\n",
       "  8: {'N': 16,\n",
       "   'M': 3,\n",
       "   'Q': 5.333333333333333,\n",
       "   'summary': 'Weight matrix 9/9 (3,16): Skipping: too small (<50)'}},\n",
       " 2: {'id': 2,\n",
       "  'type': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 3: {'id': 3,\n",
       "  'type': ReLU(inplace),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 4: {'id': 4, 'type': Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "    )\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 5: {'id': 5, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 6: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (16,16): Skipping: too small (<50)'},\n",
       "  1: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (16,16): Skipping: too small (<50)'},\n",
       "  2: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (16,16): Skipping: too small (<50)'},\n",
       "  3: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (16,16): Skipping: too small (<50)'},\n",
       "  4: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (16,16): Skipping: too small (<50)'},\n",
       "  5: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (16,16): Skipping: too small (<50)'},\n",
       "  6: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (16,16): Skipping: too small (<50)'},\n",
       "  7: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (16,16): Skipping: too small (<50)'},\n",
       "  8: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (16,16): Skipping: too small (<50)'}},\n",
       " 7: {'id': 7,\n",
       "  'type': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 8: {'id': 8, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 9: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (16,16): Skipping: too small (<50)'},\n",
       "  1: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (16,16): Skipping: too small (<50)'},\n",
       "  2: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (16,16): Skipping: too small (<50)'},\n",
       "  3: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (16,16): Skipping: too small (<50)'},\n",
       "  4: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (16,16): Skipping: too small (<50)'},\n",
       "  5: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (16,16): Skipping: too small (<50)'},\n",
       "  6: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (16,16): Skipping: too small (<50)'},\n",
       "  7: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (16,16): Skipping: too small (<50)'},\n",
       "  8: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (16,16): Skipping: too small (<50)'}},\n",
       " 10: {'id': 10,\n",
       "  'type': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 11: {'id': 11, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 12: {'id': 12, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 13: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (16,16): Skipping: too small (<50)'},\n",
       "  1: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (16,16): Skipping: too small (<50)'},\n",
       "  2: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (16,16): Skipping: too small (<50)'},\n",
       "  3: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (16,16): Skipping: too small (<50)'},\n",
       "  4: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (16,16): Skipping: too small (<50)'},\n",
       "  5: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (16,16): Skipping: too small (<50)'},\n",
       "  6: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (16,16): Skipping: too small (<50)'},\n",
       "  7: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (16,16): Skipping: too small (<50)'},\n",
       "  8: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (16,16): Skipping: too small (<50)'}},\n",
       " 14: {'id': 14,\n",
       "  'type': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 15: {'id': 15, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 16: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (16,16): Skipping: too small (<50)'},\n",
       "  1: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (16,16): Skipping: too small (<50)'},\n",
       "  2: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (16,16): Skipping: too small (<50)'},\n",
       "  3: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (16,16): Skipping: too small (<50)'},\n",
       "  4: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (16,16): Skipping: too small (<50)'},\n",
       "  5: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (16,16): Skipping: too small (<50)'},\n",
       "  6: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (16,16): Skipping: too small (<50)'},\n",
       "  7: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (16,16): Skipping: too small (<50)'},\n",
       "  8: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (16,16): Skipping: too small (<50)'}},\n",
       " 17: {'id': 17,\n",
       "  'type': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 18: {'id': 18, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 19: {'id': 19, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 20: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (16,16): Skipping: too small (<50)'},\n",
       "  1: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (16,16): Skipping: too small (<50)'},\n",
       "  2: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (16,16): Skipping: too small (<50)'},\n",
       "  3: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (16,16): Skipping: too small (<50)'},\n",
       "  4: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (16,16): Skipping: too small (<50)'},\n",
       "  5: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (16,16): Skipping: too small (<50)'},\n",
       "  6: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (16,16): Skipping: too small (<50)'},\n",
       "  7: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (16,16): Skipping: too small (<50)'},\n",
       "  8: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (16,16): Skipping: too small (<50)'}},\n",
       " 21: {'id': 21,\n",
       "  'type': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 22: {'id': 22, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 23: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (16,16): Skipping: too small (<50)'},\n",
       "  1: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (16,16): Skipping: too small (<50)'},\n",
       "  2: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (16,16): Skipping: too small (<50)'},\n",
       "  3: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (16,16): Skipping: too small (<50)'},\n",
       "  4: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (16,16): Skipping: too small (<50)'},\n",
       "  5: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (16,16): Skipping: too small (<50)'},\n",
       "  6: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (16,16): Skipping: too small (<50)'},\n",
       "  7: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (16,16): Skipping: too small (<50)'},\n",
       "  8: {'N': 16,\n",
       "   'M': 16,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (16,16): Skipping: too small (<50)'}},\n",
       " 24: {'id': 24,\n",
       "  'type': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 25: {'id': 25, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 26: {'id': 26, 'type': Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "    )\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 27: {'id': 27, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 28: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 1/9 (16,32): Skipping: too small (<50)'},\n",
       "  1: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 2/9 (16,32): Skipping: too small (<50)'},\n",
       "  2: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 3/9 (16,32): Skipping: too small (<50)'},\n",
       "  3: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 4/9 (16,32): Skipping: too small (<50)'},\n",
       "  4: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 5/9 (16,32): Skipping: too small (<50)'},\n",
       "  5: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 6/9 (16,32): Skipping: too small (<50)'},\n",
       "  6: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 7/9 (16,32): Skipping: too small (<50)'},\n",
       "  7: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 8/9 (16,32): Skipping: too small (<50)'},\n",
       "  8: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 9/9 (16,32): Skipping: too small (<50)'}},\n",
       " 29: {'id': 29,\n",
       "  'type': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 30: {'id': 30, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 31: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (32,32): Skipping: too small (<50)'},\n",
       "  1: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (32,32): Skipping: too small (<50)'},\n",
       "  2: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (32,32): Skipping: too small (<50)'},\n",
       "  3: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (32,32): Skipping: too small (<50)'},\n",
       "  4: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (32,32): Skipping: too small (<50)'},\n",
       "  5: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (32,32): Skipping: too small (<50)'},\n",
       "  6: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (32,32): Skipping: too small (<50)'},\n",
       "  7: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (32,32): Skipping: too small (<50)'},\n",
       "  8: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (32,32): Skipping: too small (<50)'}},\n",
       " 32: {'id': 32,\n",
       "  'type': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 33: {'id': 33, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 34: {'id': 34, 'type': Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 35: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 32,\n",
       "   'M': 16,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 1/1 (16,32): Skipping: too small (<50)'}},\n",
       " 36: {'id': 36,\n",
       "  'type': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 37: {'id': 37, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 38: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (32,32): Skipping: too small (<50)'},\n",
       "  1: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (32,32): Skipping: too small (<50)'},\n",
       "  2: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (32,32): Skipping: too small (<50)'},\n",
       "  3: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (32,32): Skipping: too small (<50)'},\n",
       "  4: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (32,32): Skipping: too small (<50)'},\n",
       "  5: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (32,32): Skipping: too small (<50)'},\n",
       "  6: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (32,32): Skipping: too small (<50)'},\n",
       "  7: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (32,32): Skipping: too small (<50)'},\n",
       "  8: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (32,32): Skipping: too small (<50)'}},\n",
       " 39: {'id': 39,\n",
       "  'type': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 40: {'id': 40, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 41: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (32,32): Skipping: too small (<50)'},\n",
       "  1: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (32,32): Skipping: too small (<50)'},\n",
       "  2: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (32,32): Skipping: too small (<50)'},\n",
       "  3: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (32,32): Skipping: too small (<50)'},\n",
       "  4: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (32,32): Skipping: too small (<50)'},\n",
       "  5: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (32,32): Skipping: too small (<50)'},\n",
       "  6: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (32,32): Skipping: too small (<50)'},\n",
       "  7: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (32,32): Skipping: too small (<50)'},\n",
       "  8: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (32,32): Skipping: too small (<50)'}},\n",
       " 42: {'id': 42,\n",
       "  'type': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 43: {'id': 43, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 44: {'id': 44, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 45: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (32,32): Skipping: too small (<50)'},\n",
       "  1: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (32,32): Skipping: too small (<50)'},\n",
       "  2: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (32,32): Skipping: too small (<50)'},\n",
       "  3: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (32,32): Skipping: too small (<50)'},\n",
       "  4: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (32,32): Skipping: too small (<50)'},\n",
       "  5: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (32,32): Skipping: too small (<50)'},\n",
       "  6: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (32,32): Skipping: too small (<50)'},\n",
       "  7: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (32,32): Skipping: too small (<50)'},\n",
       "  8: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (32,32): Skipping: too small (<50)'}},\n",
       " 46: {'id': 46,\n",
       "  'type': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 47: {'id': 47, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 48: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 1/9 (32,32): Skipping: too small (<50)'},\n",
       "  1: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 2/9 (32,32): Skipping: too small (<50)'},\n",
       "  2: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 3/9 (32,32): Skipping: too small (<50)'},\n",
       "  3: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 4/9 (32,32): Skipping: too small (<50)'},\n",
       "  4: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 5/9 (32,32): Skipping: too small (<50)'},\n",
       "  5: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 6/9 (32,32): Skipping: too small (<50)'},\n",
       "  6: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 7/9 (32,32): Skipping: too small (<50)'},\n",
       "  7: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 8/9 (32,32): Skipping: too small (<50)'},\n",
       "  8: {'N': 32,\n",
       "   'M': 32,\n",
       "   'Q': 1.0,\n",
       "   'summary': 'Weight matrix 9/9 (32,32): Skipping: too small (<50)'}},\n",
       " 49: {'id': 49,\n",
       "  'type': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 50: {'id': 50, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 51: {'id': 51, 'type': Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu2): ReLU()\n",
       "    )\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 52: {'id': 52, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "    (downsample): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 53: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 1/9 (32,64): Skipping: too small (<50)'},\n",
       "  1: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 2/9 (32,64): Skipping: too small (<50)'},\n",
       "  2: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 3/9 (32,64): Skipping: too small (<50)'},\n",
       "  3: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 4/9 (32,64): Skipping: too small (<50)'},\n",
       "  4: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 5/9 (32,64): Skipping: too small (<50)'},\n",
       "  5: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 6/9 (32,64): Skipping: too small (<50)'},\n",
       "  6: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 7/9 (32,64): Skipping: too small (<50)'},\n",
       "  7: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 8/9 (32,64): Skipping: too small (<50)'},\n",
       "  8: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 9/9 (32,64): Skipping: too small (<50)'}},\n",
       " 54: {'id': 54,\n",
       "  'type': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 55: {'id': 55, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 56: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.81231827,\n",
       "   'summary': 'Weight matrix 1/9 (64,64): Lognorm: 0.8123182654380798'},\n",
       "  1: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.83928967,\n",
       "   'summary': 'Weight matrix 2/9 (64,64): Lognorm: 0.839289665222168'},\n",
       "  2: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.82493687,\n",
       "   'summary': 'Weight matrix 3/9 (64,64): Lognorm: 0.8249368667602539'},\n",
       "  3: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.8256705,\n",
       "   'summary': 'Weight matrix 4/9 (64,64): Lognorm: 0.8256704807281494'},\n",
       "  4: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.8701464,\n",
       "   'summary': 'Weight matrix 5/9 (64,64): Lognorm: 0.8701463937759399'},\n",
       "  5: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.857111,\n",
       "   'summary': 'Weight matrix 6/9 (64,64): Lognorm: 0.8571109771728516'},\n",
       "  6: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.8277814,\n",
       "   'summary': 'Weight matrix 7/9 (64,64): Lognorm: 0.8277813792228699'},\n",
       "  7: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.86400855,\n",
       "   'summary': 'Weight matrix 8/9 (64,64): Lognorm: 0.8640085458755493'},\n",
       "  8: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.8383705,\n",
       "   'summary': 'Weight matrix 9/9 (64,64): Lognorm: 0.8383705019950867'}},\n",
       " 57: {'id': 57,\n",
       "  'type': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 58: {'id': 58, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 59: {'id': 59, 'type': Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 60: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 64,\n",
       "   'M': 32,\n",
       "   'Q': 2.0,\n",
       "   'summary': 'Weight matrix 1/1 (32,64): Skipping: too small (<50)'}},\n",
       " 61: {'id': 61,\n",
       "  'type': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 62: {'id': 62, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 63: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.79943484,\n",
       "   'summary': 'Weight matrix 1/9 (64,64): Lognorm: 0.7994348406791687'},\n",
       "  1: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.78365344,\n",
       "   'summary': 'Weight matrix 2/9 (64,64): Lognorm: 0.7836534380912781'},\n",
       "  2: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.79597294,\n",
       "   'summary': 'Weight matrix 3/9 (64,64): Lognorm: 0.7959729433059692'},\n",
       "  3: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.77803314,\n",
       "   'summary': 'Weight matrix 4/9 (64,64): Lognorm: 0.7780331373214722'},\n",
       "  4: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.76537883,\n",
       "   'summary': 'Weight matrix 5/9 (64,64): Lognorm: 0.7653788328170776'},\n",
       "  5: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.77683717,\n",
       "   'summary': 'Weight matrix 6/9 (64,64): Lognorm: 0.776837170124054'},\n",
       "  6: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.79230237,\n",
       "   'summary': 'Weight matrix 7/9 (64,64): Lognorm: 0.7923023700714111'},\n",
       "  7: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.78286105,\n",
       "   'summary': 'Weight matrix 8/9 (64,64): Lognorm: 0.782861053943634'},\n",
       "  8: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.8010708,\n",
       "   'summary': 'Weight matrix 9/9 (64,64): Lognorm: 0.8010708093643188'}},\n",
       " 64: {'id': 64,\n",
       "  'type': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 65: {'id': 65, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 66: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.2838607,\n",
       "   'summary': 'Weight matrix 1/9 (64,64): Lognorm: 0.2838607132434845'},\n",
       "  1: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.27320388,\n",
       "   'summary': 'Weight matrix 2/9 (64,64): Lognorm: 0.27320387959480286'},\n",
       "  2: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.26901588,\n",
       "   'summary': 'Weight matrix 3/9 (64,64): Lognorm: 0.2690158784389496'},\n",
       "  3: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.2771391,\n",
       "   'summary': 'Weight matrix 4/9 (64,64): Lognorm: 0.2771390974521637'},\n",
       "  4: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.27194324,\n",
       "   'summary': 'Weight matrix 5/9 (64,64): Lognorm: 0.27194324135780334'},\n",
       "  5: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.2745091,\n",
       "   'summary': 'Weight matrix 6/9 (64,64): Lognorm: 0.27450910210609436'},\n",
       "  6: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.2866516,\n",
       "   'summary': 'Weight matrix 7/9 (64,64): Lognorm: 0.286651611328125'},\n",
       "  7: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.28060603,\n",
       "   'summary': 'Weight matrix 8/9 (64,64): Lognorm: 0.2806060314178467'},\n",
       "  8: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.28349158,\n",
       "   'summary': 'Weight matrix 9/9 (64,64): Lognorm: 0.2834915816783905'}},\n",
       " 67: {'id': 67,\n",
       "  'type': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 68: {'id': 68, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 69: {'id': 69, 'type': BasicBlock(\n",
       "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU()\n",
       "  ), 'message': 'Skipping (Layer not supported)'},\n",
       " 70: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.761654,\n",
       "   'summary': 'Weight matrix 1/9 (64,64): Lognorm: 0.7616540193557739'},\n",
       "  1: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.77223295,\n",
       "   'summary': 'Weight matrix 2/9 (64,64): Lognorm: 0.7722329497337341'},\n",
       "  2: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.7702672,\n",
       "   'summary': 'Weight matrix 3/9 (64,64): Lognorm: 0.7702671885490417'},\n",
       "  3: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.7390477,\n",
       "   'summary': 'Weight matrix 4/9 (64,64): Lognorm: 0.7390477061271667'},\n",
       "  4: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.7307151,\n",
       "   'summary': 'Weight matrix 5/9 (64,64): Lognorm: 0.7307150959968567'},\n",
       "  5: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.75148773,\n",
       "   'summary': 'Weight matrix 6/9 (64,64): Lognorm: 0.7514877319335938'},\n",
       "  6: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.7828033,\n",
       "   'summary': 'Weight matrix 7/9 (64,64): Lognorm: 0.7828032970428467'},\n",
       "  7: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.7815274,\n",
       "   'summary': 'Weight matrix 8/9 (64,64): Lognorm: 0.7815274000167847'},\n",
       "  8: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': 0.7855195,\n",
       "   'summary': 'Weight matrix 9/9 (64,64): Lognorm: 0.7855194807052612'}},\n",
       " 71: {'id': 71,\n",
       "  'type': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 72: {'id': 72, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 73: {'layer_type': <LAYER_TYPE.CONV2D: 4>,\n",
       "  0: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.05979418,\n",
       "   'summary': 'Weight matrix 1/9 (64,64): Lognorm: -0.05979418009519577'},\n",
       "  1: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.099358946,\n",
       "   'summary': 'Weight matrix 2/9 (64,64): Lognorm: -0.0993589460849762'},\n",
       "  2: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.05574345,\n",
       "   'summary': 'Weight matrix 3/9 (64,64): Lognorm: -0.05574344843626022'},\n",
       "  3: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.087387025,\n",
       "   'summary': 'Weight matrix 4/9 (64,64): Lognorm: -0.08738702535629272'},\n",
       "  4: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.13694188,\n",
       "   'summary': 'Weight matrix 5/9 (64,64): Lognorm: -0.13694187998771667'},\n",
       "  5: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.08808553,\n",
       "   'summary': 'Weight matrix 6/9 (64,64): Lognorm: -0.08808553218841553'},\n",
       "  6: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.048493158,\n",
       "   'summary': 'Weight matrix 7/9 (64,64): Lognorm: -0.0484931580722332'},\n",
       "  7: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.089487955,\n",
       "   'summary': 'Weight matrix 8/9 (64,64): Lognorm: -0.0894879549741745'},\n",
       "  8: {'N': 64,\n",
       "   'M': 64,\n",
       "   'Q': 1.0,\n",
       "   'lognorm': -0.055739712,\n",
       "   'summary': 'Weight matrix 9/9 (64,64): Lognorm: -0.055739711970090866'}},\n",
       " 74: {'id': 74,\n",
       "  'type': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 75: {'id': 75, 'type': ReLU(), 'message': 'Skipping (Layer not supported)'},\n",
       " 76: {'id': 76,\n",
       "  'type': AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
       "  'message': 'Skipping (Layer not supported)'},\n",
       " 77: {'id': 77,\n",
       "  'type': Linear(in_features=64, out_features=10, bias=True),\n",
       "  'layer_type': <LAYER_TYPE.DENSE: 1>,\n",
       "  0: {'N': 64,\n",
       "   'M': 10,\n",
       "   'Q': 6.4,\n",
       "   'summary': 'Weight matrix 1/1 (10,64): Skipping: too small (<50)'}}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww.analyze(resnet20_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Iterative Pruning\n",
    "\n",
    "\n",
    "### Compare comptessed models to PyTorch AlexNet and Resnet18\n",
    "\n",
    "\n",
    "### AlexNet Iterative Pruning\n",
    "\n",
    "alexnet.checkpoint.89.pth.tar\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/nndistiller/sensitivity-pruning/alexnet.checkpoint.89.pth.tar\n",
    "\n",
    "\n",
    "Our reference is TorchVision's pretrained Alexnet model which has a Top1 accuracy of 56.55 and Top5=79.09. \n",
    "\n",
    "We prune away 88.44% of the parameters and achieve Top1=56.61 and Top5=79.45. \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-12 19:19:21--  https://s3-us-west-1.amazonaws.com/nndistiller/sensitivity-pruning/alexnet.checkpoint.89.pth.tar\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 54.231.236.41\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|54.231.236.41|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733174951 (699M) [application/x-tar]\n",
      "Saving to: ‘alexnet.checkpoint.89.pth.tar’\n",
      "\n",
      "alexnet.checkpoint. 100%[===================>] 699.21M  10.7MB/s    in 66s     \n",
      "\n",
      "2019-04-12 19:20:27 (10.6 MB/s) - ‘alexnet.checkpoint.89.pth.tar’ saved [733174951/733174951]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/nndistiller/sensitivity-pruning/alexnet.checkpoint.89.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==> using imagenet dataset\n",
      "INFO:root:=> using alexnet model for ImageNet\n",
      "INFO:root:=> loading checkpoint alexnet.checkpoint.89.pth.tar\n",
      "INFO:root:   best top@1: 52.043\n",
      "INFO:root:Loaded compression schedule from checkpoint (epoch 89)\n",
      "INFO:root:=> loaded checkpoint 'alexnet.checkpoint.89.pth.tar' (epoch 89)\n",
      "INFO:app_cfg:\n",
      "WeightWatcher v0.1.2 by Calculation Consulting\n",
      "Analyze weight matrices of Deep Neural Networks\n",
      "https://calculationconsulting.com/\n",
      "python      version 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "numpy       version 1.15.3\n",
      "tensforflow version 1.10.1\n",
      "keras       version 2.2.2\n",
      "INFO:app_cfg:Analyzing model\n",
      "INFO:app_cfg:### Printing results ###\n",
      "INFO:app_cfg:LogNorm: min: -0.13694187998771667, max: 0.8701463937759399, avg: 0.5175516605377197\n",
      "INFO:app_cfg:LogNorm compound: min: -0.08011464857392842, max: 0.8399592306878831, avg: 0.5175515964627266\n",
      "INFO:app_cfg:Alpha: min: 1.490533392059119, max: 10.44447500619053, avg: 3.196053477405699\n",
      "INFO:app_cfg:Alpha compound: min: 1.525600282571416, max: 4.1041530584371255, avg: 3.196053477405699\n",
      "INFO:app_cfg:Alpha Weighted: min: -4.778228304138091, max: 4.84831647823597, avg: 0.3071941264704492\n",
      "INFO:app_cfg:Alpha Weighted compound: min: -2.236248250745935, max: 1.842976302987391, avg: 0.30719412647044925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lognorm': 0.51755166,\n",
       " 'lognorm_compound': 0.5175515964627266,\n",
       " 'alpha': 3.196053477405699,\n",
       " 'alpha_compound': 3.196053477405699,\n",
       " 'alpha_weighted': 0.3071941264704492,\n",
       " 'alpha_weighted_compound': 0.30719412647044925}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet89_model = create_model(False, 'imagenet', 'alexnet', parallel=True)\n",
    "checkpoint_file = 'alexnet.checkpoint.89.pth.tar'\n",
    "try:\n",
    "    load_checkpoint(alexnet89_model, checkpoint_file)\n",
    "    watcher = ww.WeightWatcher(model=resnet20_model, logger=logger)\n",
    "    watcher.analyze(compute_alphas=True)\n",
    "    summary = watcher.get_summary()\n",
    "except NameError as e:\n",
    "    print(\"Did you forget to download the checkpoint file?\")\n",
    "    raise e\n",
    "    \n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-12 22:27:05,874 INFO \n",
      "WeightWatcher v0.1.2 by Calculation Consulting\n",
      "Analyze weight matrices of Deep Neural Networks\n",
      "https://calculationconsulting.com/\n",
      "python      version 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "numpy       version 1.15.3\n",
      "tensforflow version 1.10.1\n",
      "keras       version 2.2.2\n",
      "INFO:weightwatcher.weightwatcher:\n",
      "WeightWatcher v0.1.2 by Calculation Consulting\n",
      "Analyze weight matrices of Deep Neural Networks\n",
      "https://calculationconsulting.com/\n",
      "python      version 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "numpy       version 1.15.3\n",
      "tensforflow version 1.10.1\n",
      "keras       version 2.2.2\n",
      "2019-04-12 22:27:05,884 INFO Analyzing model\n",
      "INFO:weightwatcher.weightwatcher:Analyzing model\n",
      "2019-04-12 22:29:21,593 INFO ### Printing results ###\n",
      "INFO:weightwatcher.weightwatcher:### Printing results ###\n",
      "2019-04-12 22:29:21,594 INFO LogNorm: min: 0.510020911693573, max: 1.759843349456787, avg: 0.8426187634468079\n",
      "INFO:weightwatcher.weightwatcher:LogNorm: min: 0.510020911693573, max: 1.759843349456787, avg: 0.8426187634468079\n",
      "2019-04-12 22:29:21,596 INFO LogNorm compound: min: 0.6662279224395752, max: 1.759843349456787, avg: 1.2041019360602847\n",
      "INFO:weightwatcher.weightwatcher:LogNorm compound: min: 0.6662279224395752, max: 1.759843349456787, avg: 1.2041019360602847\n",
      "2019-04-12 22:29:21,597 INFO Alpha: min: 1.6171942693316663, max: 4.722972798056681, avg: 2.8006915646495956\n",
      "INFO:weightwatcher.weightwatcher:Alpha: min: 1.6171942693316663, max: 4.722972798056681, avg: 2.8006915646495956\n",
      "2019-04-12 22:29:21,598 INFO Alpha compound: min: 2.2451601086445683, max: 3.7108900208070548, avg: 2.79836381328899\n",
      "INFO:weightwatcher.weightwatcher:Alpha compound: min: 2.2451601086445683, max: 3.7108900208070548, avg: 2.79836381328899\n",
      "2019-04-12 22:29:21,599 INFO Alpha Weighted: min: -0.19479981089775408, max: 4.980225911128444, avg: 1.7093696729133068\n",
      "INFO:weightwatcher.weightwatcher:Alpha Weighted: min: -0.19479981089775408, max: 4.980225911128444, avg: 1.7093696729133068\n",
      "2019-04-12 22:29:21,600 INFO Alpha Weighted compound: min: 0.9209278229619574, max: 4.980225911128444, avg: 2.892666070125807\n",
      "INFO:weightwatcher.weightwatcher:Alpha Weighted compound: min: 0.9209278229619574, max: 4.980225911128444, avg: 2.892666070125807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lognorm': 0.84261876,\n",
       " 'lognorm_compound': 1.2041019360602847,\n",
       " 'alpha': 2.8006915646495956,\n",
       " 'alpha_compound': 2.79836381328899,\n",
       " 'alpha_weighted': 1.7093696729133068,\n",
       " 'alpha_weighted_compound': 2.892666070125807}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "alexnet_baseline_model = models.alexnet(pretrained=True)\n",
    "watcher = ww.WeightWatcher(model=alexnet_baseline_model)\n",
    "results = watcher.analyze(compute_alphas=True)\n",
    "watcher.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Linear(in_features=9216, out_features=4096, bias=True)\n",
      "20 Linear(in_features=4096, out_features=4096, bias=True)\n",
      "22 Linear(in_features=4096, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "layers = alexnet89_model.modules()\n",
    "for i, l in enumerate(layers):\n",
    "    if (type(l)==torch.nn.modules.linear.Linear):\n",
    "        print(i,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_mat(W):\n",
    "    Wshape = W.shape\n",
    "    Wrand = W.flatten()\n",
    "    np.random.shuffle(Wrand)\n",
    "    Wrand = Wrand.reshape(Wshape)\n",
    "    return Wrand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Linear(in_features=9216, out_features=4096, bias=True)\n",
      "actual 3.2532797\n",
      "random  1.404957 0.4318587\n",
      "20 Linear(in_features=4096, out_features=4096, bias=True)\n",
      "actual 4.510313\n",
      "random  2.8153994 0.62421376\n",
      "22 Linear(in_features=4096, out_features=1000, bias=True)\n",
      "actual 4.740425\n",
      "random  1.6384666 0.34563705\n"
     ]
    }
   ],
   "source": [
    "layers = alexnet89_model.modules()\n",
    "for i, l in enumerate(layers):\n",
    "    if (type(l)==torch.nn.modules.linear.Linear):\n",
    "        print(i,l)\n",
    "        W = [np.array(l.weight.data.clone().cpu())][0]\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=999)\n",
    "        svd.fit(W)\n",
    "        sv = svd.singular_values_\n",
    "        max_sv =  np.max(sv)\n",
    "        print(\"actual\", max_sv)\n",
    "\n",
    "        svd = TruncatedSVD(n_components=999)\n",
    "        svd.fit(randomize_mat(W))\n",
    "        sv = svd.singular_values_\n",
    "        max_rand_sv =  np.max(sv)\n",
    "        print(\"random \", max_rand_sv, max_rand_sv/max_sv )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Linear(in_features=9216, out_features=4096, bias=True)\n",
      "actual 5.9382377\n",
      "random  3.6277204 0.61090857\n",
      "20 Linear(in_features=4096, out_features=4096, bias=True)\n",
      "actual 9.300288\n",
      "random  6.8204 0.7333536\n",
      "22 Linear(in_features=4096, out_features=1000, bias=True)\n",
      "actual 6.6769724\n",
      "random  1.7816509 0.26683515\n"
     ]
    }
   ],
   "source": [
    "layers = alexnet_baseline_model.modules()\n",
    "for i, l in enumerate(layers):\n",
    "    if (type(l)==torch.nn.modules.linear.Linear):\n",
    "        print(i,l)\n",
    "        W = [np.array(l.weight.data.clone().cpu())][0]\n",
    "        \n",
    "        svd = TruncatedSVD(n_components=999)\n",
    "        svd.fit(W)\n",
    "        sv = svd.singular_values_\n",
    "        max_sv =  np.max(sv)\n",
    "        print(\"actual\", max_sv)\n",
    "\n",
    "        svd = TruncatedSVD(n_components=999)\n",
    "        svd.fit(randomize_mat(W))\n",
    "        sv = svd.singular_values_\n",
    "        max_rand_sv =  np.max(sv)\n",
    "        print(\"random \", max_rand_sv, max_rand_sv/max_sv )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models not working yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet   Iterative Pruning\n",
    "\n",
    "As our baseline we used a pretrained PyTorch MobileNet model (width=1) which has Top1=68.848 and Top5=88.740.\n",
    "In their paper, \n",
    "\n",
    "Zhu and Gupta prune 50% of the elements of MobileNet (width=1) with a 1.1% drop in accuracy. We pruned about 51.6% of the elements, with virtually no change in the accuracies (Top1: 68.808 and Top5: 88.656)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-12 21:18:48--  https://s3-us-west-1.amazonaws.com/nndistiller/agp-pruning/mobilenet/checkpoint.pth.tar\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.120.32\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.120.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50527563 (48M) [application/x-tar]\n",
      "Saving to: ‘checkpoint.pth.tar’\n",
      "\n",
      "checkpoint.pth.tar  100%[===================>]  48.19M  9.26MB/s    in 5.0s    \n",
      "\n",
      "2019-04-12 21:18:54 (9.67 MB/s) - ‘checkpoint.pth.tar’ saved [50527563/50527563]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/nndistiller/agp-pruning/mobilenet/checkpoint.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18  Iterative Pruning\n",
    "\n",
    "Results not reportedon Blog\n",
    "\n",
    "Can not run ?  IDK why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-12 21:24:17--  https://s3-us-west-1.amazonaws.com/nndistiller/agp-pruning/resnet18/checkpoint.pth.tar\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.28.5\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.28.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 138082569 (132M) [application/x-tar]\n",
      "Saving to: ‘checkpoint.pth.tar.2’\n",
      "\n",
      "checkpoint.pth.tar. 100%[===================>] 131.69M  10.6MB/s    in 18s     \n",
      "\n",
      "2019-04-12 21:24:36 (7.29 MB/s) - ‘checkpoint.pth.tar.2’ saved [138082569/138082569]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/nndistiller/agp-pruning/resnet18/checkpoint.pth.tar\n",
    "!mv checkpoint.pth.tar resnet18.checkpoint.pth.tar"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "resnet18_model = create_model(False, 'imagenet', 'resnet18', parallel=True)\n",
    "checkpoint_file = 'resnet18.checkpoint.pth.tar'\n",
    "try:\n",
    "    load_checkpoint(resnet18_model, checkpoint_file)\n",
    "    watcher = ww.WeightWatcher(model=resent18_model, logger=logger)\n",
    "    watcher.analyze(compute_alphas=True)\n",
    "    summary = watcher.get_summary()\n",
    "except NameError as e:\n",
    "    print(\"Did you forget to download the checkpoint file?\")\n",
    "    raise e\n",
    "    \n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.resnet18(pretrained=True)\n",
    "watcher = ww.WeightWatcher(model=model)\n",
    "results = watcher.analyze(compute_alphas=True)\n",
    "watcher.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet56 .  Network thinning  CIFAR10\n",
    "\n",
    "We started by training the baseline ResNet56-Cifar dense network (180 epochs) since we didn't have a pre-trained model.\n",
    "\n",
    "We trained a ResNet56-Cifar10 network and achieve accuracy results which are on-par with published results: Top1: 92.970 and Top5: 99.740.\n",
    "\n",
    "We used Hao et al.'s algorithm to remove 37.3% of the original convolution MACs, while maintaining virtually the same accuracy as the baseline: Top1: 92.830 and Top5: 99.760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-12 21:42:51--  https://s3-us-west-1.amazonaws.com/nndistiller/pruning_filters_for_efficient_convnets/checkpoint.resnet56_cifar_baseline.pth.tar\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.116.88\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.116.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6937961 (6.6M) [application/x-tar]\n",
      "Saving to: ‘checkpoint.resnet56_cifar_baseline.pth.tar’\n",
      "\n",
      "checkpoint.resnet56 100%[===================>]   6.62M  3.11MB/s    in 2.1s    \n",
      "\n",
      "2019-04-12 21:42:53 (3.11 MB/s) - ‘checkpoint.resnet56_cifar_baseline.pth.tar’ saved [6937961/6937961]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/nndistiller/pruning_filters_for_efficient_convnets/checkpoint.resnet56_cifar_baseline.pth.tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-12 21:42:53--  https://s3-us-west-1.amazonaws.com/nndistiller/pruning_filters_for_efficient_convnets/checkpoint_finetuned.pth.tar\n",
      "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.116.88\n",
      "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.116.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5235933 (5.0M) [application/x-tar]\n",
      "Saving to: ‘checkpoint_finetuned.pth.tar’\n",
      "\n",
      "checkpoint_finetune 100%[===================>]   4.99M  2.41MB/s    in 2.1s    \n",
      "\n",
      "2019-04-12 21:42:56 (2.41 MB/s) - ‘checkpoint_finetuned.pth.tar’ saved [5235933/5235933]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/nndistiller/pruning_filters_for_efficient_convnets/checkpoint_finetuned.pth.tar\n",
    "!mv checkpoint_finetuned.pth.tar checkpoint.resnet56_cifar_finetuned.pth.tar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==> using cifar10 dataset\n",
      "INFO:root:=> creating resnet56_cifar model for CIFAR10\n",
      "INFO:root:=> loading checkpoint checkpoint.resnet56_cifar_baseline.pth.tar\n",
      "INFO:root:   best top@1: 92.920\n",
      "INFO:root:Loaded compression schedule from checkpoint (epoch 179)\n",
      "INFO:root:=> loaded checkpoint 'checkpoint.resnet56_cifar_baseline.pth.tar' (epoch 179)\n",
      "INFO:app_cfg:\n",
      "WeightWatcher v0.1.2 by Calculation Consulting\n",
      "Analyze weight matrices of Deep Neural Networks\n",
      "https://calculationconsulting.com/\n",
      "python      version 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "numpy       version 1.15.3\n",
      "tensforflow version 1.10.1\n",
      "keras       version 2.2.2\n",
      "INFO:app_cfg:Analyzing model\n",
      "INFO:app_cfg:### Printing results ###\n",
      "INFO:app_cfg:LogNorm: min: 0.5676514506340027, max: 1.3524690866470337, avg: 0.7858442068099976\n",
      "INFO:app_cfg:LogNorm compound: min: 0.5736563735538058, max: 1.3524690866470337, avg: 0.8627014862166511\n",
      "INFO:app_cfg:Alpha: min: 2.123183184600569, max: 35.58388398861787, avg: 7.889852459338699\n",
      "INFO:app_cfg:Alpha compound: min: 4.5517436861442855, max: 13.27393416482163, avg: 8.006562435126574\n",
      "INFO:app_cfg:Alpha Weighted: min: -3.5906372305916014, max: 7.121340380064339, avg: -0.5332915968457409\n",
      "INFO:app_cfg:Alpha Weighted compound: min: -1.749841419862558, max: 7.121340380064339, avg: 0.4614291985654078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lognorm': 0.7858442,\n",
       " 'lognorm_compound': 0.8627014862166511,\n",
       " 'alpha': 7.889852459338699,\n",
       " 'alpha_compound': 8.006562435126574,\n",
       " 'alpha_weighted': -0.5332915968457409,\n",
       " 'alpha_weighted_compound': 0.4614291985654078}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet56_model = create_model(False, 'cifar10', 'resnet56_cifar', parallel=True)\n",
    "checkpoint_file = 'checkpoint.resnet56_cifar_baseline.pth.tar'\n",
    "\n",
    "try:\n",
    "    load_checkpoint(resnet56_model, checkpoint_file)\n",
    "    watcher = ww.WeightWatcher(model=resent18_model, logger=logger)\n",
    "    watcher.analyze(compute_alphas=True)\n",
    "    summary = watcher.get_summary()\n",
    "except NameError as e:\n",
    "    print(\"Did you forget to download the checkpoint file?\")\n",
    "    raise e\n",
    "    \n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:==> using cifar10 dataset\n",
      "INFO:root:=> creating resnet56_cifar model for CIFAR10\n",
      "INFO:root:=> loading checkpoint checkpoint.resnet56_cifar_finetuned.pth.tar\n",
      "INFO:root:   best top@1: 92.960\n",
      "INFO:root:Loaded compression schedule from checkpoint (epoch 59)\n",
      "INFO:root:=> loaded checkpoint 'checkpoint.resnet56_cifar_finetuned.pth.tar' (epoch 59)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNetCifar:\n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.0.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.0.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.0.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.0.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.0.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.1.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.1.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.1.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.1.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.1.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.2.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.2.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.2.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.2.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.2.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.2.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.3.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.3.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.3.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.3.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.3.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.3.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.4.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.4.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.4.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.4.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.4.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.4.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.5.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.5.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.5.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.5.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.5.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.5.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.6.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.6.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.6.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.6.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.6.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.6.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for layer2.0.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.0.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.0.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.0.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.0.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.1.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.1.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.1.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.1.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.1.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.2.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.2.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.2.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.2.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.2.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.2.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.3.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.3.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.3.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.3.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.3.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.3.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.5.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.5.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.5.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.5.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.5.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.5.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.6.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.6.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.6.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.6.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.6.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.6.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.7.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.7.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.7.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.7.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.7.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.7.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer3.2.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.2.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.2.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.2.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.2.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.2.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.3.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.3.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.3.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.3.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.3.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.3.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.5.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.5.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.5.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.5.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.5.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.5.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.6.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.6.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.6.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.6.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.6.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.6.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.7.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.7.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.7.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.7.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.7.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.7.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.8.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.8.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.8.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.8.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.8.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.8.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-b3ab1cd139b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet56_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mwatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeightWatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresent18_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mwatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_alphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/work/distiller/distiller/apputils/checkpoint.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(model, chkpt_file, optimizer)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mcompression_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression_sched'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_dataparallel_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;31m# A very common source of this KeyError is loading a GPU model on the CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;31m# We rename all of the DataParallel keys because DataParallel does not execute on the CPU.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mnormalize_dataparallel_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNetCifar:\n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.0.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.0.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.0.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.0.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.0.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.1.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.1.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.1.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.1.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.1.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.2.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.2.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.2.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.2.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.2.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.2.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.3.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.3.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.3.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.3.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.3.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.3.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.4.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.4.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.4.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.4.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.4.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.4.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.5.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.5.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.5.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.5.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.5.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.5.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.6.conv1.weight: copying a param with shape torch.Size([7, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer1.6.bn1.weight: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.6.bn1.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.6.bn1.running_mean: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.6.bn1.running_var: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer1.6.conv2.weight: copying a param with shape torch.Size([16, 7, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for layer2.0.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.0.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.0.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.0.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.0.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.1.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.1.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.1.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.1.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.1.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.2.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.2.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.2.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.2.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.2.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.2.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.3.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.3.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.3.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.3.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.3.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.3.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.5.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.5.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.5.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.5.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.5.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.5.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.6.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.6.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.6.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.6.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.6.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.6.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.7.conv1.weight: copying a param with shape torch.Size([16, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer2.7.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.7.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.7.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.7.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer2.7.conv2.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for layer3.2.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.2.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.2.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.2.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.2.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.2.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.3.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.3.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.3.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.3.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.3.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.3.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.5.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.5.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.5.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.5.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.5.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.5.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.6.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.6.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.6.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.6.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.6.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.6.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.7.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.7.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.7.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.7.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.7.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.7.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.8.conv1.weight: copying a param with shape torch.Size([45, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer3.8.bn1.weight: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.8.bn1.bias: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.8.bn1.running_mean: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.8.bn1.running_var: copying a param with shape torch.Size([45]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer3.8.conv2.weight: copying a param with shape torch.Size([64, 45, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3])."
     ]
    }
   ],
   "source": [
    "resnet56_model = create_model(False, 'cifar10', 'resnet56_cifar', parallel=True)\n",
    "checkpoint_file = 'checkpoint.resnet56_cifar_finetuned.pth.tar'\n",
    "\n",
    "try:\n",
    "    load_checkpoint(resnet56_model, checkpoint_file)\n",
    "    watcher = ww.WeightWatcher(model=resent18_model, logger=logger)\n",
    "    watcher.analyze(compute_alphas=True)\n",
    "    summary = watcher.get_summary()\n",
    "except NameError as e:\n",
    "    print(\"Did you forget to download the checkpoint file?\")\n",
    "    raise e\n",
    "    \n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
