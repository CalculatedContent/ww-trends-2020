\section{Methods}
\label{sxn:methods}


Let us 
write the Energy Landscape (or optimization function, parameterized by $\mathbf{W}_{l}$s and $\mathbf{b}_{l}$s) for a DNN with $L$ layers, activation functions $h_{l}(\cdot)$, and $N\times M$ weight matrices $\mathbf{W}_{l}$ and biases $\mathbf{b}_{l}$,~as:
\begin{equation}
E_{DNN}=h_{L}(\mathbf{W}_{L}\times h_{L-1}(\mathbf{W}_{L-1}\times h_{L-2}(\cdots)+\mathbf{b}_{L-1})+\mathbf{b}_{L})  .
\label{eqn:dnn_energy}
\end{equation}
Assume we are given several pretrained DNNs, e.g., as part of an architecture series.
The models have been trained on (unspecified and unavailable) labeled data $\{d_{i},y_{i}\}\in\mathcal{D}$, using Backprop, by minimizing some (also unspecified and unavailable) loss function $\mathcal{L}(\cdot)$. 
We expect that most well-trained, production-quality models will employ one or more forms of on regularization, such as Batch Normalization, Dropout, etc., and many will also contain additional structure such as Skip Connections, etc. 
Here, we will ignore these details, and will focus only on the layer weight matrices $\mathbf{W}_{l}$.


\paragraph{DNN Quality Metrics.}

Each DNN layer contains one or more layer 2D weight matrices $\mathbf{W}_{l}$, and/or 2D feature maps $\mathbf{W}_{i,l}$ extracted from 2D Convolutional layers. 
(For notational convenience, we may drop the $i$ and/or $i,l$ subscripts below; and we let $\mathbf{W}$ be a generic $N\times M$ weight matrix, with $N\ge M$.) 
%
We have examined a large number of possible quality metrics.
The best performing metrics (recall that we can only consider metrics that do not use training/test data) depend on the norm and/or spectral properties of weight matrices, $\mathbf{W}$.%
\footnote{We do not use intra-layer information from the models in our quality metrics, but as we will describe our metrics can be used to learn about that.}
Consider the following.
\charles{Note I write X for the $\alpha$-norm and not W.  This is because it would be $2\alpha$.  I know its a bit confusing.
We have to be careful here with the Frobenius norm also and check that we are not off by a factor of 2}
\begin{itemize}
\item 
Frobenius Norm: $\Vert\mathbf{W}\Vert^{2}_{F}=\sum_{i,j}w^{2}_{i,j} = \sum_{i=1}^{M} \lambda_{i}^{2}$
\item 
Spectral Norm: $\Vert\mathbf{W}\Vert_{\infty}=\lambda_{max}$
\michael{Should this be $\sqrt{\lambda_{max}}$?}
\item 
Weighted Alpha Metric: $\hat{\alpha}=\alpha\log\lambda_{max}$
\item 
$\alpha$-Norm (or $\alpha$-Shatten Norm): $\Vert\mathbf{W}\Vert^{\alpha}_{\alpha}=\sum_{i=1}^{M}\lambda_{i}^{\alpha}$
\end{itemize}
The first two quantities are well-known in ML.
The last two deserve special mention.
For all these quantities, $\lambda_{i}$ is the $i^{th}$ eigenvalue of the \emph{Empirical Correlation Matrix},
$ %% $$
\mathbf{X}=\mathbf{W}^{T}\mathbf{W} ,
$ %% $$
and so $\lambda_{max}$ is the maximum eigenvalue of $\mathbf{X}$. 
(These eigenvalues are the squares of the singular values $\sigma_{i}$ of $\mathbf{W}$, i.e., $\lambda_{i}=\sigma^{2}_{i}$.)
For the last two quantities, $\alpha$ is the PL exponent that arises in the recently-developed HT-SR Theory~\cite{MM18_TR, MM19_HTSR_ICML, MM20_SDM}.
Operationally, $\alpha$ is determined by using the publicly-available \emph{WeightWatcher} tool~\cite{weightwatcher_package} to fit the Empirical Spectral Density (ESD) of $\mathbf{X}$, i.e., a histogram of the eigenvalues, call it $\rho(\lambda)$, to a truncated PL, 
\begin{equation}
\rho(\lambda)\sim\lambda^{\alpha},\;\;\lambda\le\lambda_{max}  ,
\end{equation}
where $\lambda_{max}$ \emph{is} the largest eigenvalue of $\mathbf{X}=\mathbf{W}^{T}\mathbf{W}$.
Each of these quantities is defined for a given layer $\mathbf{W}$ matrix.

For norm-based metrics, we use the average of the log norm to the appropriate power.
For example, consider the \emph{$\alpha$-Norm metric}, 
\begin{equation}
\label{eqn:sum_log_alpha_norm_alpha}
\sum\nolimits_l \log \Vert\mathbf{W}_l\Vert_{\alpha_l}^{\alpha_l} 
=
\sum\nolimits_l \alpha_l \log \Vert\mathbf{W}_l\Vert_{\alpha_l} .
\end{equation}
Informallly, this amounts to assuming that the layer weight matrices are statistically independent, in which case we can estimate the model complexity $\mathcal{C}$, or test accuracy, with a standard Product Norm (which resembles a data dependent VC complexity),
\begin{equation}
\mathcal{C}\sim\Vert\mathbf{W}_{1}\Vert\times\Vert\mathbf{W}_{2}\Vert \times \cdots \times \Vert\mathbf{W}_{L}\Vert ,
\end{equation}
where $\Vert\cdot\Vert$ is a matrix norm.   
The log Complexity,
\begin{equation}
\label{eqn:eqn:sum_log_norm}
\log\mathcal{C} \sim \log\Vert\mathbf{W}_{1}\Vert+\log\Vert\mathbf{W}_{2}\Vert + \cdots + \log\Vert\mathbf{W}_{L}\Vert  ,
\end{equation}
 takes the form of an Average Log Norm.
For the \emph{Frobenius norm metric} and \emph{Spectral norm metric}, we can use Eqn.~(\ref{eqn:eqn:sum_log_norm}) directly. 
(When taking $\log\Vert\mathbf{W}_{l}\Vert_{F}^{2}$, the $2$ comes down and out of the sum, and thus ignoring it only changes the metric by a constant factor.)
For the Weighted Alpha Norm, however, $\alpha_l$ varies from layer to layer, and so in Eqn.~(\ref{eqn:sum_log_alpha_norm_alpha}) it cannot be taken out of the sum.


For the Weighted Alpha Metric, we consider the average of $\hat{\alpha}$ over all layers, i.e., 
$ %$$
\sum_l \hat{\alpha}_l = \sum_l \alpha_l\log\lambda_{max,l}  .  
$ %$$
\michael{Be careful to get that right.}
The Weighted Alpha Metric was introduced previously~\cite{MM20_SDM}, where (on a much more limited set of data than we consider here) it was shown to correlate well with the trends in reported test accuracies of pretrained DNNs.
Based on this, in this paper, we introduce and evaluate the $\alpha$-Norm metric.
One would expect $\hat{\alpha}$ to approximate the log $\alpha$-Norm very well for $\alpha < 2$ and reasoably well for $\alpha\in[2,5]$~\cite{MM20_unpub_work}.




\paragraph{Spectral Analysis of Convolutional 2D Layers.}

There is some ambiguity in performing spectral analysis on Convolutional 2D (Conv2D) layers.  
A Conv2D layer can be represented as a 4-index tensor of dimension $(w,h,in\_ch,out\_ch)$, specified by an $(w\times h)$ filter (or kernel) and $in\_ch$ / $out\_ch$ input / output channels, respectively (usually $in\_ch\le out\_ch$). 
Typically, $w=h=k$,  giving $(k\times k)$ tensor slices, or \emph{pre-Activation Maps} $\mathbf{W}_{i,L}$ of dimension $(in\_ch\times out\_ch)$ each. 
%
There are at least three different approaches that have been advocated for applying the Singular Values Decomposition (SVD) to an Conv2D layer:
run an SVD on each of the pre-Activation Maps $\mathbf{W}_{i,L}$, yielding $(k\times k)$ sets of $M$ singular values; 
stack the feature maps into a single rectangular matrix of, say, dimension $((k\times k\times out\_ch)\times in\_ch)$, yielding $in\_ch$ singular values;
compute the 2D Fourier Transform (FFT) for each of the $(in\_ch, out\_ch)$ pairs, and run SVD on the resulting Fourier coeffients~\cite{Long2019}, leading to $\sim(k\times in\_ch\times out\_ch)$ non-zero singular values.
Each method has tradeoffs.  
In principle, the third method is mathematically sound, but it is computationally expensive. 
For our analysis, because we are performing tens of thousands of calculations, we select the first method, which is numerically the fastest and is easiest to reproduce.%
\footnote{We provide a Google Colab notebook where all results can be reproduced, with the option to redo the calculations with the third option for the SVD of the Conv2D.}


\paragraph{Normalization of Empirical Matrices.}  
Normalization is an important, if underappreciated, practical issue.
Importantly, the normalization of weight matrices does \emph{not} affect the Power Law fits because the Heavy Tailed exponent $\alpha$ (as well as other metrics such as the Stable Rank and MP Soft Rank~\cite{MM18_TR,MM19_HTSR_ICML}) is scale-invariant.
Norm-based metrics, however, do depend strongly on the scale of the weight matrix--\nred{that's the point.}
\nred{Indeed, early theoretical work by Bartlett suggests that the test accuracy depends strongly on the ``total size'' of the weight matrics.}
Typically, to apply RMT, we would usually define Correlation Matrix with $1/N$ normalization and assume that the variance of $\mathbf{X}$ is either unity or a known constant.% 
\footnote{For Heavy Tailed theorems, one typically needs a normalization such as \nred{$1/N^{\alpha-1}$. check this}}
Pretrained DNNs are typically initialized with random weight matrices $\mathbf{W}_{0}$, with the variance already normalized to $\sqrt{1/N}$, or some variant of this, e.g., the Glorot/Xavier normalization~\cite{GloRot}, or a $\sqrt{2/Nk^2}$ normalization for Convolutional 2D Layers.
We do not have conrol over the final empirical normalization of these models; and we do \emph{not} normalize (or renormalize) the Empirical Correlation Matrices, i.e., we use them as-is.
The only exception to this is that we do rescale the Conv2D pre-Activation Maps $\mathbf{W}_{i,L}$ by $k/\sqrt{2}$ so that they are on the same scale as the Linear layers.

