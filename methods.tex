\section{Methods}
\label{sxn:methods}

We assume we are given several pretrained Deep Neural Networks (DNNs), as part of a similar architecture.
We would like to estimate the trends in the reported test / generalization accuracy accross a series of similar archtectures.  
For example, below we compare the 8 pretrained models in the VGG series: (VGG11, VGG11\_BN$\cdots$ VGG19), with
and without Batch Normalization, trained on ImageNet, and widely available in the pyTorch distribution.

To do this, we will compute a variety of \emph{Complexity Metrics} based on the Product Norm of the layer weight matrics.
Note that unlike traditional ML approaches, however, we do not seek a bound on the complexity (i.e. test error), 
nor are we trying to evaluating a single model with differing hyperparmeters.  We wish to examine different models a 
common architecture series. And, also, compare different architectures themselves.  

Let us write the Energy Landscape (or optimization function) for a typical DNN with $L$ layers as
\begin{equation}
E_{DNN}=h_{L}(\mathbf{W}_{L}\times h_{L-1}(\mathbf{W}_{L-1}\times h_{L-2}(\cdots)+\mathbf{b}_{L-1})+\mathbf{b}_{L})  .
\label{eqn:dnn_energy}
\end{equation}

with activation functions $h_{l}(\cdot)$,  weight matrices $\mathbf{W}_{l}$, and the biases $\mathbf{b}_{l}$.

The model has been (or will be) trained on (unspecified) labeled data $\{d_{i},y_{i}\}\in\mathcal{D}$, 
using Backprop, by minimizing some (also unspecified) loss function $\mathcal{L}()$.  Moreover, we expect that most well trained,. production quality models will employ 1 or more forms of on regularization, such as Batch Normalization, Dropout, etc, and will also contain additional structure such as Skip Connections etc. Here, we ignore these details, and focus only on the weight matrices. 

Each layer contains by one or more layer 2D weight matrices $\mathbf{W}_{L}$, and/or the 2D feature maps $\mathbf{W}_{i,L}$ extracted from 2D Convolutional layers.  (For notational convenience, we may drop the $i$ and/or $i,l$ subscripts below.) We assume the layer weight matrices are statistically independent, allowing us to estimate the Complexity $\mathcal{C}$, or test accuracy, with a standard Product Norm, which resembles a data dependent VC complexity

\begin{equation}
\mathcal{C}\sim\Vert\mathbf{W}_{1}\Vert\times\Vert\mathbf{W}_{2}\Vert\cdots\Vert\mathbf{W}_{L}\Vert ,
\end{equation}
where $\mathbf{W}$ is an $(N\times M)$ weight matrix, with $N\ge M$, and 
 $\Vert\mathbf{W}\Vert$ is some matrix Norm.  Here, we will consider the following Norms:

\begin{itemize}
 \item Frobenius Norm: $\Vert\mathbf{W}\Vert^{2}_{F}=\Vert\mathbf{W}\Vert^{2}_{2}=\sum_{i,j}w^{2}_{i,j}$
 \item Spectral Norm:  $\Vert\mathbf{W}\Vert_{\infty}=\lambda_{max}$
 \item $\alpha-$Norm (or Shatten Norm) $\Vert\mathbf{W}\Vert^{\alpha}_{\alpha}=\sum_{i=1}^{M}\lambda^{\alpha}$,
\end{itemize}

where $\lambda_{i}$ is the $i-th$ eigenvalue of the correlation matrix $\mathbf{X}=\dfrac{1}{N}\mathbf{W}^{T}\mathbf{W}$, and $\lambda_{max}$ is the maximum eigenvalue.

The exponent $\alpha$ is the power law exponent that arise in our \emph{Theory of Heavy Tailed Self Regularization}, and is the determined by fitting the Empirical Spectral Density (ESD) of $\mathbf{X}$--i.e. a histogram of the eigenvalues--$\rho(\lambda)$ to a truncated power law

\begin{equation}
\rho(\lambda)\sim\lambda^{\alpha},\;\;\lambda\le\lambda_{max}
\end{equation}

We will also consider an approximate capacity metric, $\hat{\alpha}$, shown previously to correlate well the reported test accuracy of pretrained DNNs

\begin{itemize}
 \item $\hat{\alpha}=\alpha\log\lambda_{max}\approx\log\Vert\mathbf{W}\Vert^{\alpha}_{\alpha}$
\end{itemize}

which approxmiates the log $\alpha-$Norm for both Very Heavy Tailed weight matrices as well as finite size, Moderately Heavy Tailed $\mathbf{W}$.
---

CLARIFY HOW CONV2D lsyers are treated

----

\begin{eqnarray*}
\log\mathcal{C} &\sim& \log\bigg[\Vert\mathbf{W}_{1}\Vert\times\Vert\mathbf{W}_{2}\Vert\cdots\Vert\mathbf{W}_{L}\Vert\bigg]  \\
                &\sim& \bigg[\log\Vert\mathbf{W}_{1}\Vert+\log\Vert\mathbf{W}_{2}\Vert\cdots\log\Vert\mathbf{W}_{L}\Vert\bigg]  ,
\end{eqnarray*}
and we define the average log norm of the weight matrices as






