\begin{thebibliography}{10}

\bibitem{MM18_TR}
C.~H. Martin and M.~W. Mahoney.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock Technical Report Preprint: arXiv:1810.01075, 2018.

\bibitem{MM19_HTSR_ICML}
C.~H. Martin and M.~W. Mahoney.
\newblock Traditional and heavy-tailed self regularization in neural network
  models.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 4284--4293, 2019.

\bibitem{MM20_SDM}
C.~H. Martin and M.~W. Mahoney.
\newblock Heavy-tailed {U}niversality predicts trends in test accuracies for
  very large pre-trained deep neural networks.
\newblock In {\em Proceedings of the 20th SIAM International Conference on Data
  Mining}, 2020.

\bibitem{JC17_TR}
K.~Janocha and W.~M. Czarnecki.
\newblock On loss functions for deep neural networks in classification.
\newblock Technical Report Preprint: arXiv:1702.05659, 2017.

\bibitem{osmr}
{Sandbox for training convolutional networks for computer vision}.
\newblock \url{https://github.com/osmr/imgclsmob}.

\bibitem{weightwatcher_package}
{WeightWatcher}, 2018.
\newblock \url{https://pypi.org/project/WeightWatcher/}.

\bibitem{kdd20_sub_repo}
\url{https://github.com/CalculatedContent/ww-trends-2020}.

\bibitem{SornetteBook}
D.~Sornette.
\newblock {\em Critical phenomena in natural sciences: chaos, fractals,
  selforganization and disorder: concepts and tools}.
\newblock Springer-Verlag, Berlin, 2006.

\bibitem{nishimori01}
H.~Nishimori.
\newblock {\em Statistical Physics of Spin Glasses and Information Processing:
  An Introduction}.
\newblock Oxford University Press, Oxford, 2001.

\bibitem{SOC87}
P.~Bak, C.~Tang, and K.~Wiesenfeld.
\newblock Self-organized criticality: an explanation of $1/f$ noise.
\newblock {\em Physical Review Letters}, 59(4):381--384, 1987.

\bibitem{SOCat25yrs}
N.~W. Watkins, G.~Pruessner, S.~C. Chapman, N.~B. Crosby, and H.~J. Jensen.
\newblock 25 years of self-organized criticality: Concepts and controversies.
\newblock {\em Space Science Reviews}, 198:3--44, 2016.

\bibitem{HodMah20A_TR}
L.~Hodgkinson and M.~W. Mahoney.
\newblock Multiplicative noise and heavy tails in stochastic optimization,.
\newblock Technical Report Preprint: arXiv:2006.06293, 2020.

\bibitem{SorCon97}
D.~Sornette and R.~Cont.
\newblock Convergent multiplicative processes repelled from zero: Power laws
  and truncated power laws.
\newblock {\em Journal De Physique I}, 7:431--444, 1997.

\bibitem{SYYRP11}
W.~L. Shew, H.~Yang, S.~Yu, R.~Roy, and D.~Plenz.
\newblock Information capacity and transmission are maximized in balanced
  cortical networks with neuronal avalanches.
\newblock {\em The Journal of Neuroscience}, 31(1):55--63, 2011.

\bibitem{YKYP14}
S.~Yu, A.~Klaus, H.~Yang, and D.~Plenz.
\newblock Scale-invariant neuronal avalanche dynamics and the cut-off in size
  distributions.
\newblock {\em PLoS ONE}, 9(6):e99761, 2014.

\bibitem{CSN09_powerlaw}
A.~Clauset, C.~R. Shalizi, and M.~E.~J. Newman.
\newblock Power-law distributions in empirical data.
\newblock {\em SIAM Review}, 51(4):661--703, 2009.

\bibitem{ABP14}
J.~Alstott, E.~Bullmore, and D.~Plenz.
\newblock powerlaw: A python package for analysis of heavy-tailed
  distributions.
\newblock {\em PLoS ONE}, 9(1):e85777, 2014.

\bibitem{newman2005_zipf}
M.~E.~J. Newman.
\newblock Power laws, {P}areto distributions and {Z}ipf's law.
\newblock {\em Contemporary Physics}, 46:323--351, 2005.

\bibitem{imagenet}
O.~Russakovsky et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{pytorch}
A.~Paszke et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Annual Advances in Neural Information Processing Systems 32:
  Proceedings of the 2019 Conference}, pages 8024--8035, 2019.

\bibitem{BouchaudPotters03}
J.~P. Bouchaud and M.~Potters.
\newblock {\em Theory of Financial Risk and Derivative Pricing: From
  Statistical Physics to Risk Management}.
\newblock Cambridge University Press, 2003.

\bibitem{BP11}
J.~P. Bouchaud and M.~Potters.
\newblock Financial applications of random matrix theory: a short review.
\newblock In G.~Akemann, J.~Baik, and P.~Di Francesco, editors, {\em The Oxford
  Handbook of Random Matrix Theory}. Oxford University Press, 2011.

\bibitem{bun2017}
J.~Bun, J.-P. Bouchaud, and M.~Potters.
\newblock Cleaning large correlation matrices: tools from random matrix theory.
\newblock {\em Physics Reports}, 666:1--109, 2017.

\bibitem{TZ15}
N.~Tishby and N.~Zaslavsky.
\newblock Deep learning and the information bottleneck principle.
\newblock In {\em Proceedings of the 2015 IEEE Information Theory Workshop, ITW
  2015}, pages 1--5, 2015.

\bibitem{ST17_TR}
R.~Shwartz-Ziv and N.~Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock Technical Report Preprint: arXiv:1703.00810, 2017.

\bibitem{CWZZ17_TR}
Y.~Cheng, D.~Wang, P.~Zhou, and T.~Zhang.
\newblock A survey of model compression and acceleration for deep neural
  networks.
\newblock Technical Report Preprint: arXiv:1710.09282, 2017.

\bibitem{distiller}
{Intel Distiller package}.
\newblock \url{https://nervanasystems.github.io/distiller}.

\bibitem{Attn2017}
A.~Vaswani et~al.
\newblock Attention is all you need.
\newblock Technical Report Preprint: arXiv:1706.03762, 2017.

\bibitem{huggingface}
T.~Wolf et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock Technical Report Preprint: arXiv:1910.03771, 2019.

\bibitem{gpt2-xl}
{OpenAI GPT-2: 1.5B Release}.
\newblock \url{https://openai.com/blog/gpt-2-1-5b-release/}.

\bibitem{BHMM19}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  biasâ€“variance trade-off.
\newblock {\em Proc. Natl. Acad. Sci. USA}, 116:15849--15854, 2019.

\bibitem{resnet1000}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock Technical Report Preprint: arXiv:1603.05027, 2016.

\bibitem{EB01_BOOK}
A.~Engel and C.~P. L.~Van den Broeck.
\newblock {\em Statistical mechanics of learning}.
\newblock Cambridge University Press, New York, NY, USA, 2001.

\bibitem{MM17_TR}
C.~H. Martin and M.~W. Mahoney.
\newblock Rethinking generalization requires revisiting old ideas: statistical
  mechanics approaches and complex learning behavior.
\newblock Technical Report Preprint: arXiv:1710.09553, 2017.

\bibitem{BKPx20}
Y.~Bahri, J.~Kadmon, J.~Pennington, S.~Schoenholz, J.~Sohl-Dickstein, and
  S.~Ganguli.
\newblock Statistical mechanics of deep learning.
\newblock {\em Annual Review of Condensed Matter Physics}, 11:501--528, 2020.

\bibitem{MM19_KDD}
C.~H. Martin and M.~W. Mahoney.
\newblock Statistical mechanics methods for discovering knowledge from modern
  production quality neural networks.
\newblock In {\em Proceedings of the 25th Annual ACM SIGKDD Conference}, pages
  3239--3240, 2019.

\bibitem{NTS15}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In {\em Proceedings of the 28th Annual Conference on Learning
  Theory}, pages 1376--1401, 2015.

\bibitem{BFT17_TR}
P.~Bartlett, D.~J. Foster, and M.~Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock Technical Report Preprint: arXiv:1706.08498, 2017.

\bibitem{LMBx18_TR}
Q.~Liao, B.~Miranda, A.~Banburski, J.~Hidary, and T.~Poggio.
\newblock A surprising linear relationship predicts test performance in deep
  networks.
\newblock Technical Report Preprint: arXiv:1807.09659, 2018.

\bibitem{EJRUY20_TR}
G.~Eilertsen, D.~J{\"o}nsson, T.~Ropinski, J.~Unger, and A.~Ynnerman.
\newblock Classifying the classifier: dissecting the weight space of neural
  networks.
\newblock Technical Report Preprint: arXiv:2002.05688, 2020.

\bibitem{UKGBT20_TR}
T.~Unterthiner, D.~Keysers, S.~Gelly, O.~Bousquet, and I.~Tolstikhin.
\newblock Predicting neural network accuracy from weights.
\newblock Technical Report Preprint: arXiv:2002.11448, 2020.

\bibitem{CNNSVD}
H.~Sedghi, V.~Gupta, and P.~M. Long.
\newblock The singular values of convolutional layers.
\newblock Technical Report Preprint: arXiv:1805.10408, 2018.

\bibitem{GloBen10}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the 13th International Workshop on Artificial
  Intelligence and Statistics}, pages 249--256, 2010.

\end{thebibliography}
