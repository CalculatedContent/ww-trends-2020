\section{Discussion}


\paragraph{Comparison of VGG, ResNet, and DenseNet Architectures.}

Going beyond the goal of predicting trends in the quality of state-of-the-art neural networks without access to training or testing data, observations such as
the layer-wise observations we described in Figure~\ref{fig:3models-alpha-layers} can be understood in terms of architectural differences between VGG, ResNet, and DenseNet.
VGG resembles the traditional convolutional architectures, such as LeNet5, and consists of several [Conv2D-Maxpool-ReLu] blocks, followed by 3 large Fully Connected (FC) layers.
ResNet greatly improved on VGG by replacing the large FC layers, shrinking the Conv2D blocks, and introducing residual connections.
This optimized approach allows for greater accuracy with far fewer parameters, and ResNet models of up to 1000 layers have been trained~\cite{resnet1000}.

The efficiency and effectiveness of ResNet seems to be reflected in the smaller and more stable $\alpha\sim 2.0$, across nearly all layers, indicating that the inner layers are very well correlated and more strongly optimized.
This contrasts with the DenseNet models, which contains many connections between every layer.
These results (large $\alpha$, meaning that even a PL model is probably a poor fit) suggest that DenseNet has too many connections, diluting high quality interactions across layers, and leaving many layers very poorly~optimized.
%
Fine-scale measurements such as these enable us to form hypotheses as to the inner workings of DNN models, opening the door to an improved understanding of why DNNs work, as well as how to design better DNN models.
Correlation Flow and Scale Collapse are two such~examples.


\paragraph{Related work.}

Statistical mechanics has long had influence on DNN theory and practice~\cite{EB01_BOOK, MM17_TR, BKPx20}.
Our best-performing PL-based metrics are based on statistical mechanics via HT-SR Theory~\cite{MM17_TR, MM18_TR, MM19_HTSR_ICML, MM19_KDD, MM20_SDM}.
The way in which we (and HT-SR Theory) use statistical mechanics theory is quite different than the way it is more commonly formulated~\cite{EB01_BOOK, BKPx20}.
Going beyond idealized models, we use statistical mechanics in a broader sense, drawing upon techniques from quantitative finance, random matrix theory, and the statistical mechanics of heavy tailed and strongly correlated systems~\cite{BouchaudPotters03, SornetteBook, BP11, bun2017}.
There is also a large body of work in ML on using norm-based metrics to bound generalization error~\cite{NTS15, BFT17_TR, LMBx18_TR}.
This theoretical work aims to prove generalization bounds, and this applied work then uses these norms to construct regularizers to improve training.
Proving generalization bounds and developing new regularizers is very different than our focus of validating pretrained models.

Our work also has intriguing similarities and differences with work on understanding DNNs with the information bottleneck principle~\cite{TZ15,ST17_TR}, which posits that DNNs can be quantified by the mutual information between their layers and the input and output variables.
Most importantly, our approach does not require access to any data, while information measures used in the information bottleneck approach do require this.
Nevertheless, several results from HT-SR Theory, on which our metrics are based, have parallels in the information bottleneck approach.
Perhaps most notably, the quick transition from a \textsc{Random-like} phase to \textsc{Bulk+Spikes} phase, followed by slow transition to a \textsc{Heavy-Tailed} phase, as noted previously~\cite{MM18_TR}, is reminiscent of the dynamics on the Information Plane~\cite{ST17_TR}.

Finally, 
our work, starting in 2018 with the \texttt{WeightWatcher} tool~\cite{weightwatcher_package}, is the first to perform a detailed analysis of the weight matrices of DNNs~\cite{MM18_TR, MM19_HTSR_ICML, MM20_SDM}.
Subsequent to the initial version of this paper, we became aware of two other works that were posted to the in 2020 within weeks of the initial version of this paper~\cite{EJRUY20_TR,UKGBT20_TR}.
Both of these papers validate our basic result that one can gain substantial insight into model quality by examining weight matrices without access to any training or testing data.
However, both consider smaller models drawn from a much narrower range of applications than we consider.
Previous results in HT-SR Theory suggest that insights from these smaller models may not extend to the state-of-the-art CV and NLP models we consider.


\paragraph{Conclusions.}

We have developed and evaluated methods to predict trends in the quality of state-of-the-art neural networks---without access to training or testing data.
Our main methodology involved weight matrix meta-analysis, using the publicly-available \texttt{WeightWatcher} tool~\cite{weightwatcher_package}, and informed by the recently-developed HT-SR Theory~\cite{MM18_TR, MM19_HTSR_ICML, MM20_SDM}.
Prior to our work, it was not even obvious that norm-based metrics would perform well to predict trends in quality across models (as they are usually used within a given model or parameterized model class, e.g., to bound generalization error or to construct regularizers).
Our results are the first to demonstrate that they can be used for this important practical problem.
Our results also demonstrate that PL-based metrics perform better than norm-based metrics. 
This should not be surprising---at least to those familiar with the statistical mechanics of heavy tailed and strongly correlated systems~\cite{BouchaudPotters03, SornetteBook, BP11, bun2017}---since our use of PL exponents is designed to capture the idea that well-trained models capture information correlations over many size scales in the data.
Again, though, our results are the first to demonstrate this.
Our approach can also be used to provide fine-scale insight (rationalizing the flow of correlations or the collapse of size scale) throughout a network. 
Both Correlation Flow and Scale Collapse are important for improved diagnostics on pretrained models as well as for improved training methodologies.


\paragraph{Looking forward.}

More generally, our results suggest what a practical theory of DNNs should look like.
To see this, let's distinguish between two types of theories:
non-empirical or analogical theories, in which one creates, often from general principles, a very simple toy model that can be analyzed rigorously, and one then claims that the model is relevant to the system of interest; and 
semi-empirical theories, in which there exists a rigorous asymptotic theory, which comes with parameters, for the system of interest, and one then adjusts or fits those parameters to the finite non-asymptotic data, to make predictions about practical problems.
A drawback of the former approach is that it typically makes very strong assumptions, and the strength of those assumptions can limit the practical applicability of the theory.
Nearly all of the work on DNN theory focuses on the former type of theory.
Our approach focuses on the latter type of theory.
Our results, which are based on using sophisticated statistical mechanics theory and solving important practical DNN problems, suggests that the latter approach should be of interest more generally for those interested in developing a practical DNN~theory.


