\subsection{Comparing Hundreds of Models}
\label{sxn:all_cv_models}

\michael{Refine text in this subsection after we get the final version of all the tables and figures in the appendix.  Things to emphasize: we have a huge amount of data that we are summarizing very succicently; the table here is the tip of the iceberg; the results show our metrics are good, as a sanity check; they show that you can get some results in general, but looking at the details in the appendix show that results are more reliable within a given architecture series than across different architectures; point to out publicly-availab e repo; mention that we do Kendall-$\tau$ rank correlation also.}

We have performed a large-scale analysis of hundreds of publicly-available models.
This broader analysis is on a much larger set of CV and NLP models, with a more diverse set of architectures, that have been developed for a wider range of tasks; and it complements the previous more detailed analysis on CV and NLP models, where we have analyzed only a single architecture series at a time.
See the Supplementary Information
%~\ref{sxn:appendix} 
(and our publicly-available repo)
for details.
To quantify the relationship between quality metrics and the reported test error and/or accuracy metrics, we use ordinary least squares (similar results are seen with rank correlation metrics) to regress the metrics on the Top1 (and Top5) reported errors (as dependent variables).
These include Top5 errors for the ImageNet-1K model, percent error for the CIFAR-10/100, SVHN, CUB-200-2011 models, and Pixel accuracy (Pix.Acc.) and Intersection-Over-Union (IOU) for other models.
We regress them individually on each of the norm-based and PL-based metrics.


\begin{table}[t]
\small
\begin{center}
\begin{tabular}{|p{1.25in}|c|c|c|c|}
\hline
                        & $\log\Vert\cdot\Vert_{F}$ & $\log\Vert\cdot\Vert_{\infty}$ & $\hat{\alpha}$ & $\log\Vert\cdot\Vert^{\alpha}_{\alpha}$ \\
\hline
$R^{2}$ (mean)          & 0.63 &  0.55 & \textbf{0.64} & \textbf{0.64} \\
$R^{2}$ (std)           & 0.34 &  0.36 & \textbf{0.29} &          0.30 \\
\hline
$MSE$ (mean)            & 4.54 &  9.62 &          3.14 & \textbf{2.92} \\
$MSE$ (std)             & 8.69 & 23.06 &          5.14 & \textbf{5.00} \\
\hline
Kendall-$\tau$ (mean)   &  XXX &   XXX &           XXX &           XXX \\
Kendall-$\tau$ (std)    &  XXX &   XXX &           XXX &           XXX \\
\hline
\end{tabular}
\end{center}
\caption{Comparison of linear regression fits for different average Log Norm and Weighted Alpha metrics across 5 CV datasets, 17 architectures, covering 108 (out of over 400) different pretrained DNNs.  
         We include regressions only for architectures with five or more data points, and which are positively correlated with test error.
         These results can be readily reproduced using the Google Colab notebooks. 
         (See the Supplementary Information
         %~\ref{sxn:appendix} 
         for details.) 
\charles{THIS TABLE IS AUTOGENERATED BUT I CNT INCLUDE FOR SOME REASON}
        }
\label{table:results}
\end{table}

%why is this broken ?\input{table_3.tex}

Results are summarized in Table~\ref{table:results}.
For the mean, larger $R^{2}$ and smaller $MSE$ are desirable; and for the standard deviation, smaller values are desirable.
Taken as a whole, over the entire corpus of data, PL-based metrics are somewhat better for both the $R^{2}$ mean and standard deviation;
and PL-based metrics are much better for $MSE$ mean and standard deviation.
These and other results suggest our conclusions hold much more generally.

