\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sxn:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{weightwatcher_package}
\citation{kdd20_sub_repo}
\@writefile{toc}{\contentsline {paragraph}{The WeightWatcher Tool.}{3}{section*.1}}
\citation{EB01_BOOK}
\citation{MM17_TR}
\citation{BKPx20}
\citation{MM17_TR}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM19_KDD}
\citation{MM20_SDM}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{EB01_BOOK}
\citation{BKPx20}
\citation{BouchaudPotters03}
\citation{SornetteBook}
\citation{BP11}
\citation{bun2017}
\citation{NTS15}
\citation{BFT17_TR}
\citation{LMBx18_TR}
\citation{weightwatcher_package}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{EJRUY20_TR}
\citation{UKGBT20_TR}
\@writefile{toc}{\contentsline {paragraph}{Organization of this paper.}{4}{section*.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{4}{section.2}}
\newlabel{sxn:background}{{2}{4}{Background and Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Statistical mechanics theory for DNNs.}{4}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Norm-based capacity control theory.}{4}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Weight matrix analysis.}{4}{section*.5}}
\citation{JC17_TR}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{5}{section.3}}
\newlabel{sxn:methods}{{3}{5}{Methods}{section.3}{}}
\newlabel{eqn:dnn_energy}{{2}{5}{Methods}{equation.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{DNN Empirical Quality Metrics.}{5}{section*.6}}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{weightwatcher_package}
\citation{CSN09_powerlaw}
\citation{ABP14}
\citation{MM20_SDM}
\citation{MM20_unpub_work}
\newlabel{eqn:eqn:sum_log_norm}{{5}{6}{DNN Empirical Quality Metrics}{equation.3.5}{}}
\newlabel{eqn:sum_log_alpha_norm_alpha}{{7}{6}{DNN Empirical Quality Metrics}{equation.3.7}{}}
\citation{imagenet}
\citation{pytorch}
\citation{imagenet}
\citation{osmr}
\citation{pytorch}
\citation{MM20_unpub_work}
\@writefile{toc}{\contentsline {paragraph}{Convolutional Layers and Normalization issues.}{7}{section*.7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Comparison of CV models}{7}{section.4}}
\newlabel{sxn:cv}{{4}{7}{Comparison of CV models}{section.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Average Quality Metrics versus Reported Test Accuracies.}{7}{section*.8}}
\newlabel{fig:vgg-fnorm}{{1(a)}{8}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@fig:vgg-fnorm}{{(a)}{8}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:vgg-snorm}{{1(b)}{8}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@fig:vgg-snorm}{{(b)}{8}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{fig:vgg-walpha}{{1(c)}{8}{Subfigure 1(c)}{subfigure.1.3}{}}
\newlabel{sub@fig:vgg-walpha}{{(c)}{8}{Subfigure 1(c)\relax }{subfigure.1.3}{}}
\newlabel{fig:vgg-pnorm}{{1(d)}{8}{Subfigure 1(d)}{subfigure.1.4}{}}
\newlabel{sub@fig:vgg-pnorm}{{(d)}{8}{Subfigure 1(d)\relax }{subfigure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of Average Log Norm and Weighted Alpha quality metrics versus reported test accuracy for pretrained VGG models (with and without BN), trained on ImageNet, available in pyTorch (v1.4). Metrics fit by linear regression, RMSE reported. }}{8}{figure.1}}
\newlabel{fig:vgg-metrics}{{1}{8}{Comparison of Average Log Norm and Weighted Alpha quality metrics versus reported test accuracy for pretrained VGG models (with and without BN), trained on ImageNet, available in pyTorch (v1.4). Metrics fit by linear regression, RMSE reported}{figure.1}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Log Frobenius Norm, VGG }}}{8}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm, VGG }}}{8}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { Weighted Alpha, VGG }}}{8}{figure.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Log $\alpha $-Norm, VGG }}}{8}{figure.1}}
\@writefile{toc}{\contentsline {paragraph}{Variation in Data Set Size.}{8}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Layer Analysis: Metrics as a Function of Depth.}{8}{section*.10}}
\newlabel{fig:resnet-accuracy}{{2(a)}{9}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@fig:resnet-accuracy}{{(a)}{9}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{fig:resnet1k-accuracy}{{2(b)}{9}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@fig:resnet1k-accuracy}{{(b)}{9}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of Average $\alpha $-Norm quality metric ($\delimiter "426830A \qopname  \relax o{log}\delimiter "026B30D \mathbf  {X}\delimiter "026B30D _{\alpha }^{\alpha }\delimiter "526930B $) versus reported Top1 test accuracy for the ResNet and ResNet-1K pretrained (pyTorch) models. }}{9}{figure.2}}
\newlabel{fig:cv2-accuracy}{{2}{9}{Comparison of Average $\alpha $-Norm quality metric ($\langle \log \Vert \mathbf {X}\Vert _{\alpha }^{\alpha }\rangle $) versus reported Top1 test accuracy for the ResNet and ResNet-1K pretrained (pyTorch) models}{figure.2}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { ResNet, Log $\alpha $-Norm }}}{9}{figure.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { ResNet-1K, Log $\alpha $-Norm }}}{9}{figure.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces RMSE (smaller is better) for linear fits of quality metrics to reported Top1 test error for pretrained models in each architecture series. Column \# refers to number of models. VGG, ResNet, and DenseNet were pretrained on ImageNet, and ResNet-1K was pretrained on ImageNet-1K. }}{9}{table.1}}
\newlabel{table:cv-models}{{1}{9}{RMSE (smaller is better) for linear fits of quality metrics to reported Top1 test error for pretrained models in each architecture series. Column \# refers to number of models. VGG, ResNet, and DenseNet were pretrained on ImageNet, and ResNet-1K was pretrained on ImageNet-1K}{table.1}{}}
\citation{resnet1000}
\newlabel{fig:vgg-alpha-layers}{{3(a)}{10}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@fig:vgg-alpha-layers}{{(a)}{10}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{fig:resnet-alpha-layer}{{3(b)}{10}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@fig:resnet-alpha-layer}{{(b)}{10}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\newlabel{fig:densenet-alpha-layer}{{3(c)}{10}{Subfigure 3(c)}{subfigure.3.3}{}}
\newlabel{sub@fig:densenet-alpha-layer}{{(c)}{10}{Subfigure 3(c)\relax }{subfigure.3.3}{}}
\newlabel{fig:resnet_alpha_overlaid_depth}{{3(d)}{10}{Subfigure 3(d)}{subfigure.3.4}{}}
\newlabel{sub@fig:resnet_alpha_overlaid_depth}{{(d)}{10}{Subfigure 3(d)\relax }{subfigure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces PL exponent ($\alpha $) versus layer id, for the least and the most accurate models in VGG (a), ResNet (b), and DenseNet (c) series. (VGG is without BN; and note that the Y axes on each plot are different.) Subfigure (d) displays the ResNet models (b), zoomed in to $\alpha \in [1,5]$, and with the layer ids overlaid on the X-axis, from smallest to largest, to allow a more detailed analysis of the most strongly correlated layers. Notice that ResNet152 exhibits different and much more stable behavior of $\alpha $ across layers. This contrasts with how both VGG models gradually worsen in deeper layers and how the DenseNet models are much more erratic. In the text, this is interpreted in terms of \emph  {Correlation Flow}. }}{10}{figure.3}}
\newlabel{fig:3models-alpha-layers}{{3}{10}{PL exponent ($\alpha $) versus layer id, for the least and the most accurate models in VGG (a), ResNet (b), and DenseNet (c) series. (VGG is without BN; and note that the Y axes on each plot are different.) Subfigure (d) displays the ResNet models (b), zoomed in to $\alpha \in [1,5]$, and with the layer ids overlaid on the X-axis, from smallest to largest, to allow a more detailed analysis of the most strongly correlated layers. Notice that ResNet152 exhibits different and much more stable behavior of $\alpha $ across layers. This contrasts with how both VGG models gradually worsen in deeper layers and how the DenseNet models are much more erratic. In the text, this is interpreted in terms of \emph {Correlation Flow}}{figure.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { VGG }}}{10}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { ResNet }}}{10}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { DenseNet }}}{10}{figure.3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { ResNet (overlaid) }}}{10}{figure.3}}
\@writefile{toc}{\contentsline {paragraph}{Comparison of VGG, ResNet, and DenseNet Architectures.}{10}{section*.11}}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{BouchaudPotters03}
\citation{SornetteBook}
\citation{BP11}
\citation{bun2017}
\citation{BouchaudPotters03}
\citation{SornetteBook}
\citation{MM18_TR}
\citation{SornetteBook}
\citation{ST17_TR}
\citation{CWZZ17_TR}
\newlabel{fig:resnet204Dmaxev}{{4(a)}{11}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@fig:resnet204Dmaxev}{{(a)}{11}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{fig:resnet204Dalpha}{{4(b)}{11}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@fig:resnet204Dalpha}{{(b)}{11}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ResNet20, distilled with Group Regularization, as implemented in the \texttt  {distiller} (4D\_regularized\_5Lremoved) pretrained models. Log Spectral Norm ($\qopname  \relax o{log}\lambda _{max}$) and PL exponent ($\alpha $) for individual layers, versus layer id, for both baseline (before distillation, green) and fine-tuned (after distillation, red) pretrained models. }}{11}{figure.4}}
\newlabel{fig:resnet204D5L}{{4}{11}{ResNet20, distilled with Group Regularization, as implemented in the \texttt {distiller} (4D\_regularized\_5Lremoved) pretrained models. Log Spectral Norm ($\log \lambda _{max}$) and PL exponent ($\alpha $) for individual layers, versus layer id, for both baseline (before distillation, green) and fine-tuned (after distillation, red) pretrained models}{figure.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\lambda _{max}$ for ResNet20 layers}}}{11}{figure.4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha $ for ResNet20 layers}}}{11}{figure.4}}
\@writefile{toc}{\contentsline {paragraph}{Correlation Flow.}{11}{section*.12}}
\citation{distiller}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{Attn2017}
\citation{BouchaudPotters03}
\citation{SornetteBook}
\@writefile{toc}{\contentsline {paragraph}{Scale Collapse; or How Distillation May Break Models.}{12}{section*.13}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison of NLP Models}{12}{section.5}}
\newlabel{sxn:nlp}{{5}{12}{Comparison of NLP Models}{section.5}{}}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{huggingface}
\@writefile{toc}{\contentsline {paragraph}{What do large values of $\alpha $ mean?}{13}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{OpenAI GPT Models.}{13}{section*.15}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average value for the average Log Norm and Weighted Alpha metrics for pretrained OpenAI GPT and GPT2 models. Column \# refers to number of layers treated. Note that the averages do not include the first embedding layer(s) because they are not (implicitly) normalized. }}{14}{table.2}}
\newlabel{table:nlp}{{2}{14}{Average value for the average Log Norm and Weighted Alpha metrics for pretrained OpenAI GPT and GPT2 models. Column \# refers to number of layers treated. Note that the averages do not include the first embedding layer(s) because they are not (implicitly) normalized}{table.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Average Quality Metrics for GPT and GPT2.}{14}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Scale Collapse in Poorly Trained Models.}{14}{section*.17}}
\newlabel{fig:GPT-alpha-hist}{{5(a)}{15}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@fig:GPT-alpha-hist}{{(a)}{15}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{fig:GPT-snorm-hist}{{5(b)}{15}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@fig:GPT-snorm-hist}{{(b)}{15}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Histogram of PL exponents ($\alpha $) and Log Spectral Norms ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {W}\delimiter "026B30D _{\infty }$) for weight matrices from the OpenAI GPT and GPT2-small pretrained models.}}{15}{figure.5}}
\newlabel{fig:GPT-hist}{{5}{15}{Histogram of PL exponents ($\alpha $) and Log Spectral Norms ($\log \Vert \mathbf {W}\Vert _{\infty }$) for weight matrices from the OpenAI GPT and GPT2-small pretrained models}{figure.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{15}{figure.5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm ($\qopname \relax o{log}\delimiter "026B30D \mathbf {W}\delimiter "026B30D _{\infty }$)}}}{15}{figure.5}}
\@writefile{toc}{\contentsline {paragraph}{Layer Analysis: Correlation Flow and Scale Collapse in GPT and GPT2.}{15}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{GPT2: medium, large, xl.}{15}{section*.19}}
\citation{gpt2-xl}
\newlabel{fig:gpt-alpha-layer}{{6(a)}{16}{Subfigure 6(a)}{subfigure.6.1}{}}
\newlabel{sub@fig:gpt-alpha-layer}{{(a)}{16}{Subfigure 6(a)\relax }{subfigure.6.1}{}}
\newlabel{fig:gpt-snorm-layer}{{6(b)}{16}{Subfigure 6(b)}{subfigure.6.2}{}}
\newlabel{sub@fig:gpt-snorm-layer}{{(b)}{16}{Subfigure 6(b)\relax }{subfigure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces PL exponents ($\alpha $) (in (a)) and Log Spectral Norms ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {W}\delimiter "026B30D _{\infty }$) (in (b)) for weight matrices from the OpenAI GPT and GPT2-small pretrained models. (Note that the quantities being shown on each Y axis are different.) In the text, this is interpreted in terms of \emph  {Correlation Flow} and \emph  {Scale\nobreakspace  {}Collapse}. }}{16}{figure.6}}
\newlabel{fig:gpt-alpha-layers}{{6}{16}{PL exponents ($\alpha $) (in (a)) and Log Spectral Norms ($\log \Vert \mathbf {W}\Vert _{\infty }$) (in (b)) for weight matrices from the OpenAI GPT and GPT2-small pretrained models. (Note that the quantities being shown on each Y axis are different.) In the text, this is interpreted in terms of \emph {Correlation Flow} and \emph {Scale~Collapse}}{figure.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{16}{figure.6}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm ($\qopname \relax o{log}\delimiter "026B30D \mathbf {W}\delimiter "026B30D _{\infty }$)}}}{16}{figure.6}}
\newlabel{fig:gpt2-alpha-hist}{{7(a)}{16}{Subfigure 7(a)}{subfigure.7.1}{}}
\newlabel{sub@fig:gpt2-alpha-hist}{{(a)}{16}{Subfigure 7(a)\relax }{subfigure.7.1}{}}
\newlabel{fig:gpt2-pnorm-hist}{{7(b)}{16}{Subfigure 7(b)}{subfigure.7.2}{}}
\newlabel{sub@fig:gpt2-pnorm-hist}{{(b)}{16}{Subfigure 7(b)\relax }{subfigure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Histogram of PL exponents ($\alpha $) and Log Alpha Norm ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {X}\delimiter "026B30D _{\alpha }^{\alpha }$) for weight matrices from models of different sizes in the GPT2 architecture series. (Plots omit the first 2 (embedding) layers, because they are normalized differently giving anomalously large values.) }}{16}{figure.7}}
\newlabel{fig:gpt2-histograms}{{7}{16}{Histogram of PL exponents ($\alpha $) and Log Alpha Norm ($\log \Vert \mathbf {X}\Vert _{\alpha }^{\alpha }$) for weight matrices from models of different sizes in the GPT2 architecture series. (Plots omit the first 2 (embedding) layers, because they are normalized differently giving anomalously large values.)}{figure.7}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{16}{figure.7}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Alpha Norm}}}{16}{figure.7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Comparing Hundreds of Models}{16}{section.6}}
\newlabel{sxn:all_cv_models}{{6}{16}{Comparing Hundreds of Models}{section.6}{}}
\citation{weightwatcher_package}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{BouchaudPotters03}
\citation{SornetteBook}
\citation{BP11}
\citation{bun2017}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of linear regression fits for different average Log Norm and Weighted Alpha metrics across 5 CV datasets, 17 architectures, covering 108 (out of over 400) different pretrained DNNs. We include regressions only for architectures with five or more data points, and which are positively correlated with test error. These results can be readily reproduced using the Google Colab notebooks (see Appendix\nobreakspace  {}\ref  {sxn:appendix}). }}{17}{table.3}}
\newlabel{table:results}{{3}{17}{Comparison of linear regression fits for different average Log Norm and Weighted Alpha metrics across 5 CV datasets, 17 architectures, covering 108 (out of over 400) different pretrained DNNs. We include regressions only for architectures with five or more data points, and which are positively correlated with test error. These results can be readily reproduced using the Google Colab notebooks (see Appendix~\ref {sxn:appendix})}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{17}{section.7}}
\newlabel{sxn:conc}{{7}{17}{Conclusion}{section.7}{}}
\bibstyle{unsrt}
\bibdata{dnns}
\bibcite{MM18_TR}{1}
\bibcite{MM19_HTSR_ICML}{2}
\bibcite{MM20_SDM}{3}
\bibcite{weightwatcher_package}{4}
\bibcite{kdd20_sub_repo}{5}
\bibcite{EB01_BOOK}{6}
\bibcite{MM17_TR}{7}
\bibcite{BKPx20}{8}
\bibcite{MM19_KDD}{9}
\bibcite{BouchaudPotters03}{10}
\bibcite{SornetteBook}{11}
\bibcite{BP11}{12}
\bibcite{bun2017}{13}
\@writefile{toc}{\contentsline {paragraph}{Acknowledgements.}{18}{section*.20}}
\bibcite{NTS15}{14}
\bibcite{BFT17_TR}{15}
\bibcite{LMBx18_TR}{16}
\bibcite{EJRUY20_TR}{17}
\bibcite{UKGBT20_TR}{18}
\bibcite{MM20_unpub_work}{19}
\bibcite{imagenet}{20}
\bibcite{pytorch}{21}
\bibcite{osmr}{22}
\bibcite{resnet1000}{23}
\bibcite{ST17_TR}{24}
\bibcite{CWZZ17_TR}{25}
\bibcite{distiller}{26}
\bibcite{Attn2017}{27}
\bibcite{huggingface}{28}
\bibcite{gpt2-xl}{29}
\bibcite{CNNSVD}{30}
\bibcite{GloBen10}{31}
\citation{weightwatcher_package}
\citation{kdd20_sub_repo}
\@writefile{toc}{\contentsline {section}{\numberline {A}Reproducibility Appendix}{19}{appendix.A}}
\newlabel{sxn:appendix}{{A}{19}{Reproducibility Appendix}{appendix.A}{}}
\citation{CNNSVD}
\citation{GloBen10}
\citation{kdd20_sub_repo}
\citation{weightwatcher_package}
\citation{pytorch}
\citation{huggingface}
\citation{distiller}
\citation{osmr}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Reproducibility Considerations}{20}{subsection.A.1}}
\@writefile{toc}{\contentsline {paragraph}{SVD of Convolutional 2D Layers.}{20}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{Normalization of Empirical Matrices.}{20}{section*.23}}
\@writefile{toc}{\contentsline {paragraph}{Special consideration for NLP models.}{20}{section*.24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Reproducing Sections \ref  {sxn:cv} and \ref  {sxn:nlp} }{20}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Reproducing Figure\nobreakspace  {}\ref  {fig:resnet204D5L}, for the Distiller Model}{20}{subsection.A.3}}
\citation{osmr}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Jupyter notebooks used to reproduce all results in Sections\nobreakspace  {}\ref  {sxn:cv} and\nobreakspace  {}\ref  {sxn:nlp}.}}{21}{table.4}}
\newlabel{table:notebooks}{{4}{21}{Jupyter notebooks used to reproduce all results in Sections~\ref {sxn:cv} and~\ref {sxn:nlp}}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Datasets used}}{21}{table.5}}
\newlabel{table:datasets}{{5}{21}{Datasets used}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Reproducing Table\nobreakspace  {}\ref  {table:results} in Section\nobreakspace  {}\ref  {sxn:all_cv_models} }{21}{subsection.A.4}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Architectures used}}{22}{table.6}}
\newlabel{table:architectures}{{6}{22}{Architectures used}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces $MSE$ Results for all CV model regressions. }}{23}{table.7}}
\newlabel{table:MSEresults}{{7}{23}{$MSE$ Results for all CV model regressions}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces $R^{2}$ Results for all CV model regressions. }}{23}{table.8}}
\newlabel{table:R2results}{{8}{23}{$R^{2}$ Results for all CV model regressions}{table.8}{}}
\newlabel{fig:imagenet1k-alpha}{{8(a)}{24}{Subfigure 8(a)}{subfigure.8.1}{}}
\newlabel{sub@fig:imagenet1k-alpha}{{(a)}{24}{Subfigure 8(a)\relax }{subfigure.8.1}{}}
\newlabel{fig:cifar10.alpha}{{8(b)}{24}{Subfigure 8(b)}{subfigure.8.2}{}}
\newlabel{sub@fig:cifar10.alpha}{{(b)}{24}{Subfigure 8(b)\relax }{subfigure.8.2}{}}
\newlabel{fig:cifar100.alpha}{{8(c)}{24}{Subfigure 8(c)}{subfigure.8.3}{}}
\newlabel{sub@fig:cifar100.alpha}{{(c)}{24}{Subfigure 8(c)\relax }{subfigure.8.3}{}}
\newlabel{fig:svhn.alpha}{{8(d)}{24}{Subfigure 8(d)}{subfigure.8.4}{}}
\newlabel{sub@fig:svhn.alpha}{{(d)}{24}{Subfigure 8(d)\relax }{subfigure.8.4}{}}
\newlabel{fig:cub200.alpha}{{8(e)}{24}{Subfigure 8(e)}{subfigure.8.5}{}}
\newlabel{sub@fig:cub200.alpha}{{(e)}{24}{Subfigure 8(e)\relax }{subfigure.8.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces PL exponent $\alpha $ versus reported Top1 Test Accuracies for pretrained DNNs available for five different data sets. }}{24}{figure.8}}
\newlabel{fig:DSalphas}{{8}{24}{PL exponent $\alpha $ versus reported Top1 Test Accuracies for pretrained DNNs available for five different data sets}{figure.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ImageNet 1K}}}{24}{figure.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { CIFAR 10 }}}{24}{figure.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { CIFAR 100 }}}{24}{figure.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { SVHN }}}{24}{figure.8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces { CUB 200 }}}{24}{figure.8}}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{CSN09_powerlaw}
\citation{ABP14}
\citation{newman2005_zipf}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\citation{MM18_TR}
\citation{MM19_HTSR_ICML}
\citation{MM20_SDM}
\@writefile{toc}{\contentsline {section}{\numberline {B}Overview of Heavy-Tailed Self-Regularization}{25}{appendix.B}}
\newlabel{sxn:theory-review-appendix}{{B}{25}{Overview of Heavy-Tailed Self-Regularization}{appendix.B}{}}
\newlabel{eqn:eigenval_hist}{{8}{25}{Overview of Heavy-Tailed Self-Regularization}{equation.B.8}{}}
\newlabel{eqn:eigenval_pl}{{9}{25}{Overview of Heavy-Tailed Self-Regularization}{equation.B.9}{}}
\citation{SornetteBook}
\citation{HodMah20A_TR}
\citation{SorCon97}
