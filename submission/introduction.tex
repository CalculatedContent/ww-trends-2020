\vspace{-1mm}
\section{Introduction}
\label{sxn:intro}
%\vspace{-1mm}

A common problem in machine learning (ML) 
%and artificial intelligence (AI) 
is to evaluate the quality of a given model.
A popular way to accomplish this
%, in particular in academic environments, 
is to train a model and then evaluate its training/testing error.
There are many problems with this approach.
The training/testing curves give very limited insight into the overall properties of the model; 
they do not take into account the (often large human and CPU/GPU) time for hyperparameter fiddling;
they typically do not correlate with other properties of interest such as robustness or fairness or interpretability; 
and so on.
A less well-known problem, but one that is increasingly important, in particular in industrial-scale artificial intelligence (AI), arises when the model \emph{user} is not the model \emph{developer}.
Here, one may not have access to either the training data or the testing data.
Instead, one may simply be given a model that has already been trained---a \emph{pretrained model}---and need to use it ``as is,'' or to fine-tune and/or compress it and then use it.

Na\"{\i}vely---but in our experience commonly, among ML practitioners and ML theorists---if one does not have access to training or testing data, then one can say absolutely nothing about the quality of a ML model.
This may be true in worst-case theory, but models are used in practice, and there is a need for a \emph{practical theory} to guide that practice.
Moreover, if ML is to become an industrial process, then that process will become siloed: some groups will gather data, other groups will develop models, and other groups will use those models.
Users of models can not be expected to know the precise details of how models were built, the specifics of data that were used to train the model, what was the loss function or hyperparameter values, how precisely the model was regularized,~etc.

Moreover, for many large scale, practical applications, there is no obvious way to define an ideal test metric. 
For example, models that generate fake text or conversational chatbots may use a proxy, like perplexity, as a test metric.
In the end, however, they really require human evaluation. 
Alternatively, models that cluster user profiles, which are widely used in areas such as marketing and advertising, are unsupervised and have no obvious labels for comparison and/or evaluation.
In these and other areas, ML objectives can be poor proxies for downstream goals.

Most importantly, in industry, one faces unique practical problems such as: do we have enough data for this model? 
Indeed, high quality, labeled data can be very expensive to acquire, and this cost can make or break a project.
Methods that are developed and evaluated on any well-defined publicly-available coprus of data, no matter how large or diverse or interesting, are clearly not going to be well-suited to address problems such as this.
It is of great practical interest to have metrics to evaluate the quality of a trained model---in the absence of training/testing data and without any detailed knowledge of the training/testing process.  
We seek a practical theory for pretrained models which can predict how, when, and why such models can be expected to perform well or~poorly.

In this paper, we present and evaluate quality metrics for pretrained deep neural network (DNN) models, and we do so at scale.
We consider a large suite of hundreds of publicly-available models, mostly from computer vision (CV) and natural language processing (NLP).
%
By now, there are many such state-of-the-art models that are publicly-available, e.g., 
there are now hundreds of pretrained models in CV ($\ge 500$) and NLP ($\approx 100$).%
\footnote{When we began this work in 2018, there were fewer than tens of such models; now in 2020, there are hundreds of such models; and we expect that in a year or two there will be an order of magnitude or more of such models.}
These provide a large corpus of models that by some community standard are state-of-the-art.%
\footnote{Clearly, there is a selection bias or survivorship bias here---people tend not to make publicly-available their poorly-performing models---but these models are things in the world that (like social networks or the internet) can be analyzed for their properties.}
Importantly, all of these models have been trained by someone else and have been viewed to be of sufficient interest/quality to be made publicly-available; and, for all of these models, we have no access to training data or testing data, and we have no knowledge of the training/testing protocols. 

The \emph{quality metrics} we consider are based on the spectral properties of the layer weight matrics.
They are based on norms of weight matrices (such norms have been used in traditional statistical learning theory to bound capacity and construct regularizers) and/or parameters of power law (PL) fits of the eigenvalues of weight matrices (such PL fits are based on statistical mechanics approaches to DNNs).
Note that, while we use traditional norm-based and PL-based metrics, our goals are not the traditional goals.
Unlike more common ML approaches, \emph{we do not seek a bound on the generalization} (e.g., by evaluating training/test error during training), \emph{we do not seek a new regularizer}, and \emph{we do not aim to evaluate a single model} (e.g., as with hyperparmeter optimization).% 
\footnote{One could of course use these techniques to improve training, and we have been asked about that, but we are not interested in that here. Our main goal here is to use these techniques to evaluate properties of state-of-the-art pretrained DNN models.}
Instead, we want to examine different models across common architecture series, and we want to compare models between different architectures themselves, and in both cases, we ask:
\begin{quote}
\emph{Can we predict trends in the quality of pretrained DNN models without access to training or testing data?}  
\end{quote}


%\begin{itemize}[leftmargin=*]
%\item 
%First, 
%Motivated by practical problems, we formulate the question ``Can one predict trends in the quality of state-of-the-art neural networks without access to training or testing data''
%\item
%Second, 
%To answer this question, we analyze hundreds of publicly-available pretrained models, including older and current state-of-the-art models in CV and NLP.
%\end{itemize}

To answer this question, we analyze hundreds of publicly-available pretrained state-of-the-art CV and NLP models. 
Here is a summary of our main results.
\begin{itemize}
\item
\textbf{Norm-based metrics and well-trained models.}
Norm-based metrics do a reasonably good job at predicting quality trends in well-trained CV/NLP models.
\item
\textbf{Norm-based metrics and poorly-trained models.}
Norm-based metrics may give spurious results when applied to poorly-trained models (e.g., models trained without enough data, etc.), exhibiting \emph{Scale Collapse} for these models. 
\item 
\textbf{PL-based metrics and model quality.}
PL-based metrics do much better at predicting quality trends in pretrained CV/NLP models.  
They are quantitatively better at discriminating ``good-better-best'' trends, and qualitatively better at distinguishing ``good-versus-bad''~models.
\item 
\textbf{PL-based metrics and model diagnostics.}
PL-based metrics can also be used to characterize fine-scale model properties (including layer-wise \emph{Correlation Flow}) in well-trained and poorly-trained models, and they can be used to evaluate model enhancements (e.g., distillation, fine-tuning, etc.).
\end{itemize}

\noindent
We emphasize that our goal is a practical theory to predict trends in the quality of state-of-the-art DNN models, i.e., not to make a statement about every publicly-available model.
We have examined hundreds of models, and we identify general trends, but we also highlight interesting exceptions.
%%Several of the most interesting are described below.


\vspace{-1mm}
\paragraph{The WeightWatcher Tool.}

All of our computations were performed with the publicly-available \emph{WeightWatcher} tool (version 0.2.7)~\cite{weightwatcher_package}.
To be fully reproducible, we only examine publically-available, pretrained models, and we also provide all Jupyter and Google Colab notebooks used in an accompanying github repository~\cite{kdd20_sub_repo_anonymized}.
See Appendix~\ref{sxn:appendix} for details on how to reproduce all results.


\vspace{-1mm}
\paragraph{Organization of this paper.}

We start in Section~\ref{sxn:background} and Section~\ref{sxn:methods} with background and an overview of our general approach.
In Section \ref{sxn:cv}, we study three well-known widely-available DNN CV architectures (the VGG, ResNet, and DenseNet series of models); and we provide an illustration of our basic methodology, both to evaluate the different metrics against reported test accuracies and to use quality metrics to understand model properties.
Then, in Section~\ref{sxn:nlp}, we look at several variations of a popular NLP DNN architecture (the OpenAI GPT and GPT2 models); and we show how model quality and properties vary between several variants of GPT and GPT2, including how metrics behave similarly and differently.
Then, in Section \ref{sxn:all_cv_models}, we present results based on an analysis of hundreds of pretraind DNN models, showing how well each metric predicts the reported test accuracies, and how the PL-based metrics perform remarkably well.
Finally, in Section~\ref{sxn:conc}, we provide a brief discussion and conclusion.



