
In many practical applications, one works with deep neural networks (DNNs) models trained by someone else.
For such \emph{pretrained models}, one typically does not have access to training data or test data, 
and, moreover,  does not know many detals aout the  model, such as
the specifics of the training data, the loss function, the hyperparameter values, etc.
Given just one or more pretrained models, can one say anything about the expected performance or quality of the models ?
Here, we present and evaluate empirical quality metrics for pretrained neural network models at scale.
%%Our metrics are drawn from traditional statistical learning theory (e.g., norm-based capacity control metrics) as well as Heavy-Tailed Random Matrix Theory (HT-RMT), as used in the recently-developed Theory of Heavy-Tailed Self Regularization (e.g., fitted power law metrics used to characterize the degree of strong correlations in trained models).
%%The most promising are metrics drawn from traditional statistical learning theory (e.g., norm-based capacity control metrics) as well as metrics (e.g., fitted power law metrics used to characterize the degree of strong correlations in a system) derived from the recently-developed Theory of Heavy-Tailed Self Regularization (HT-SR).
%The most promising are norm-based metrics
% (such as those used to provide capacity control in traditional statistical learning theory) 
%and metrics based on fitting eigenvalue distributions to truncated power law (PL) distributions 
%(which derive from statistical mechanics, in particular the recently-developed Theory of Heavy-Tailed Self Regularization, and which characterize the strength of correlations in a DNN). 
%
Using the publicly-available \emph{WeightWatcher} tool, we analyze hundreds of publicly-available pretrained models, including older and current state-of-the-art models in computer vision (CV) and natural language processing (NLP).
We examine both familiar norm-based capacity control metrics (the Frobenius and  Spectral norms). as well as
new Power-Law (PL) based metrics, the PL exponent $\alpha$ and the Weighted Alpha ($\hat{\alpha}$) metrics,
from the recently-developed Theory of Heavy-Tailed Self Regularization (HT-SR).
We also introduce the $\alpha$ (Shatten) Norm metric.
We find that norm-based metrics correlate well with reported test accuracies of well-trained models
across nearly all CV architecture series.
PL-based metrics can also be used to characterize fine-scale properties of these models, and
we define the layer-wise \emph{Correlation Flow} as new quality assesment.
We also find that PL-based metrics do much better--quantitatively-- at discriminating among
a series of ``good-better-best'' models, and qualitatively better at discriminating ``good-versus-bad'' models.
On the other hand, we find that norm-based metrics can not distinguish ``good-versus-bad'' models--which, 
arguably is the point of needing quality metrics.  Indeed, they may give spurious results.
We show how poorly-trained (and/or poorly-finetuned) models may exhbit both \emph{Scale Collapse}
and unsually large PL exponents $\alpha\gg 6$.  In particular, we show that the \emph{WeightWatcher} tool
can be used to identify when a pretrained DNN has problems that can not be detected simply
by examining the test accuracy.




