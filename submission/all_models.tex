\section{Comparison of all 168 CV Models}
\label{sxn:all_cv_models}

\michael{Now we go back to the CV models and look at over 100 of them, and draw'
more broad conclusions about why alpha and the new norm is better, by looking at the MSE}

Here, we use the Weightwatcher tool to analyze over 100 pretrained computer vision (CV) models Pytorch, including image classification and segmentation models.  They have been pretrained on nine datasets, ImageNet-1K, and CIFAR-10, CIFAR-100, Street View House Numbers (SVHN), Caltech-UCSD Birds-200-2011 (CUB-200-2011), Pascal VOC2012, ADE20K, Cityscapes, and Common Objects in Context (COCO). The pretrained models and their accuracy metrics are summarized in the osmr github \charles{add link}

We provide more details in Appendix~\ref{sxn:appendix}.



In this paper, we propose that the Weightwatcher tool could be used to predict the trends in the generalization accuracy of deep neutral network without a test set. To test our proposition, we choose simple linear regression to analyze the relationship between the Weightwatcher metrics and the traditional accuracy metric obtained with a test set.
% (we avoid polynomial regressions as they are more prone to overfitting and does not make economic sense).
We regress the metrics on the Top1 (and Top5) reported errors (as dependent variables).  These include the Top5 errors for the ImageNet-1K mode, the percent error for the CIFAR-10/100, SVHN, CUB-200-2011 models, and the Pixel accuracy (Pix.Acc.) and Intersection-Over-Union (mIOU) for XXX.
We regress them individually on each of the Weightwatcher log Norm metrics, as described above.


\begin{table}[t]
\small
\begin{center}
%\begin{tabular}{|p{1in}|c|c|c|c|}
\begin{tabular}{|p{0.75in}|c|c|c|c|}
\hline
%    & Frobenius Norm & Spectral Norm & Weighted Alpha & Alpha-Norm \\
%    & $\langle\log\Vert\mathbf{W}\Vert_{F}\rangle$ & $\langle\log\Vert\mathbf{W}\Vert_{\infty}\rangle$ & $\langle\hat{\alpha}=\alpha\log\lambda_{max}\rangle$ & $\langle\log\Vert\mathbf{X}\Vert^{\alpha}_{\alpha}\rangle$ \\
Series        & $\log\Vert\cdot\Vert_{F}$ & $\log\Vert\cdot\Vert_{\infty}$ & $\hat{\alpha}$ & $\log\Vert\cdot\Vert^{\alpha}_{\alpha}$ \\
\hline
$R^{2}$ (mean) & 0.63 &0.55 &0.64 &0.64 \\
$R^{2}$ (std)  & 0.34 &0.36 &0.29 &0.30 \\
\hline
$MSE$ (mean)   & 4.54 &9.62 &3.14 &2.92 \\
$MSE$ (std)    & 8.69 &23.06 &5.14 &5.00 \\
\hline
\end{tabular}
\end{center}
\caption{Comparison of linear regression fits for different average log norm metrics across 5 computer vision datasets, 17 Architectures, covering 168 (out of 309) different pretrained DNNs.  We only conclude regressions for architecturs with 4 or more data points, and which are postively correlated with the test error.  These results can be readily reproduced using the Google Colab notebooks (see Appendix)}
\label{table:results}
\end{table}



