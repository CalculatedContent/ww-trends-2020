%KDD% \vspace{-1mm}
\section{Methods}
\label{sxn:methods}
%\vspace{-1mm}


Let us write the objective/optimization function (parameterized by $\mathbf{W}_{l}$s and $\mathbf{b}_{l}$s) for aE DNN with $L$ layers, activation functions $h_{l}(\cdot)$, and $N\times M$ weight matrices $\mathbf{W}_{l}$ and biases $\mathbf{b}_{l}$,~as \nred{the minimization of a general Loss Function $\mathcal{L}$ over  N training data instances and labels  $\{\mathbf{x}_{i},y_{i}\}\in\mathcal{D}$
For a typical supervised classification problem, one has
\begin{equation}
\underset{\mathbf{W}_{l},\mathbf{b}_{L}}{\text{argmin}}\;\sum_{i=1}^{N}\left(\mathcal{L}(E_{DNN}(\mathbf{x}_{i})-y_{i})\right)
\end{equation}
where the loss function $\mathcal{L}()$ can take on a myriad of forms\cite{Janocha2017}, and 
\begin{equation}
E_{DNN}=h_{L}(\mathbf{W}_{L}\times h_{L-1}(\mathbf{W}_{L-1}\times h_{L-2}(\cdots)+\mathbf{b}_{L-1})+\mathbf{b}_{L})
\label{eqn:dnn_energy}
\end{equation}
is what we call the Energy Landscape function.  Notice the $E_{DNN}$ does not explicitly depend on the data, but, rather,
maps data instances vectors $(\mathbf{x}_i)$ to predictions (i.e. labels).
Therefore, we can analyze $E_{DNN}$ absent of any training or test data. }


Each DNN layer contains one or more layer 2D  $N\times M$ weight matrices, $\mathbf{W}_{l}$, or pre-activation maps, $\mathbf{W}_{i,l}$, extracted from 2D Convolutional layers, and where $N > M$.% 
\footnote{We do not use intra-layer information from the models in our quality metrics, but (as we will describe) our metrics can be used to learn about intra-layer model properties.}
(We may drop the $i$ and/or $i,l$ subscripts below.)
See Appendix~\ref{sxn:appendix} for how we define the Conv2D layer matrixes and for our choices of normalization. 

Assume we are given several pretrained DNNs, e.g., as part of an architecture series.
The models have been trained and evaluated on the labeled data $\{\mathbf{x}_{i},y_{i}\}\in\mathcal{D}$, using standard techniques.  
The pretrained pytroch model files are publicly-available, and the test accuracies have been reported online.  
In this study, we do not have access to this data, and we have not trained any of the models ourselves. 
%nor have we re-evaluated the test accuracies.
We expect that most well-trained, production-quality models will employ one or more forms of regularization, such as Batch Normalization (BN), Dropout, etc., and many will also contain additional structure such as Skip Connections, etc. 
Here, we will ignore these details, and will focus only on the pretrained layer weight matrices $\mathbf{W}_{l}$.
Our methodological approach is thus similar to a statistical meta-analysis, common in biomedical research.


%KDD% \vspace{-1mm}
\paragraph{DNN Empirical Quality Metrics.}

Since we analyze pretrained models, most of our analysis involves analyzing DNN weight matrices.
The best performing empirical quality metrics depend on the norms and/or spectral properties of each weight matrix,
$\mathbf{W}$, and/or, equivalently, it's \emph{Empirical Correlation Matrix}: $\mathbf{X}=\mathbf{W}^{T}\mathbf{W}$.%

Here, we consider the following metrics.

\begin{itemize}
\item 
Frobenius Norm: $\Vert\mathbf{W}\Vert^{2}_{F}=\Vert\mathbf{X}\Vert_{F}=\sum_{i=1}^{M} \lambda_{i}$
\item 
Spectral Norm: $\Vert\mathbf{W}\Vert_{\infty}^{2}=\Vert\mathbf{X}\Vert_{\infty}=\lambda_{max}$
\item 
Weighted Alpha: $\hat{\alpha}=\alpha\log\lambda_{max}$
\item 
$\alpha$-Norm (or $\alpha$-Shatten Norm):%
\footnote{Notice $\Vert\mathbf{W}\Vert^{2\alpha}_{2\alpha}=\Vert\mathbf{X}\Vert^{\alpha}_{\alpha}$. We use $\mathbf{X}$ to emphasize that $\alpha$ depends on the ESD of $\mathbf{X}$.}
 $\Vert\mathbf{X}\Vert^{\alpha}_{\alpha}=\sum_{i=1}^{M}\lambda_{i}^{\alpha}$
\end{itemize}
Here, $\lambda_{i}$ is the $i^{th}$ eigenvalue of the $\mathbf{X}$, and $\lambda_{max}$ is the maximum eigenvalue.
Recall that the eigenvalues are squares of the singular values $\sigma_{i}$ of $\mathbf{W}$: $\lambda_{i}=\sigma^{2}_{i}$.
Also, note that we do \emph{not} normalize $\mathbf{X}$ by $1/N$.
See Appendix~\ref{sxn:appendix} for a discussion of this issue.
Observe that all four metrics can be computed easily from DNN weight matrices.

The first two metrics are well-known in ML; the last two deserve special mention.
The empirical parameter $\alpha$ is the Power Law (PL) exponent that arises in the recently-developed HT-SR Theory~\cite{MM18_TR, MM19_HTSR_ICML, MM20_SDM}.
The bottom line from HT-SR Theory is that smaller values of $\alpha$ should correspond to models with better correlation over multiple size scales and thus to better models; see Appendix~\ref{sxn:theory-review-appendix} for more details.
Operationally, $\alpha$ is determined by analyzing DNN weight matrices using the publicly-available \emph{WeightWatcher} tool~\cite{weightwatcher_package} to fit the Empirical Spectral Density (ESD) of $\mathbf{X}$, i.e., a histogram of the eigenvalues, call it $\rho(\lambda)$, to a truncated PL, 
\begin{equation}
\rho(\lambda)\sim\lambda^{\alpha},\;\;\lambda\in[\lambda_{min},\lambda_{max}].
\end{equation}
Each of these quantities is defined for a given layer $\mathbf{W}$ matrix and the PL fitting method. \cite{Clauset}
\nred{Here, where $\lambda_{max}$ \emph{is} the largest eigenvalue of $\mathbf{X}=\mathbf{W}^{T}\mathbf{W}$, 
and $\lambda_{min}$ is selected automaticlly to yield the best PL fit (which minimizes the K-S distance). }

For norm-based metrics, we use the average of the log norm, and to the appropriate power.
Informally, this amounts to assuming that the layer weight matrices are statistically independent, in which case we can estimate the model complexity $\mathcal{C}$, or test accuracy, with a standard Product Norm (which resembles a data dependent VC complexity),
\begin{equation}
\mathcal{C}\sim\Vert\mathbf{W}_{1}\Vert\times\Vert\mathbf{W}_{2}\Vert \times \cdots \times \Vert\mathbf{W}_{L}\Vert ,
\end{equation}
where $\Vert\cdot\Vert$ is a matrix norm.   
The log complexity,
\begin{equation}
\label{eqn:eqn:sum_log_norm}
\log\mathcal{C} \sim \log\Vert\mathbf{W}_{1}\Vert+\log\Vert\mathbf{W}_{2}\Vert + \cdots + \log\Vert\mathbf{W}_{L}\Vert  ,
\end{equation}
 takes the form of an average Log Norm.
For the \emph{Frobenius Norm metric} and \emph{Spectral Norm metric}, we can use Eqn.~(\ref{eqn:eqn:sum_log_norm}) directly.% 
\footnote{When taking $\log\Vert\mathbf{W}_{l}\Vert_{F}^{2}$, the $2$ comes down and out of the sum, and thus ignoring it only changes the metric by a constant factor.}


The \emph{Weighted Alpha metric} is an average of $\alpha_l$ over all layers $l \in \{1,\ldots,l\}$, weighted by the size, or scale, or each matrix,
%(and it approximates the average log $\alpha$-Shatten Norm metric),
\begin{equation}
\hat{\alpha} = \dfrac{1}{L}\sum_l \alpha_l\log\lambda_{max,l}\approx\langle\log\Vert\mathbf{X}\Vert_{\alpha}^{\alpha}\rangle    ,
\end{equation}
where $L$ is the total number of layer weight matrices.
The Weighted Alpha metric was introduced and justified previously~\cite{MM20_SDM}, where it was shown to correlate well with trends in reported test accuracies of pretrained DNNs, albeit on a much smaller and more limited set of models.

Based on this, in this paper, we introduce and evaluate the \emph{$\alpha$-Shatten Norm metric}.
Notice for the $\alpha$-Shatten Norm metric, however, $\alpha_l$ varies from layer to layer, and so in Eqn.~(\ref{eqn:sum_log_alpha_norm_alpha}) it can not be taken out of the sum:

\begin{equation}
\label{eqn:sum_log_alpha_norm_alpha}
\sum\nolimits_l \log \Vert\mathbf{X}_l\Vert_{\alpha_l}^{\alpha_l} 
=
\sum\nolimits_l \alpha_l \log \Vert\mathbf{X}_l\Vert_{\alpha_l} .
\end{equation}

\noindent
For small $\alpha$, the Weighted Alpha metric approximates the Log $\alpha$-Shatten norm, as can be shown with a statistical mechanics and random matrix theory derivation \cite{MM20_unpub_work}; and the Weighted Alpha and $\alpha$-Shatten norm metrics often behave like an improved, weighted average Log Spectral Norm.

To avoid confusion, let us clarify the relationship between $\alpha$ and $\hat{\alpha}$.  
We fit the ESD of the correlation matrix $\mathbf{X}$ to a truncated PL, parameterized by 2 values: the PL exponent $\alpha$, and the maximum eigenvalue $\lambda_{max}$.% 
\footnote{Technically, we also need the minimum eigenvalue $\lambda_{min}$, but this detail does not affect our analysis.}
The PL exponent $\alpha$ measures the amount of correlation in a DNN layer weight matrix $\mathbf{W}$. 
It is valid for $\lambda\le\lambda_{max}$, and it is scale-invariant, i.e., it does not depend on the normalization of $\mathbf{W}$ or $\mathbf{X}$.
The $\lambda_{max}$ is a measure of the size, or scale, of $\mathbf{W}$.
%
Multiplying each $\alpha$ by the corresponding $\log\lambda_{max}$ weighs ``bigger'' layers more, and averaging this product leads to a balanced, Weighted Alpha metric for the entire~DNN.


%KDD% \vspace{-1mm}
\paragraph{Convolutional Layers and Normalization issues.}
There are several technical issues, e.g., regarding spectral analysis of convolutional layers and normalization of empirical matrices, that are important for reproducibility of our results.
See Appendix~\ref{sxn:appendix} for a discussion.


