\section{Methods}
\label{sxn:methods}


Let us write the Energy Landscape (or optimization function, parameterized by $\mathbf{W}_{l}$s and $\mathbf{b}_{l}$s) for a DNN with $L$ layers, activation functions $h_{l}(\cdot)$, and $N\times M$ weight matrices $\mathbf{W}_{l}$ and biases $\mathbf{b}_{l}$,~as:
\begin{equation}
E_{DNN}=h_{L}(\mathbf{W}_{L}\times h_{L-1}(\mathbf{W}_{L-1}\times h_{L-2}(\cdots)+\mathbf{b}_{L-1})+\mathbf{b}_{L})  .
\label{eqn:dnn_energy}
\end{equation}
Each DNN layer contains one or more layer 2D  $N\times M$ weight matrices, $\mathbf{W}_{l}$, or pre-activation maps, $\mathbf{W}_{i,l}$, extracted from 2D Convolutional layers, and where $N > M$.% 
\footnote{We do not use intra-layer information from the models in our quality metrics, but (as we will describe) our metrics can be used to learn about intra-layer model properties.}
(We may drop the $i$ and/or $i,l$ subscripts below.)
See Appendix~\ref{sxn:appendix} for how we define the Conv2D layer matrixes and for our choices of normalization. 

Assume we are given several pretrained DNNs, e.g., as part of an architecture series.
The models have been trained and evaluated on labeled data $\{d_{i},y_{i}\}\in\mathcal{D}$, using standard techniques.  
The pretained pytroch model files are publicly-available, and the test accuracies have been reported online.  
In this study, we do not have access to this data, and we have not trained any of the models ourselves,
nor have we re-evaluated the test accuracies.
We expect that most well-trained, production-quality models will employ one or more forms of on regularization, such as Batch Normalization (BN), Dropout, etc., and many will also contain additional structure such as Skip Connections, etc. 
Here, we will ignore these details, and will focus only on the pretrained layer weight matrices $\mathbf{W}_{l}$.

\paragraph{DNN Empirical Quality Metrics.}

The best performing empirical quality metrics depend on the norms and/or spectral properties of each weight matrix,
$\mathbf{W}$ and/or, equivalently, it's \emph{Empirical Correlation Matrix}: $\mathbf{X}=\mathbf{W}^{T}\mathbf{W}$ .%

Here, we consider the following metrics.

\begin{itemize}
\item 
Frobenius Norm: $\Vert\mathbf{W}\Vert^{2}_{F}=\Vert\mathbf{X}\Vert_{F}=\sum_{i=1}^{M} \lambda_{i}$
\item 
Spectral Norm: $\Vert\mathbf{W}\Vert_{\infty}^{2}=\Vert\mathbf{X}\Vert_{\infty}=\lambda_{max}$
\item 
Weighted Alpha: $\hat{\alpha}=\alpha\log\lambda_{max}$
\item 
$\alpha$-Norm (or $\alpha$-Shatten Norm):%
\footnote{Notice $\Vert\mathbf{W}\Vert^{2\alpha}_{2\alpha}=\Vert\mathbf{X}\Vert^{\alpha}_{\alpha}$. We use $\mathbf{X}$ to emphasize that $\alpha$ depends on the ESD of $\mathbf{X}$.}
 $\Vert\mathbf{X}\Vert^{\alpha}_{\alpha}=\sum_{i=1}^{M}\lambda_{i}^{\alpha}$
\end{itemize}
Here, $\lambda_{i}$ is the $i^{th}$ eigenvalue of the $\mathbf{X}$, and $\lambda_{max}$ is the maximum eigenvalue.
Recall that the eigenvalues are square of the singular values $\sigma_{i}$ of $\mathbf{W}$: $\lambda_{i}=\sigma^{2}_{i}$.
Also, note that we do \emph{not} normalize $\mathbf{X}$ by $1/N$; see Appendix~\ref{sxn:appendix} for a discussion of this issue.

The first two norms are well-known in ML; the last two deserve special mention.
The empirical parameter $\alpha$ is the Power Law (PL) exponent that arises in the recently-developed HT-SR Theory~\cite{MM18_TR, MM19_HTSR_ICML, MM20_SDM}.
Operationally, $\alpha$ is determined by using the publicly-available \emph{WeightWatcher} tool~\cite{weightwatcher_package} to fit the Empirical Spectral Density (ESD) of $\mathbf{X}$, i.e., a histogram of the eigenvalues, call it $\rho(\lambda)$, to a truncated PL, 
\begin{equation}
\rho(\lambda)\sim\lambda^{\alpha},\;\;\lambda\le\lambda_{max}  ,
\end{equation}
where $\lambda_{max}$ \emph{is} the largest eigenvalue of $\mathbf{X}=\mathbf{W}^{T}\mathbf{W}$.
Each of these quantities is defined for a given layer $\mathbf{W}$ matrix.

For norm-based metrics, we use the average of the log norm, and to the appropriate power.
Informallly, this amounts to assuming that the layer weight matrices are statistically independent, in which case we can estimate the model complexity $\mathcal{C}$, or test accuracy, with a standard Product Norm (which resembles a data dependent VC complexity),
\begin{equation}
\mathcal{C}\sim\Vert\mathbf{W}_{1}\Vert\times\Vert\mathbf{W}_{2}\Vert \times \cdots \times \Vert\mathbf{W}_{L}\Vert ,
\end{equation}
where $\Vert\cdot\Vert$ is a matrix norm.   
The log complexity,
\begin{equation}
\label{eqn:eqn:sum_log_norm}
\log\mathcal{C} \sim \log\Vert\mathbf{W}_{1}\Vert+\log\Vert\mathbf{W}_{2}\Vert + \cdots + \log\Vert\mathbf{W}_{L}\Vert  ,
\end{equation}
 takes the form of an average Log Norm.
For the \emph{Frobenius norm metric} and \emph{Spectral norm metric}, we can use Eqn.~(\ref{eqn:eqn:sum_log_norm}) directly.% 
\footnote{When taking $\log\Vert\mathbf{W}_{l}\Vert_{F}^{2}$, the $2$ comes down and out of the sum, and thus ignoring it only changes the metric by a constant factor.}
For the $\alpha$-Shatten Norm metric, however, $\alpha_l$ varies from layer to layer, and so in Eqn.~(\ref{eqn:sum_log_alpha_norm_alpha}) it can not be taken out of the sum:

\begin{equation}
\label{eqn:sum_log_alpha_norm_alpha}
\sum\nolimits_l \log \Vert\mathbf{X}_l\Vert_{\alpha_l}^{\alpha_l} 
=
\sum\nolimits_l \alpha_l \log \Vert\mathbf{X}_l\Vert_{\alpha_l} .
\end{equation}


The \emph{Weighted Alpha metric} is an average of $\alpha_l$ over all layers $l \in \{1,\ldots,l\}$, weighted by the size, or scale, or each matrix
(and it approximates the average log $\alpha$-Shatten Norm metric),
\begin{equation}
\hat{\alpha} = \dfrac{1}{L}\sum_l \alpha_l\log\lambda_{max,l}\approx\langle\log\Vert\mathbf{X}\Vert_{\alpha}^{\alpha}\rangle    ,
\end{equation}
where $L$ is the total nuumber of layer weight matrices.
The Weighted Alpha metric was introduced previously~\cite{MM20_SDM}, where it was shown to correlate well with trends in reported test accuracies of pretrained DNNs, albeit on a limited set of models.
Based on this, in this paper, we introduce and evaluate the $\alpha$-Shatten Norm metric.
One expects $\hat{\alpha}$ approximates the average log $\alpha$-Shatten Norm very well for $\alpha < 2$ and reasonably well for $\alpha\in[2,5]$~\cite{MM20_unpub_work}.  

To avoid confusion, let us clarify the relationship between $\alpha$ and $\hat{\alpha}$.  
We fit the ESD of the correlation matrix $\mathbf{X}$ to a truncated PL, parameterized by 2 values: the PL exponent $\alpha$, and the maximum eigenvalue $\lambda_{max}$.  
(Technically, we also need the minimum eigenvalue $\lambda_{min}$, but this detail does not affect our analysis.)
The PL exponent $\alpha$ measures of the amount of correlation in a DNN layer weight matrix $\mathbf{W}$. 
It is valid for $\lambda<\lambda_{max}$, and it is scale-invariant, i.e., it does not depend on the normalization of $\mathbf{W}$ or $\mathbf{X}$.
The $\lambda_{max}$ is a measure of the size, or scale, of $\mathbf{W}$.
%
Multiplying each $\alpha$ by the corresponding $\log\lambda_{max}$ weighs ``bigger'' layers more, and averaging this product leads to a balanced, Weighted Alpha metric for the entire~DNN.%
\footnote{For small $\alpha$, this Weighted Alpha metric approximates the Log $\alpha$-Shatten norm, as can be shown with a statistical mechanics and random matrix theory derivation \cite{MM20_unpub_work}; and the Weighted Alpha and $\alpha$-Shatten norm metrics often behave like an improved, weighted average Log Spectral Norm, and may track this metric in some cases.}


\paragraph{Convolutional Layers and Normalization issues.}
There are several technical issues (regarding spectral analysis of convolutional layers and normalization of empirical matrices) that are important for reproducibility of our results.
See Appendix~\ref{sxn:appendix} for a discussion.


