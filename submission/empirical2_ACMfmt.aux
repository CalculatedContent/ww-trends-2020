\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sxn:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{weightwatcher_package}
\citation{kdd20_sub_repo}
\citation{EB01_BOOK,MM17_TR,BKPx20}
\citation{MM17_TR,MM18_TR,MM19_HTSR_ICML,MM19_KDD,MM20_SDM}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{EB01_BOOK,BKPx20}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\@writefile{toc}{\contentsline {paragraph}{The WeightWatcher Tool.}{2}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Organization of this paper.}{2}{section*.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{2}{section.2}}
\newlabel{sxn:background}{{2}{2}{Background and Related Work}{section.2}{}}
\citation{NTS15,BFT17_TR,LMBx18_TR}
\citation{weightwatcher_package}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{EJRUY20_TR,UKGBT20_TR}
\citation{JC17_TR}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{weightwatcher_package}
\citation{CSN09_powerlaw,ABP14}
\@writefile{toc}{\contentsline {paragraph}{Statistical mechanics theory for DNNs.}{3}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Norm-based capacity control theory.}{3}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Weight matrix analysis.}{3}{section*.7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}}
\newlabel{sxn:methods}{{3}{3}{Methods}{section.3}{}}
\newlabel{eqn:dnn_energy}{{2}{3}{Methods}{equation.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{DNN Empirical Quality Metrics.}{3}{section*.8}}
\citation{MM20_SDM}
\citation{MM20_unpub_work}
\citation{imagenet}
\citation{pytorch}
\citation{imagenet}
\citation{osmr}
\citation{pytorch}
\citation{MM20_unpub_work}
\newlabel{eqn:eqn:sum_log_norm}{{5}{4}{DNN Empirical Quality Metrics}{equation.3.5}{}}
\newlabel{eqn:sum_log_alpha_norm_alpha}{{7}{4}{DNN Empirical Quality Metrics}{equation.3.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional Layers and Normalization issues.}{4}{section*.9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Comparison of CV models}{4}{section.4}}
\newlabel{sxn:cv}{{4}{4}{Comparison of CV models}{section.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces RMSE (smaller is better) for linear fits of quality metrics to reported Top1 test error for pretrained models in each architecture series. Column \# refers to number of models. VGG, ResNet, and DenseNet were pretrained on ImageNet, and ResNet-1K was pretrained on ImageNet-1K. \relax }}{4}{table.caption.13}}
\newlabel{table:cv-models}{{1}{4}{RMSE (smaller is better) for linear fits of quality metrics to reported Top1 test error for pretrained models in each architecture series. Column \# refers to number of models. VGG, ResNet, and DenseNet were pretrained on ImageNet, and ResNet-1K was pretrained on ImageNet-1K. \relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Average Quality Metrics versus Reported Test Accuracies.}{4}{section*.10}}
\citation{resnet1000}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\citation{BouchaudPotters03,SornetteBook}
\citation{MM18_TR,SornetteBook}
\citation{ST17_TR}
\citation{CWZZ17_TR}
\@writefile{toc}{\contentsline {paragraph}{Variation in Data Set Size.}{5}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Layer Analysis: Metrics as a Function of Depth.}{5}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{Comparison of VGG, ResNet, and DenseNet Architectures.}{5}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{Correlation Flow.}{5}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Scale Collapse; or How Distillation May Break Models.}{5}{section*.20}}
\citation{distiller}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{Attn2017}
\citation{BouchaudPotters03,SornetteBook}
\citation{MM18_TR,MM19_HTSR_ICML}
\citation{MM18_TR,MM19_HTSR_ICML}
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison of NLP Models}{6}{section.5}}
\newlabel{sxn:nlp}{{5}{6}{Comparison of NLP Models}{section.5}{}}
\@writefile{toc}{\contentsline {paragraph}{What do large values of $\alpha $ mean?}{6}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{OpenAI GPT Models.}{6}{section*.22}}
\citation{huggingface}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average value for the average Log Norm and Weighted Alpha metrics for pretrained OpenAI GPT and GPT2 models. Column \# refers to number of layers treated. Note that the averages do not include the first embedding layer(s) because they are not (implicitly) normalized. \relax }}{7}{table.caption.23}}
\newlabel{table:nlp}{{2}{7}{Average value for the average Log Norm and Weighted Alpha metrics for pretrained OpenAI GPT and GPT2 models. Column \# refers to number of layers treated. Note that the averages do not include the first embedding layer(s) because they are not (implicitly) normalized. \relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Average Quality Metrics for GPT and GPT2.}{7}{section*.24}}
\@writefile{toc}{\contentsline {paragraph}{Scale Collapse in Poorly Trained Models.}{7}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Layer Analysis: Correlation Flow and Scale Collapse in GPT and GPT2.}{7}{section*.27}}
\citation{gpt2-xl}
\citation{weightwatcher_package}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of linear regression fits for different average Log Norm and Weighted Alpha metrics across 5 CV datasets, 17 architectures, covering 108 (out of over 400) different pretrained DNNs. We include regressions only for architectures with five or more data points, and which are positively correlated with test error. These results can be readily reproduced using the Google Colab notebooks (see Appendix\nonbreakingspace \ref  {sxn:appendix}). \relax }}{8}{table.caption.31}}
\newlabel{table:results}{{3}{8}{Comparison of linear regression fits for different average Log Norm and Weighted Alpha metrics across 5 CV datasets, 17 architectures, covering 108 (out of over 400) different pretrained DNNs. We include regressions only for architectures with five or more data points, and which are positively correlated with test error. These results can be readily reproduced using the Google Colab notebooks (see Appendix~\ref {sxn:appendix}). \relax }{table.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{GPT2: medium, large, xl.}{8}{section*.29}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Comparing Hundreds of Models}{8}{section.6}}
\newlabel{sxn:all_cv_models}{{6}{8}{Comparing Hundreds of Models}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}}
\newlabel{sxn:conc}{{7}{8}{Conclusion}{section.7}{}}
\bibstyle{ACM-Reference-Format}
\bibdata{dnns}
\bibcite{kdd20_sub_repo}{{1}{[n.d.]}{{kdd}}{{??}}}
\bibcite{distiller}{{2}{[n.d.]}{{dis}}{{??}}}
\bibcite{gpt2-xl}{{3}{[n.d.]}{{gpt}}{{??}}}
\bibcite{osmr}{{4}{[n.d.]}{{osm}}{{??}}}
\bibcite{weightwatcher_package}{{5}{2018}{{wei}}{{??}}}
\bibcite{BKPx20}{{6}{2020}{{Bahri et~al\unhbox \voidb@x \hbox {.}}}{{Bahri, Kadmon, Pennington, Schoenholz, Sohl-Dickstein, and Ganguli}}}
\bibcite{BFT17_TR}{{7}{2017}{{Bartlett et~al\unhbox \voidb@x \hbox {.}}}{{Bartlett, Foster, and Telgarsky}}}
\bibcite{BouchaudPotters03}{{8}{2003}{{Bouchaud and Potters}}{{Bouchaud and Potters}}}
\bibcite{BP11}{{9}{2011}{{Bouchaud and Potters}}{{Bouchaud and Potters}}}
\bibcite{bun2017}{{10}{2017}{{Bun et~al\unhbox \voidb@x \hbox {.}}}{{Bun, Bouchaud, and Potters}}}
\bibcite{CWZZ17_TR}{{11}{2017}{{Cheng et~al\unhbox \voidb@x \hbox {.}}}{{Cheng, Wang, Zhou, and Zhang}}}
\bibcite{EB01_BOOK}{{12}{2001}{{Engel and den Broeck}}{{Engel and den Broeck}}}
\bibcite{GloBen10}{{13}{2010}{{Glorot and Bengio}}{{Glorot and Bengio}}}
\bibcite{resnet1000}{{14}{2016}{{He et~al\unhbox \voidb@x \hbox {.}}}{{He, Zhang, Ren, and Sun}}}
\bibcite{LMBx18_TR}{{15}{2018}{{Liao et~al\unhbox \voidb@x \hbox {.}}}{{Liao, Miranda, Banburski, Hidary, and Poggio}}}
\bibcite{MM17_TR}{{16}{2017}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM18_TR}{{17}{2018}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM19_KDD}{{18}{2019a}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM19_HTSR_ICML}{{19}{2019b}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM20_SDM}{{20}{2020a}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{MM20_unpub_work}{{21}{2020b}{{Martin and Mahoney}}{{Martin and Mahoney}}}
\bibcite{NTS15}{{22}{2015}{{Neyshabur et~al\unhbox \voidb@x \hbox {.}}}{{Neyshabur, Tomioka, and Srebro}}}
\bibcite{pytorch}{{23}{2019}{{Paszke et~al\unhbox \voidb@x \hbox {.}}}{{Paszke et~al\unhbox \voidb@x \hbox {.}}}}
\bibcite{imagenet}{{24}{2015}{{Russakovsky et~al\unhbox \voidb@x \hbox {.}}}{{Russakovsky et~al\unhbox \voidb@x \hbox {.}}}}
\bibcite{CNNSVD}{{25}{2018}{{Sedghi et~al\unhbox \voidb@x \hbox {.}}}{{Sedghi, Gupta, and Long}}}
\bibcite{SornetteBook}{{26}{2006}{{Sornette}}{{Sornette}}}
\bibcite{Attn2017}{{27}{2017}{{Vaswani et~al\unhbox \voidb@x \hbox {.}}}{{Vaswani et~al\unhbox \voidb@x \hbox {.}}}}
\bibcite{huggingface}{{28}{2019}{{Wolf et~al\unhbox \voidb@x \hbox {.}}}{{Wolf et~al\unhbox \voidb@x \hbox {.}}}}
\@writefile{toc}{\contentsline {section}{References}{9}{section*.33}}
\citation{weightwatcher_package}
\citation{kdd20_sub_repo}
\citation{CNNSVD}
\citation{GloBen10}
\citation{kdd20_sub_repo}
\citation{weightwatcher_package}
\citation{pytorch}
\citation{huggingface}
\citation{distiller}
\citation{osmr}
\@writefile{toc}{\contentsline {section}{\numberline {A}Reproducibility Appendix}{10}{appendix.A}}
\newlabel{sxn:appendix}{{A}{10}{Reproducibility Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Reproducibility Considerations}{10}{subsection.A.1}}
\@writefile{toc}{\contentsline {paragraph}{SVD of Convolutional 2D Layers.}{10}{section*.34}}
\@writefile{toc}{\contentsline {paragraph}{Normalization of Empirical Matrices.}{10}{section*.35}}
\@writefile{toc}{\contentsline {paragraph}{Special consideration for NLP models.}{10}{section*.36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Reproducing Sections \ref  {sxn:cv} and \ref  {sxn:nlp} }{10}{subsection.A.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Jupyter notebooks used to reproduce all results in Sections\nonbreakingspace \ref  {sxn:cv} and\nonbreakingspace \ref  {sxn:nlp}.\relax }}{10}{table.caption.37}}
\newlabel{table:notebooks}{{4}{10}{Jupyter notebooks used to reproduce all results in Sections~\ref {sxn:cv} and~\ref {sxn:nlp}.\relax }{table.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Reproducing Figure\nonbreakingspace \ref  {fig:resnet204D5L}, for the Distiller Model}{10}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Reproducing Table\nonbreakingspace \ref  {table:results} in Section\nonbreakingspace \ref  {sxn:all_cv_models} }{10}{subsection.A.4}}
\citation{osmr}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Datasets used\relax }}{11}{table.caption.38}}
\newlabel{table:datasets}{{5}{11}{Datasets used\relax }{table.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Architectures used\relax }}{11}{table.caption.39}}
\newlabel{table:architectures}{{6}{11}{Architectures used\relax }{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces $MSE$ Results for all CV model regressions. \relax }}{11}{table.caption.41}}
\newlabel{table:MSEresults}{{7}{11}{$MSE$ Results for all CV model regressions. \relax }{table.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces $R^{2}$ Results for all CV model regressions. \relax }}{11}{table.caption.42}}
\newlabel{table:R2results}{{8}{11}{$R^{2}$ Results for all CV model regressions. \relax }{table.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Overview of Heavy-Tailed Self-Regularization}{11}{appendix.B}}
\newlabel{sxn:theory-review-appendix}{{B}{11}{Overview of Heavy-Tailed Self-Regularization}{appendix.B}{}}
\citation{CSN09_powerlaw,ABP14}
\citation{newman2005_zipf}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{SornetteBook}
\citation{HodMah20A_TR,SorCon97}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{6.25499pt}
\newlabel{tocindent2}{12.41998pt}
\newlabel{tocindent3}{0pt}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:vgg-fnorm}{{1(a)}{12}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@fig:vgg-fnorm}{{(a)}{12}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:vgg-snorm}{{1(b)}{12}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@fig:vgg-snorm}{{(b)}{12}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{fig:vgg-walpha}{{1(c)}{12}{Subfigure 1(c)}{subfigure.1.3}{}}
\newlabel{sub@fig:vgg-walpha}{{(c)}{12}{Subfigure 1(c)\relax }{subfigure.1.3}{}}
\newlabel{fig:vgg-pnorm}{{1(d)}{12}{Subfigure 1(d)}{subfigure.1.4}{}}
\newlabel{sub@fig:vgg-pnorm}{{(d)}{12}{Subfigure 1(d)\relax }{subfigure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of Average Log Norm and Weighted Alpha quality metrics versus reported test accuracy for pretrained VGG models (with and without BN), trained on ImageNet, available in pyTorch (v1.4). Metrics fit by linear regression, RMSE reported. \relax }}{12}{figure.caption.11}}
\newlabel{fig:vgg-metrics}{{1}{12}{Comparison of Average Log Norm and Weighted Alpha quality metrics versus reported test accuracy for pretrained VGG models (with and without BN), trained on ImageNet, available in pyTorch (v1.4). Metrics fit by linear regression, RMSE reported. \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Log Frobenius Norm, VGG }}}{12}{figure.caption.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm, VGG }}}{12}{figure.caption.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { Weighted Alpha, VGG }}}{12}{figure.caption.11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Log $\alpha $-Norm, VGG }}}{12}{figure.caption.11}}
\newlabel{fig:resnet-accuracy}{{2(a)}{12}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@fig:resnet-accuracy}{{(a)}{12}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{fig:resnet1k-accuracy}{{2(b)}{12}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@fig:resnet1k-accuracy}{{(b)}{12}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of Average $\alpha $-Norm quality metric ($\delimiter "426830A \qopname  \relax o{log}\delimiter "026B30D \mathbf  {X}\delimiter "026B30D _{\alpha }^{\alpha }\delimiter "526930B $) versus reported Top1 test accuracy for the ResNet and ResNet-1K pretrained (pyTorch) models. \relax }}{12}{figure.caption.12}}
\newlabel{fig:cv2-accuracy}{{2}{12}{Comparison of Average $\alpha $-Norm quality metric ($\langle \log \Vert \mathbf {X}\Vert _{\alpha }^{\alpha }\rangle $) versus reported Top1 test accuracy for the ResNet and ResNet-1K pretrained (pyTorch) models. \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { ResNet, Log $\alpha $-Norm }}}{12}{figure.caption.12}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { ResNet-1K, Log $\alpha $-Norm }}}{12}{figure.caption.12}}
\newlabel{eqn:eigenval_hist}{{8}{12}{Overview of Heavy-Tailed Self-Regularization}{equation.B.8}{}}
\newlabel{eqn:eigenval_pl}{{9}{12}{Overview of Heavy-Tailed Self-Regularization}{equation.B.9}{}}
\newlabel{fig:vgg-alpha-layers}{{3(a)}{13}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@fig:vgg-alpha-layers}{{(a)}{13}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{fig:resnet-alpha-layer}{{3(b)}{13}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@fig:resnet-alpha-layer}{{(b)}{13}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\newlabel{fig:densenet-alpha-layer}{{3(c)}{13}{Subfigure 3(c)}{subfigure.3.3}{}}
\newlabel{sub@fig:densenet-alpha-layer}{{(c)}{13}{Subfigure 3(c)\relax }{subfigure.3.3}{}}
\newlabel{fig:resnet_alpha_overlaid_depth}{{3(d)}{13}{Subfigure 3(d)}{subfigure.3.4}{}}
\newlabel{sub@fig:resnet_alpha_overlaid_depth}{{(d)}{13}{Subfigure 3(d)\relax }{subfigure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces PL exponent ($\alpha $) versus layer id, for the least and the most accurate models in VGG (a), ResNet (b), and DenseNet (c) series. (VGG is without BN; and note that the Y axes on each plot are different.) Subfigure (d) displays the ResNet models (b), zoomed in to $\alpha \in [1,5]$, and with the layer ids overlaid on the X-axis, from smallest to largest, to allow a more detailed analysis of the most strongly correlated layers. Notice that ResNet152 exhibits different and much more stable behavior of $\alpha $ across layers. This contrasts with how both VGG models gradually worsen in deeper layers and how the DenseNet models are much more erratic. In the text, this is interpreted in terms of \emph  {Correlation Flow}. \relax }}{13}{figure.caption.16}}
\newlabel{fig:3models-alpha-layers}{{3}{13}{PL exponent ($\alpha $) versus layer id, for the least and the most accurate models in VGG (a), ResNet (b), and DenseNet (c) series. (VGG is without BN; and note that the Y axes on each plot are different.) Subfigure (d) displays the ResNet models (b), zoomed in to $\alpha \in [1,5]$, and with the layer ids overlaid on the X-axis, from smallest to largest, to allow a more detailed analysis of the most strongly correlated layers. Notice that ResNet152 exhibits different and much more stable behavior of $\alpha $ across layers. This contrasts with how both VGG models gradually worsen in deeper layers and how the DenseNet models are much more erratic. In the text, this is interpreted in terms of \emph {Correlation Flow}. \relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { VGG }}}{13}{figure.caption.16}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { ResNet }}}{13}{figure.caption.16}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { DenseNet }}}{13}{figure.caption.16}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { ResNet (overlaid) }}}{13}{figure.caption.16}}
\newlabel{fig:resnet204Dmaxev}{{4(a)}{13}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@fig:resnet204Dmaxev}{{(a)}{13}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{fig:resnet204Dalpha}{{4(b)}{13}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@fig:resnet204Dalpha}{{(b)}{13}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces ResNet20, distilled with Group Regularization, as implemented in the \texttt  {distiller} (4D\_regularized\_5Lremoved) pretrained models. Log Spectral Norm ($\qopname  \relax o{log}\lambda _{max}$) and PL exponent ($\alpha $) for individual layers, versus layer id, for both baseline (before distillation, green) and fine-tuned (after distillation, red) pretrained models. \relax }}{13}{figure.caption.19}}
\newlabel{fig:resnet204D5L}{{4}{13}{ResNet20, distilled with Group Regularization, as implemented in the \texttt {distiller} (4D\_regularized\_5Lremoved) pretrained models. Log Spectral Norm ($\log \lambda _{max}$) and PL exponent ($\alpha $) for individual layers, versus layer id, for both baseline (before distillation, green) and fine-tuned (after distillation, red) pretrained models. \relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\lambda _{max}$ for ResNet20 layers}}}{13}{figure.caption.19}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha $ for ResNet20 layers}}}{13}{figure.caption.19}}
\newlabel{fig:gpt-alpha-layer}{{6(a)}{14}{Subfigure 6(a)}{subfigure.6.1}{}}
\newlabel{sub@fig:gpt-alpha-layer}{{(a)}{14}{Subfigure 6(a)\relax }{subfigure.6.1}{}}
\newlabel{fig:gpt-snorm-layer}{{6(b)}{14}{Subfigure 6(b)}{subfigure.6.2}{}}
\newlabel{sub@fig:gpt-snorm-layer}{{(b)}{14}{Subfigure 6(b)\relax }{subfigure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces PL exponents ($\alpha $) (in (a)) and Log Spectral Norms ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {W}\delimiter "026B30D _{\infty }$) (in (b)) for weight matrices from the OpenAI GPT and GPT2-small pretrained models. (Note that the quantities being shown on each Y axis are different.) In the text, this is interpreted in terms of \emph  {Correlation Flow} and \emph  {Scale\nonbreakingspace Collapse}. \relax }}{14}{figure.caption.28}}
\newlabel{fig:gpt-alpha-layers}{{6}{14}{PL exponents ($\alpha $) (in (a)) and Log Spectral Norms ($\log \Vert \mathbf {W}\Vert _{\infty }$) (in (b)) for weight matrices from the OpenAI GPT and GPT2-small pretrained models. (Note that the quantities being shown on each Y axis are different.) In the text, this is interpreted in terms of \emph {Correlation Flow} and \emph {Scale~Collapse}. \relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{14}{figure.caption.28}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm ($\qopname \relax o{log}\delimiter "026B30D \mathbf {W}\delimiter "026B30D _{\infty }$)}}}{14}{figure.caption.28}}
\newlabel{fig:gpt2-alpha-hist}{{7(a)}{14}{Subfigure 7(a)}{subfigure.7.1}{}}
\newlabel{sub@fig:gpt2-alpha-hist}{{(a)}{14}{Subfigure 7(a)\relax }{subfigure.7.1}{}}
\newlabel{fig:gpt2-pnorm-hist}{{7(b)}{14}{Subfigure 7(b)}{subfigure.7.2}{}}
\newlabel{sub@fig:gpt2-pnorm-hist}{{(b)}{14}{Subfigure 7(b)\relax }{subfigure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Histogram of PL exponents ($\alpha $) and Log Alpha Norm ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {X}\delimiter "026B30D _{\alpha }^{\alpha }$) for weight matrices from models of different sizes in the GPT2 architecture series. (Plots omit the first 2 (embedding) layers, because they are normalized differently giving anomalously large values.) \relax }}{14}{figure.caption.30}}
\newlabel{fig:gpt2-histograms}{{7}{14}{Histogram of PL exponents ($\alpha $) and Log Alpha Norm ($\log \Vert \mathbf {X}\Vert _{\alpha }^{\alpha }$) for weight matrices from models of different sizes in the GPT2 architecture series. (Plots omit the first 2 (embedding) layers, because they are normalized differently giving anomalously large values.) \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{14}{figure.caption.30}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Alpha Norm}}}{14}{figure.caption.30}}
\newlabel{fig:GPT-alpha-hist}{{5(a)}{14}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@fig:GPT-alpha-hist}{{(a)}{14}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{fig:GPT-snorm-hist}{{5(b)}{14}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@fig:GPT-snorm-hist}{{(b)}{14}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Histogram of PL exponents ($\alpha $) and Log Spectral Norms ($\qopname  \relax o{log}\delimiter "026B30D \mathbf  {W}\delimiter "026B30D _{\infty }$) for weight matrices from the OpenAI GPT and GPT2-small pretrained models.\relax }}{14}{figure.caption.26}}
\newlabel{fig:GPT-hist}{{5}{14}{Histogram of PL exponents ($\alpha $) and Log Spectral Norms ($\log \Vert \mathbf {W}\Vert _{\infty }$) for weight matrices from the OpenAI GPT and GPT2-small pretrained models.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{14}{figure.caption.26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Log Spectral Norm ($\qopname \relax o{log}\delimiter "026B30D \mathbf {W}\delimiter "026B30D _{\infty }$)}}}{14}{figure.caption.26}}
\newlabel{TotPages}{{16}{16}{}{page.16}{}}
