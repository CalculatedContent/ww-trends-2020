\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sxn:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{weightwatcher_package}
\citation{repo}
\citation{EB01_BOOK,MM17_TR,BKPx20}
\citation{MM17_TR,MM18_TR,MM19_HTSR_ICML,MM19_KDD,MM20_SDM}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{EB01_BOOK,BKPx20}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\@writefile{toc}{\contentsline {paragraph}{The WeightWatcher Tool.}{2}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Organization of this paper.}{2}{section*.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{2}{section.2}}
\newlabel{sxn:background}{{2}{2}{Background and Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Statistical mechanics theory for DNNs.}{2}{section*.7}}
\citation{NTS15,BFT17_TR,LMBx18_TR}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{weightwatcher_package}
\citation{MM20_SDM}
\citation{MM20_unpub_work}
\@writefile{toc}{\contentsline {paragraph}{Norm-based capacity control theory.}{3}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Practical problems need a practical theory.}{3}{section*.9}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{3}{section.3}}
\newlabel{sxn:methods}{{3}{3}{Methods}{section.3}{}}
\newlabel{eqn:dnn_energy}{{1}{3}{Methods}{equation.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{DNN Quality Metrics.}{3}{section*.10}}
\newlabel{eqn:sum_log_alpha_norm_alpha}{{3}{3}{DNN Quality Metrics}{equation.3.3}{}}
\newlabel{eqn:eqn:sum_log_norm}{{5}{3}{DNN Quality Metrics}{equation.3.5}{}}
\citation{imagenet}
\citation{pyTorch}
\citation{imagenet1k}
\citation{osmr}
\citation{pyTorchVgg}
\citation{MM20_unpub_work}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces RMSE (smaller is better) for linear fits of quality metrics to Reported Top1 Test Error for pretrained models in each architecture series. VGG, ResNet, and DenseNet were pretrained on ImageNet, and ResNet-1K was pretrained on ImageNet-1K. \relax }}{4}{table.caption.15}}
\newlabel{table:cv-models}{{1}{4}{RMSE (smaller is better) for linear fits of quality metrics to Reported Top1 Test Error for pretrained models in each architecture series. VGG, ResNet, and DenseNet were pretrained on ImageNet, and ResNet-1K was pretrained on ImageNet-1K. \relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional Layers and Normalization issues.}{4}{section*.11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Comparison of CV models}{4}{section.4}}
\newlabel{sxn:cv}{{4}{4}{Comparison of CV models}{section.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Average Quality Metrics versus Reported Test Accuracies.}{4}{section*.12}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:vgg-fnorm}{{1(a)}{4}{Subfigure 1(a)}{subfigure.1.1}{}}
\newlabel{sub@fig:vgg-fnorm}{{(a)}{4}{Subfigure 1(a)\relax }{subfigure.1.1}{}}
\newlabel{fig:vgg-snorm}{{1(b)}{4}{Subfigure 1(b)}{subfigure.1.2}{}}
\newlabel{sub@fig:vgg-snorm}{{(b)}{4}{Subfigure 1(b)\relax }{subfigure.1.2}{}}
\newlabel{fig:vgg-walpha}{{1(c)}{4}{Subfigure 1(c)}{subfigure.1.3}{}}
\newlabel{sub@fig:vgg-walpha}{{(c)}{4}{Subfigure 1(c)\relax }{subfigure.1.3}{}}
\newlabel{fig:vgg-pnorm}{{1(d)}{4}{Subfigure 1(d)}{subfigure.1.4}{}}
\newlabel{sub@fig:vgg-pnorm}{{(d)}{4}{Subfigure 1(d)\relax }{subfigure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of average log Norm empirical quality metrics versus reported test accuracy for pretrained VGG models (with and without Batch Normalization (BN)), trained on ImageNet, available in pyTorch (v1.x). Metrics fit by linear regression, RMSE reported. \relax }}{4}{figure.caption.13}}
\newlabel{fig:vgg-metrics}{{1}{4}{Comparison of average log Norm empirical quality metrics versus reported test accuracy for pretrained VGG models (with and without Batch Normalization (BN)), trained on ImageNet, available in pyTorch (v1.x). Metrics fit by linear regression, RMSE reported. \relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { Frobenius Norm, VGG }}}{4}{figure.caption.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { Spectral Norm, VGG }}}{4}{figure.caption.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { Weighted Alpha, VGG }}}{4}{figure.caption.13}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { $\alpha $-Norm, VGG }}}{4}{figure.caption.13}}
\newlabel{fig:resnet-accuracy}{{2(a)}{4}{Subfigure 2(a)}{subfigure.2.1}{}}
\newlabel{sub@fig:resnet-accuracy}{{(a)}{4}{Subfigure 2(a)\relax }{subfigure.2.1}{}}
\newlabel{fig:resnet1k-accuracy}{{2(b)}{4}{Subfigure 2(b)}{subfigure.2.2}{}}
\newlabel{sub@fig:resnet1k-accuracy}{{(b)}{4}{Subfigure 2(b)\relax }{subfigure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison of the avergage $\alpha $-Norm empirical quality metric $\delimiter "426830A \qopname  \relax o{log}\delimiter "026B30D \mathbf  {X}\delimiter "026B30D _{\alpha }^{\alpha }\delimiter "526930B $ versus reported Top 1 reported Test Accuracy for the ResNet and ResNet-1K pretrained (pyTorch) models. \relax }}{4}{figure.caption.14}}
\newlabel{fig:cv2-accuracy}{{2}{4}{Comparison of the avergage $\alpha $-Norm empirical quality metric $\langle \log \Vert \mathbf {X}\Vert _{\alpha }^{\alpha }\rangle $ versus reported Top 1 reported Test Accuracy for the ResNet and ResNet-1K pretrained (pyTorch) models. \relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { ResNet, $\alpha $-Norm }}}{4}{figure.caption.14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { ResNet-1K, $\alpha $-Norm }}}{4}{figure.caption.14}}
\citation{XXX-WEB-LINK}
\citation{resnet1000}
\citation{MM18_TR,MM19_HTSR_ICML,MM20_SDM}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\citation{MM18_TR,SornetteBook}
\@writefile{toc}{\contentsline {paragraph}{Variations in Data Set Size}{5}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Correlation Flow}{5}{section*.17}}
\newlabel{fig:vgg-alpha-layers}{{3(a)}{5}{Subfigure 3(a)}{subfigure.3.1}{}}
\newlabel{sub@fig:vgg-alpha-layers}{{(a)}{5}{Subfigure 3(a)\relax }{subfigure.3.1}{}}
\newlabel{fig:resnet-alpha-layer}{{3(b)}{5}{Subfigure 3(b)}{subfigure.3.2}{}}
\newlabel{sub@fig:resnet-alpha-layer}{{(b)}{5}{Subfigure 3(b)\relax }{subfigure.3.2}{}}
\newlabel{fig:densenet-alpha-layer}{{3(c)}{5}{Subfigure 3(c)}{subfigure.3.3}{}}
\newlabel{sub@fig:densenet-alpha-layer}{{(c)}{5}{Subfigure 3(c)\relax }{subfigure.3.3}{}}
\newlabel{fig:resnet_alpha_overlaid_depth}{{3(d)}{5}{Subfigure 3(d)}{subfigure.3.4}{}}
\newlabel{sub@fig:resnet_alpha_overlaid_depth}{{(d)}{5}{Subfigure 3(d)\relax }{subfigure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Correlation Flow: Power Law exponent $\alpha $ versus layer id, for the least and the most accurate models in VGG (a), ResNet (b), and DenseNet (c) series (VGG without BatchNorm, and the Y axes on each plot are different.) Figure (d) displays the ResNet models (b), zoomed in to $alpha\in [1,5]$, and with the layer ids overlaid on the X-axis, to allow a more detailed analysis for the most strongly correlated layers. Notice that ResNet152 exhibits different and much more stable behavior across layers This contrasts with how both VGG models gradually worsen in deeper layers and how the DenseNet models are much more erratic. \relax }}{5}{figure.caption.18}}
\newlabel{fig:3models-alpha-layers}{{3}{5}{Correlation Flow: Power Law exponent $\alpha $ versus layer id, for the least and the most accurate models in VGG (a), ResNet (b), and DenseNet (c) series (VGG without BatchNorm, and the Y axes on each plot are different.) Figure (d) displays the ResNet models (b), zoomed in to $alpha\in [1,5]$, and with the layer ids overlaid on the X-axis, to allow a more detailed analysis for the most strongly correlated layers. Notice that ResNet152 exhibits different and much more stable behavior across layers This contrasts with how both VGG models gradually worsen in deeper layers and how the DenseNet models are much more erratic. \relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { VGG }}}{5}{figure.caption.18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { ResNet }}}{5}{figure.caption.18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { DenseNet }}}{5}{figure.caption.18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { ResNet (overlaid) }}}{5}{figure.caption.18}}
\citation{CWZZ17_TR}
\citation{XXX-XXX}
\citation{Attn2017}
\newlabel{fig:resnet204Dmaxev}{{4(a)}{6}{Subfigure 4(a)}{subfigure.4.1}{}}
\newlabel{sub@fig:resnet204Dmaxev}{{(a)}{6}{Subfigure 4(a)\relax }{subfigure.4.1}{}}
\newlabel{fig:resnet204Dalpha}{{4(b)}{6}{Subfigure 4(b)}{subfigure.4.2}{}}
\newlabel{sub@fig:resnet204Dalpha}{{(b)}{6}{Subfigure 4(b)\relax }{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Correlation Flow. ResNet20, distilled with Group Regularization, as implemented in the \texttt  {distiller} (4D\_regularized\_5Lremoved) pre-trained models. Spectral Norm, $\qopname  \relax o{log}\lambda _{max}$, and, PL exponent, $\alpha $, for individual layer, versus layer id, for both baseline (before distillation, green) and finetuned (after distillation, red) pre-trained models. \relax }}{6}{figure.caption.20}}
\newlabel{fig:resnet204D5L}{{4}{6}{Correlation Flow. ResNet20, distilled with Group Regularization, as implemented in the \texttt {distiller} (4D\_regularized\_5Lremoved) pre-trained models. Spectral Norm, $\log \lambda _{max}$, and, PL exponent, $\alpha $, for individual layer, versus layer id, for both baseline (before distillation, green) and finetuned (after distillation, red) pre-trained models. \relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\lambda _{max}$ for ResNet20 layers}}}{6}{figure.caption.20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\alpha $ for ResNet20 layers}}}{6}{figure.caption.20}}
\@writefile{toc}{\contentsline {paragraph}{Correlation Flow: PL vs. Norm Metrics.}{6}{section*.19}}
\@writefile{toc}{\contentsline {paragraph}{Scale Collapse; or, Distillation may break models}{6}{section*.21}}
\newlabel{sxn:nlp}{{4}{6}{Scale Collapse; or, Distillation may break models}{section*.21}{}}
\citation{MM}
\citation{XXX-XXX}
\@writefile{toc}{\contentsline {paragraph}{What does large $\alpha $ mean?}{7}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{The differences between $\alpha $ and $\mathaccentV {hat}05E{\alpha }$.}{7}{section*.23}}
\@writefile{toc}{\contentsline {paragraph}{OpenAI GPT Models.}{7}{section*.24}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average value for the Average log Norm metrics and the Weighted Alpha metric for pretrainnd OpenAI GPT and GPT2 models. Column \# refers to number of layers treated. Note averages do not include the first embedding layer(s) because they are not (implicitly) normalized. \relax }}{7}{table.caption.25}}
\newlabel{table:nlp}{{2}{7}{Average value for the Average log Norm metrics and the Weighted Alpha metric for pretrainnd OpenAI GPT and GPT2 models. Column \# refers to number of layers treated. Note averages do not include the first embedding layer(s) because they are not (implicitly) normalized. \relax }{table.caption.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Average Empirical Quality Metrics for GPT and GPT2.}{7}{section*.26}}
\@writefile{toc}{\contentsline {paragraph}{Scale Collapse in Poorly Trained Models}{7}{section*.27}}
\citation{gpt2-xl}
\newlabel{fig:GPT-alpha-hist}{{5(a)}{8}{Subfigure 5(a)}{subfigure.5.1}{}}
\newlabel{sub@fig:GPT-alpha-hist}{{(a)}{8}{Subfigure 5(a)\relax }{subfigure.5.1}{}}
\newlabel{fig:GPT-snorm-hist}{{5(b)}{8}{Subfigure 5(b)}{subfigure.5.2}{}}
\newlabel{sub@fig:GPT-snorm-hist}{{(b)}{8}{Subfigure 5(b)\relax }{subfigure.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of PL exponents, $\alpha $, and log Spectral Norms, $(\qopname  \relax o{log}\delimiter "026B30D \mathbf  {W}\delimiter "026B30D _{\infty })$, for the OpenAI GPT and GPT2 (small) pretrained models.\relax }}{8}{figure.caption.28}}
\newlabel{fig:GPT-hist}{{5}{8}{Comparison of PL exponents, $\alpha $, and log Spectral Norms, $(\log \Vert \mathbf {W}\Vert _{\infty })$, for the OpenAI GPT and GPT2 (small) pretrained models.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Power law exponent ($\alpha $)}}}{8}{figure.caption.28}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Spectral Norm $(\qopname \relax o{log}\delimiter "026B30D \mathbf {W}\delimiter "026B30D _{\infty })$}}}{8}{figure.caption.28}}
\@writefile{toc}{\contentsline {paragraph}{Correlation Flow in GPT and GPT2.}{8}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{GPT2: medium, large, xl}{8}{section*.31}}
\newlabel{fig:gpt-alpha-layer}{{6(a)}{8}{Subfigure 6(a)}{subfigure.6.1}{}}
\newlabel{sub@fig:gpt-alpha-layer}{{(a)}{8}{Subfigure 6(a)\relax }{subfigure.6.1}{}}
\newlabel{fig:gpt-snorm-layer}{{6(b)}{8}{Subfigure 6(b)}{subfigure.6.2}{}}
\newlabel{sub@fig:gpt-snorm-layer}{{(b)}{8}{Subfigure 6(b)\relax }{subfigure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of Correlation Flow and Spectral Norm for OpenAI GPT and GPT2 \relax }}{8}{figure.caption.30}}
\newlabel{fig:gpt-alpha-layers}{{6}{8}{Comparison of Correlation Flow and Spectral Norm for OpenAI GPT and GPT2 \relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Power Law exponent $(\alpha )$}}}{8}{figure.caption.30}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Spectral Norm $(\qopname \relax o{log}\delimiter "026B30D \mathbf {W}\delimiter "026B30D _{\infty })$}}}{8}{figure.caption.30}}
\newlabel{fig:gpt2-alpha-hist}{{7(a)}{8}{Subfigure 7(a)}{subfigure.7.1}{}}
\newlabel{sub@fig:gpt2-alpha-hist}{{(a)}{8}{Subfigure 7(a)\relax }{subfigure.7.1}{}}
\newlabel{fig:gpt2-pnorm-hist}{{7(b)}{8}{Subfigure 7(b)}{subfigure.7.2}{}}
\newlabel{sub@fig:gpt2-pnorm-hist}{{(b)}{8}{Subfigure 7(b)\relax }{subfigure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces PL exponents ($\alpha $), log Spectral norm $(\qopname  \relax o{log}\delimiter "026B30D \mathbf  {W}\delimiter "026B30D _{\infty })$, and log Alpha norm $(\qopname  \relax o{log}\delimiter "026B30D \mathbf  {X}\delimiter "026B30D _{\alpha }^{\alpha })$ for different size models in the GPT2 architecture series. (Plots omit the first 2 (embedding) layers, because they are normalized differently giving anamolously large values.)\relax }}{8}{figure.caption.32}}
\newlabel{fig:gpt2-histograms}{{7}{8}{PL exponents ($\alpha $), log Spectral norm $(\log \Vert \mathbf {W}\Vert _{\infty })$, and log Alpha norm $(\log \Vert \mathbf {X}\Vert _{\alpha }^{\alpha })$ for different size models in the GPT2 architecture series. (Plots omit the first 2 (embedding) layers, because they are normalized differently giving anamolously large values.)\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {PL exponent ($\alpha $)}}}{8}{figure.caption.32}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {log Alpha Norm}}}{8}{figure.caption.32}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison of all 168 CV Models}{8}{section.5}}
\newlabel{sxn:all_cv_models}{{5}{8}{Comparison of all 168 CV Models}{section.5}{}}
\citation{BouchaudPotters03,SornetteBook,BP11,bun2017}
\bibstyle{ACM-Reference-Format}
\bibdata{dnns,gen_gap}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of linear regression fits for different average log norm metrics across 5 computer vision datasets, 17 Architectures, covering 168 (out of 309) different pretrained DNNs. We only conclude regressions for architecturs with 4 or more data points, and which are postively correlated with the test error. These results can be readily reproduced using the Google Colab notebooks (see Appendix)\relax }}{9}{table.caption.33}}
\newlabel{table:results}{{3}{9}{Comparison of linear regression fits for different average log norm metrics across 5 computer vision datasets, 17 Architectures, covering 168 (out of 309) different pretrained DNNs. We only conclude regressions for architecturs with 4 or more data points, and which are postively correlated with the test error. These results can be readily reproduced using the Google Colab notebooks (see Appendix)\relax }{table.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{9}{section.6}}
\newlabel{sxn:conc}{{6}{9}{Conclusion}{section.6}{}}
\@writefile{toc}{\contentsline {section}{Acknowledgments}{9}{section*.35}}
\citation{Long2019}
\citation{GloRot}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{10}{appendix.A}}
\newlabel{sxn:appendix}{{A}{10}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Reproducibility Considerations}{10}{subsection.A.1}}
\@writefile{toc}{\contentsline {paragraph}{SVD of Convolutional 2D Layers.}{10}{section*.36}}
\@writefile{toc}{\contentsline {paragraph}{Normalization of Empirical Matrices.}{10}{section*.37}}
\@writefile{toc}{\contentsline {paragraph}{Special consideration for NLP models.}{10}{section*.38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Reproducing Sections 4 and 5}{10}{subsection.A.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Jupyter notebooks used to reproduce Tables 1 and 2, and Figures 1,2,3,5,6, and 7.\relax }}{10}{table.caption.39}}
\newlabel{table:notebooks}{{4}{10}{Jupyter notebooks used to reproduce Tables 1 and 2, and Figures 1,2,3,5,6, and 7.\relax }{table.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Reproducing Section 3, Distiller Model}{10}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Reproducing Table 3, Section 6}{10}{subsection.A.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}XXX: PLACEHOLDER STUFF PROBABLY TO BE REMOVED}{10}{subsection.A.5}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{0pt}
\newlabel{tocindent1}{6.25499pt}
\newlabel{tocindent2}{12.41998pt}
\newlabel{tocindent3}{0pt}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Datasets used\relax }}{11}{table.caption.40}}
\newlabel{table:datasets}{{5}{11}{Datasets used\relax }{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Architectures used\relax }}{11}{table.caption.41}}
\newlabel{table:architectures}{{6}{11}{Architectures used\relax }{table.caption.41}{}}
\newlabel{fig:imagenet1k-alpha}{{8(a)}{11}{Subfigure 8(a)}{subfigure.8.1}{}}
\newlabel{sub@fig:imagenet1k-alpha}{{(a)}{11}{Subfigure 8(a)\relax }{subfigure.8.1}{}}
\newlabel{fig:cifar10.alpha}{{8(b)}{11}{Subfigure 8(b)}{subfigure.8.2}{}}
\newlabel{sub@fig:cifar10.alpha}{{(b)}{11}{Subfigure 8(b)\relax }{subfigure.8.2}{}}
\newlabel{fig:cifar100.alpha}{{8(c)}{11}{Subfigure 8(c)}{subfigure.8.3}{}}
\newlabel{sub@fig:cifar100.alpha}{{(c)}{11}{Subfigure 8(c)\relax }{subfigure.8.3}{}}
\newlabel{fig:svhn.alpha}{{8(d)}{11}{Subfigure 8(d)}{subfigure.8.4}{}}
\newlabel{sub@fig:svhn.alpha}{{(d)}{11}{Subfigure 8(d)\relax }{subfigure.8.4}{}}
\newlabel{fig:cub200.alpha}{{8(e)}{11}{Subfigure 8(e)}{subfigure.8.5}{}}
\newlabel{sub@fig:cub200.alpha}{{(e)}{11}{Subfigure 8(e)\relax }{subfigure.8.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces {\color  {blue}\sf  {[charles: Preliminary charts:]}} PL exponent $\alpha $ vs. reported Top1 Test Accuracies for pretrained DNNs available{\color  {blue}\sf  {[charles: ref]}} for 5 different data sets.\relax }}{11}{figure.caption.42}}
\newlabel{fig:DSalphas}{{8}{11}{\charles {Preliminary charts:} PL exponent $\alpha $ vs. reported Top1 Test Accuracies for pretrained DNNs available\charles {ref} for 5 different data sets.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ImageNet 1K}}}{11}{figure.caption.42}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces { CIFAR 10 }}}{11}{figure.caption.42}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces { CIFAR 100 }}}{11}{figure.caption.42}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces { SVHN }}}{11}{figure.caption.42}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces { CUB 200 }}}{11}{figure.caption.42}}
\newlabel{TotPages}{{11}{11}{}{page.11}{}}
