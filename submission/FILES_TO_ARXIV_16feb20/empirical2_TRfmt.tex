\documentclass[11pt]{article}

\addtolength{\textwidth}{1.4in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\addtolength{\topmargin}{-1.0in}
\addtolength{\textheight}{1.7in}
\newlength{\defbaselineskip}
\setlength{\defbaselineskip}{\baselineskip}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{framed}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{color}
\usepackage{xcolor}
\usepackage{paralist}

\usepackage{subfigure}

\usepackage{longtable} 
\usepackage{makecell}

\usepackage[multiple]{footmisc}
\usepackage[section]{placeins}

\usepackage{hyperref}
\hypersetup{
     colorlinks   = true,
     linkcolor    = blue,
     citecolor    = green
}


\newcommand{\fix}[1]{\textcolor{red}{#1}}
\newcommand{\comment}[1]{\textcolor{blue}{#1}}
\newcommand{\awk}[1]{\textcolor{green}{#1}}

\newcommand{\argmin}{\text{argmin}}
\newcommand{\Probab}[1]{\mbox{}{\bf{Pr}}\left[#1\right]}
\newcommand{\Expect}[1]{\mbox{}{\bf{E}}\left[#1\right]}
\newcommand{\ExpectBracket}[1]{\mbox{}\langle#1\rangle}

\newcommand {\nred}[1]{{\color{red}\sf{[#1]}}}
\newcommand {\ngreen}[1]{{\color{green}\sf{[#1]}}}
\newcommand {\ncyan}[1]{{\color{cyan}\sf{[#1]}}}
\newcommand {\michael}[1]{{\color{red}\sf{[michael: #1]}}}
\newcommand {\charles}[1]{{\color{blue}\sf{[charles: #1]}}}
\newcommand {\serena}[1]{{\color{orange}\sf{[charles: #1]}}}

\usepackage[normalem]{ulem}




\begin{document}

\title{%
Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data
}

\author{%
Charles H. Martin\thanks{Calculation Consulting, 8 Locksley Ave, 6B, San Francisco, CA 94122, \texttt{charles@CalculationConsulting.com}.} 
\and 
Tongsu (Serena) Peng\thanks{Calculation Consulting, 8 Locksley Ave, 6B, San Francisco, CA 94122, \texttt{serenapeng7@gmail.com}.}
\and
Michael W. Mahoney\thanks{ICSI and Department of Statistics, University of California at Berkeley, Berkeley, CA 94720, \texttt{mmahoney@stat.berkeley.edu}.}
}

\date{}
\maketitle



\begin{abstract}
\input{abstract_file}
\end{abstract}


\input{introduction}
\input{background}
\input{methods}
\input{cv_models}
\input{nlp_models}
\input{all_models}
\input{conclusion}

\vspace{-2mm}
\noindent
\paragraph{Acknowledgements.}
MWM would like to acknowledge ARO, DARPA, NSF, and ONR as well as the UC Berkeley BDD project and a gift from Intel for providing partial support of this work.
We would also like to thank Amir Khosrowshahi and colleagues at Intel for helpful discussion regarding the Group Regularization distillation technique.

   %% \bibliographystyle{unsrt}
   %% {\small
   %% \bibliography{dnns}
   %% }

{\small

\begin{thebibliography}{10}

\bibitem{weightwatcher_package}
{WeightWatcher}, 2018.
\newblock \url{https://pypi.org/project/WeightWatcher/}.

\bibitem{kdd20_sub_repo}
\url{https://github.com/CalculatedContent/ww-trends-2020}.

\bibitem{EB01_BOOK}
A.~Engel and C.~P. L.~Van den Broeck.
\newblock {\em Statistical mechanics of learning}.
\newblock Cambridge University Press, New York, NY, USA, 2001.

\bibitem{MM17_TR}
C.~H. Martin and M.~W. Mahoney.
\newblock Rethinking generalization requires revisiting old ideas: statistical
  mechanics approaches and complex learning behavior.
\newblock Technical Report Preprint: arXiv:1710.09553, 2017.

\bibitem{BKPx20}
Y.~Bahri, J.~Kadmon, J.~Pennington, S.~Schoenholz, J.~Sohl-Dickstein, and
  S.~Ganguli.
\newblock Statistical mechanics of deep learning.
\newblock {\em Annual Review of Condensed Matter Physics}, pages 000--000,
  2020.

\bibitem{MM18_TR}
C.~H. Martin and M.~W. Mahoney.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock Technical Report Preprint: arXiv:1810.01075, 2018.

\bibitem{MM19_HTSR_ICML}
C.~H. Martin and M.~W. Mahoney.
\newblock Traditional and heavy-tailed self regularization in neural network
  models.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 4284--4293, 2019.

\bibitem{MM19_KDD}
C.~H. Martin and M.~W. Mahoney.
\newblock Statistical mechanics methods for discovering knowledge from modern
  production quality neural networks.
\newblock In {\em Proceedings of the 25th Annual ACM SIGKDD Conference}, pages
  3239--3240, 2019.

\bibitem{MM20_SDM}
C.~H. Martin and M.~W. Mahoney.
\newblock Heavy-tailed {U}niversality predicts trends in test accuracies for
  very large pre-trained deep neural networks.
\newblock In {\em Proceedings of the 20th SIAM International Conference on Data
  Mining}, 2020.

\bibitem{BouchaudPotters03}
J.~P. Bouchaud and M.~Potters.
\newblock {\em Theory of Financial Risk and Derivative Pricing: From
  Statistical Physics to Risk Management}.
\newblock Cambridge University Press, 2003.

\bibitem{SornetteBook}
D.~Sornette.
\newblock {\em Critical phenomena in natural sciences: chaos, fractals,
  selforganization and disorder: concepts and tools}.
\newblock Springer-Verlag, Berlin, 2006.

\bibitem{BP11}
J.~P. Bouchaud and M.~Potters.
\newblock Financial applications of random matrix theory: a short review.
\newblock In G.~Akemann, J.~Baik, and P.~Di Francesco, editors, {\em The Oxford
  Handbook of Random Matrix Theory}. Oxford University Press, 2011.

\bibitem{bun2017}
J.~Bun, J.-P. Bouchaud, and M.~Potters.
\newblock Cleaning large correlation matrices: tools from random matrix theory.
\newblock {\em Physics Reports}, 666:1--109, 2017.

\bibitem{NTS15}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In {\em Proceedings of the 28th Annual Conference on Learning
  Theory}, pages 1376--1401, 2015.

\bibitem{BFT17_TR}
P.~Bartlett, D.~J. Foster, and M.~Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock Technical Report Preprint: arXiv:1706.08498, 2017.

\bibitem{LMBx18_TR}
Q.~Liao, B.~Miranda, A.~Banburski, J.~Hidary, and T.~Poggio.
\newblock A surprising linear relationship predicts test performance in deep
  networks.
\newblock Technical Report Preprint: arXiv:1807.09659, 2018.

\bibitem{MM20_unpub_work}
C.~H. Martin and M.~W. Mahoney.
\newblock Unpublished results, 2020.

\bibitem{imagenet}
O.~Russakovsky et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{pytorch}
A.~Paszke et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Annual Advances in Neural Information Processing Systems 32:
  Proceedings of the 2019 Conference}, pages 8024--8035, 2019.

\bibitem{osmr}
{Sandbox for training convolutional networks for computer vision}.
\newblock \url{https://github.com/osmr/imgclsmob}.

\bibitem{resnet1000}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock Technical Report Preprint: arXiv:1603.05027, 2016.

\bibitem{CWZZ17_TR}
Y.~Cheng, D.~Wang, P.~Zhou, and T.~Zhang.
\newblock A survey of model compression and acceleration for deep neural
  networks.
\newblock Technical Report Preprint: arXiv:1710.09282, 2017.

\bibitem{distiller}
{Intel Distiller package}.
\newblock \url{https://nervanasystems.github.io/distiller}.

\bibitem{Attn2017}
A.~Vaswani et~al.
\newblock Attention is all you need.
\newblock Technical Report Preprint: arXiv:1706.03762, 2017.

\bibitem{huggingface}
T.~Wolf et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock Technical Report Preprint: arXiv:1910.03771, 2019.

\bibitem{gpt2-xl}
{OpenAI GPT-2: 1.5B Release}.
\newblock \url{https://openai.com/blog/gpt-2-1-5b-release/}.

\bibitem{CNNSVD}
H.~Sedghi, V.~Gupta, and P.~M. Long.
\newblock The singular values of convolutional layers.
\newblock Technical Report Preprint: arXiv:1805.10408, 2018.

\bibitem{GloBen10}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the 13th International Workshop on Artificial
  Intelligence and Statistics}, pages 249--256, 2010.

\end{thebibliography}

}

\appendix
\input{appendix}

\end{document}
