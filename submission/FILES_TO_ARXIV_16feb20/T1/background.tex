\section{Background and Related Work}
\label{sxn:background}

Most theory for DNNs is applied to small toy models and assumes access to data.
There is very little work asking how to predict, in a theoretically-principled manner, the quality of large-scale state-of-the-art DNNs, and how to do so without access to training data or testing data or details of the training protocol, etc.
Our 
approach  
is, however, 
related to 
two  
other lines of work.


\paragraph{Statistical mechanics theory for DNNs.}

Statistical mechanics ideas have long had influence on DNN theory and practice~\cite{EB01_BOOK, MM17_TR, BKPx20}; and 
our best-performing metrics (those using fitted PL exponents) are based on statistical mechanics~\cite{MM17_TR, MM18_TR, MM19_HTSR_ICML, MM19_KDD, MM20_SDM}, in particular the recently-developed \emph{Theory of Heavy Tailed Self Regularization (HT-SR)}~\cite{MM18_TR, MM19_HTSR_ICML, MM20_SDM}.  
We emphasize that the way in which we (and HT-SR Theory) \emph{use} statistical mechanics theory is quite different than the way it is more commonly formulated.
Several very good overviews of the more common approach are available~\cite{EB01_BOOK, BKPx20}.
We \emph{use} statistical mechanics in a broader sense, drawing upon techniques from quantitative finance and random matrix theory.  
Thus, much more relevant for our methodological approach is older work of Bouchaud, Potters, Sornette, and coworkers~\cite{BouchaudPotters03, SornetteBook, BP11, bun2017} on the statistical mechanics of heavy tailed and strongly correlated systems.


\paragraph{Norm-based capacity control theory.}

There is also a large body of work on using norm-based metrics to bound generalization error~\cite{NTS15, BFT17_TR, LMBx18_TR}.
In this area, theoretical work aims to prove generalization bounds, and applied work uses these norms to construct regularizers to improve training.
While we do find that norms provide relatively good quality metrics, at least for distinguishing good-better-best among well-trained models, we are not interested in proving generalization bounds or developing new regularizers.


