\vspace{-1mm}
\section{Background and Related Work}
\label{sxn:background}
%\vspace{-1mm}

Most theory for DNNs is applied to small toy models and assumes access to data.
There is very little work asking how to predict, in a theoretically-principled manner, the quality of large-scale state-of-the-art DNNs, and how to do so without access to training data or testing data or details of the training protocol, etc.
Our 
approach  %%  work 
is, however, 
%% loosely 
related to 
two  %%  several 
other lines of work.


\vspace{-1mm}
\paragraph{Statistical mechanics theory for DNNs.}

Statistical mechanics ideas have long had influence on DNN theory and practice~\cite{EB01_BOOK, MM17_TR, BKPx20}; and 
our best-performing metrics (those using fitted PL exponents) are based on statistical mechanics~\cite{MM17_TR, MM18_TR, MM19_HTSR_ICML, MM19_KDD, MM20_SDM}, in particular the recently-developed \emph{Theory of Heavy Tailed Self Regularization (HT-SR)}~\cite{MM18_TR, MM19_HTSR_ICML, MM20_SDM}.  
We emphasize that the way in which we (and HT-SR Theory) \emph{use} statistical mechanics theory is quite different than the way it is more commonly formulated.
Several very good overviews of the more common approach are available~\cite{EB01_BOOK, BKPx20}.
We \emph{use} statistical mechanics in a broader sense, drawing upon techniques from quantitative finance and random matrix theory.  % (RMT).
Thus, much more relevant for our metholodogical approach is older work of Bouchaud, Potters, Sornette, and coworkers~\cite{BouchaudPotters03, SornetteBook, BP11, bun2017} on the statisical mechanics of heavy tailed and strongly correlated systems.
% Probably best not to cite at this point %\charles{, and current work (in progress)\cite{blog}}


\vspace{-1mm}
\paragraph{Norm-based capacity control theory.}

There is also a large body of work on using norm-based metrics to bound generalization error~\cite{NTS15, BFT17_TR, LMBx18_TR}.
In this area, theoretical work aims to prove generalization bounds, and applied work uses these norms to construct regularizers to improve training.
While we do find that norms provide relatively good quality metrics, at least for distinguishing good-better-best among well-trained models, we are not interested in proving generalization bounds or developing new regularizers.


%% \paragraph{Practical problems need a practical theory.}
%% \nred{Exiting ML theory does not address many practical AI problems that our techiques directly target.}
%% %There are many very practical problems in ML that are poorly addressed by existing theory and that either motivated our work or should be addressable by our techniques.
%% Here are several.
%% \begin{itemize}[leftmargin=*]
%% \item
%% \textbf{Simplicity, or lack of,  accuracy metrics.}
%% \michael{XXX.  Put in two or at most three sentences.}
%% \item
%% \textbf{Information leakage in the production pipeline.}
%% \michael{XXX.  Put in two or at most three sentences.}
%% \item
%% \textbf{Cost of acquring labeled data.}
%% \michael{XXX.  Put in two or at most three sentences.}
%% \end{itemize}
%% 
%% \charles{see comments in tex file}\michael{Let's touch base, not sure what to do.}
%% % we adress this above
%% %Importantly, there are many examples in ML where (as a practical matter) there is no reliable notion of accuracy, e.g., when generating fake text or realiztic chatbots, developing self driving cars, or just clusterting user profiles.
%% %\footnote{For example, current chatbots use perplexity as a proxy for passing a Turing test.}
%% %, and when distilling a reliable model in some way to obtain comparable training/test quality but that damages the model in some other subtle way. 
%% %\michael{That last example is awkward.  It would be good to have a better example and plant seeds for model distillation elsewhere.}
