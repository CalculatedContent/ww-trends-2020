\section{Background and Related Work}
\label{sxn:background}


To our knowledge, there is very little work on the particular question we are addressing: namely, how to predict, in a theoretically-principled manner, the quality of large-scale state-of-the-art NNs, and to do so without access to training data or testing data or details of the training protocol, etc.
Our work is, however, loosely related to several other lines of work, and we briefly discuss them here.

\paragraph{Statistical mechanical theory of NNs.}

Statistical mechanical ideas have long had influence on NN theory and practice~\cite{EB01_BOOK, MM17_TR, BKPx20}; and 
our best-performing metrics (i.e., those using fitted PL exponents and HT-SR Theory) are based on statistical mechanics ideas~\cite{MM17_TR, MM18_TR, MM19_HTSR_ICML, MM19_KDD, MM20_SDM}.
However, that the way in which we \emph{use} statistical mechanics theory is quite different than the way it is more commonly formulated.
Several very good overviews of the more common approach are available~\cite{EB01_BOOK, BKPx20}.
We will \emph{use} statistical mechanics theory more as it is used in quantitative finance.
Thus, much more relevant for our metholodogical approach is older work of Bouchaud, Potters, Sornette, and coworkers~\cite{BouchaudPotters03, SornetteBook, BP11, bun2017} on the statisical mechanics of heavy tailed and strongly correlated systems.


XXX.
Distinguish between what we will call a
\emph{phenomenological theory}
(that describes empirical relationship of phenomena to each other, in a way which is consistent with fundamental theory, but is not directly derived from that theory)
and what can be called a 
\emph{first principles theory} 
(that is applicable to tiny things but has no hope of scaling up).
\charles{it is the oppposite...ab initio theory scales quite well...it is the spherical cow models of physics and ML that do not scale. What we have is a semi-empirical theory.  We use real theory, but require empirical input, at least in the new stat mech work.  What we have introduced is a phenomenology, which to me is different from a semi-empirical or phenomenological theory  }
\michael{Let's touch base to get this right.}
\footnote{In most areas where there are complex highly-engineered systems (beyond complex AI/ML systems), one used phenomenological theory rather than first principles theory.  For example, one does not try to solve the Schr\"odinger equation if one is interested in building a bridge or an airplane.}


\paragraph{Norm-based capacity control theory.}

There is also a large body of work on using norm-based metrics to bound generalization error~\cite{NTS15, BFT17_TR, LMBx18_TR}.
In this area, theoretical work aims to prove generalization bounds, and applied work uses these norms as a regularization function to improve training.
While we do find that norms provide relatively good quality metrics, at least for distinguishing good-better-best among well-trained models, we are not interested in proving generalization bounds or improving training.


\paragraph{Practical problems poorly addressed by theory.}
There are many very practical problems in ML that are poorly addressed by existing theory and that either motivated our work or should be addressable by our techniques.
Here are several.
\begin{itemize}
\item
\textbf{Simplicity, or lack of,  accuracy metrics.}
XXX.
\item
\textbf{Information leakage in the production pipeline.}
XXX.
\item
\textbf{Cost of acquring labeled data.}
XXX.
\end{itemize}
Importantly, there are many examples in ML where (as a practical matter) there is no reliable notion of accuracy, e.g., when generating fake text, when developing self driving cars, or when creating realistic chatbots.%
\footnote{For example, current chatbots use perplexity as a proxy for passing a Turing test.}
%, and when distilling a reliable model in some way to obtain comparable training/test quality but that damages the model in some other subtle way. 
%\michael{That last example is awkward.  It would be good to have a better example and plant seeds for model distillation elsewhere.}
