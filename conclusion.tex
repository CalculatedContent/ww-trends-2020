\section{Conclusion}
\label{sxn:conc}

We have developed (based on strong theory) and evaluated (on a large corpus of publicly-available pretrained models from CV and NLP) methods to predict trends in the quality of state-of-the-art neural networks---without access to training or testing data.
Prior to our work, it was not obvious that norm-based metrics would perform well to predict trends \emph{across} models (as they are usually used \emph{within} a given model or parameterized model class, e.g., to bound generalization error or to construct regularizers), and our results are the first to demonstrate that they can be used for this important practical problem.
That PL-based metrics perform better (than norm-based metrics) should not be surprising---at least to those familiar with the statisical mechanics of heavy tailed and strongly correlated systems~\cite{BouchaudPotters03, SornetteBook, BP11, bun2017}, since our use of PL exponents is designed to capture the idea that well-trained models capture correlations over many size scales in the data), but again,  our results are the first to demonstrate this.
It is gratifying, also, that PL exponents can be used to provide fine-scale insight such as rationalizing the flow of correlations through a network. 

We conclude with a few thoughts on what a \emph{practical theory} of DNNs should look like.
%
We distinguish between what we will call a
\emph{phenomenological theory}
(that describes empirical relationship of phenomena to each other, in a way which is consistent with fundamental theory, but is not directly derived from that theory)
and what can be called a 
\emph{first principles theory} 
(that is applicable to toy models, but that does not scale up to realistic systems if one includes realistic apsects of realistic systems).
%
For most complex highly-engineered systems (aside from complex AI/ML systems), one \emph{uses} phenomenological theory rather than first principles theory. 
(One does not try to solve the Schr\"odinger equation if one is interested in building a bridge or flying an airplane.)
%
Our results, which are based on our \emph{use} of sophisticated statistical mechanics theory to solve an important practical DNN problems, suggests that this approach should be of interest more generally for those interested in developing a practical DNN theory.

\charles{it is the oppposite...ab initio theory scales quite well...it is the spherical cow models of physics and ML that do not scale. What we have is a semi-empirical theory.  We use real theory, but require empirical input, at least in the new stat mech work.  What we have introduced is a phenomenology, which to me is different from a semi-empirical or phenomenological theory  }
\michael{Let's touch base to get this right.}


