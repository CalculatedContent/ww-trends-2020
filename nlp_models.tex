\section{Comparison of NLP Transformer Models}
\label{sxn:nlp}

In this section, we examine quality metrics described in Section~\ref{sxn:methods} for several NLP model architectures.
%
%In particular, 
Within the past two years, nearly 100 open source, pre-trained NLP DNNs based on the revolutionary Transformer architecture have emerged.
These include variants of BERT, Transformer-XML, GPT, etc.
%
The Transformer architectures consist of blocks of so-called Attention layers, containing two large, Feed Forward (Linear) weight matrics~\cite{Attn2017}. 
In contrast to smaller pre-Activation maps arising in Cond2D layers, Attention matrices are significantly larger.
In general, we have found that they have larger $\alpha$ PL exponents.
Based on HT-SR Theory (in particular, the interpretation of values of $\alpha \sim 2$ as modeling systems with good correlations over many size scales~\cite{BouchaudPotters03, SornetteBook}), this suggests that these models fail to capture successfully many of the correlations in the data (relative to their size) and thus that they are substantially \emph{under}-trained.
%
More generally, these NLP models are quite different than the CV models we considered in Section~\ref{sxn:cv}, both in terms of their architecture (e.g., wheter there are convolutions) as well as how they are used (e.g., whether one uses prediction error or perplexity).
Thus, they provide a very different test for our metrics.

\michael{WHERE: Norm-base metrics perform reasonably well on good NLP models, but they have problems with less well-trained models.
PL-based metrics (the Log $\alpha$-Norm metric, $\log\Vert\mathbf{W}\Vert_{\alpha}^{\alpha}$, and the Weighted Alpha metric, $\hat\alpha =\alpha\log\lambda_{max} $) do even better.
We highlight how to use the theory to identify poorly trained models where the empirical norm metrics will perform poorly.}

\michael{WHERE: We show that this not only works for CV, so we look at NLP.
We now look in detail at the alpha metric, the spectral norm, and the new norm.
We compare good and bad models, and a series of increasingly big models
trained on big data sets.  We show why alpha works but spectral norm is not enough.
The layer Spectral Norm behaves unexpectedly, while $\alpha$-based metrics can give insight into how well trained a model is.}

\michael{WHERE: Say large $\alpha$ values don't mean much.}

\michael{WHERE: Highlight difference between $\alpha$ and $\hat{\alpha}$.}



\paragraph{OpenAI GPT Models.}

The OpenAI GPT and GPT2 models provide us with the opportunity to analyze two effects: training the same model with different data set sizes; and increasing sizes of both the data set and architectures.
These models have the remarkable ability to generate fake text that appears to the human to be real, and they have generated significant media attention because of the potential for their misuse.
For this reason, the original GPT model released by OpenAI was trained on on a deficient data set, rendering the model interesting but not fully functional.  
Later, OpenAI released a much improved model, GPT2 (small), which has the same architecture and number of layers as GPT, but which has been trained on a larger and better data set (and with other changes), making it remarkably good at generating (near) human-quality fake text.  
%
By comparing the poorly-trained GPT to the well-trained GPT2, we can indentify empirical indidcators for when a model has in fact been poorly-trained and thus may perform poorly when deployed.

\charles{More details here.}
The GPT models we analyze are deployed with the popular HuggingFace PyTorch library~\cite{XXX-XXX}.
GPT has 12 layers, with 4 Multi-head Attention Blocks, giving $48$ Layer Weight Matrices $\mathbf{W}$.
Each Block has 2 components, the Self Attention (attn) and the Projection (proj) matrics.  
The self-attention  matrices are larger, of dimension ($2304\times 768$) or ($3072\times 768$).
The projection layer concatenates the self-attention results into a vector (of dimension $768$).
This gives $50$ large matrices.
%
Because GPT and GPT are trained on different data sets, the initial Embedding matrices differ in shape.
\michael{Clarify that, which is second GPT.}
GPT  has an initial Token and Positional Embedding layers, of dimension $(40478\times 768)$ and $(512\times 768)$, respectively, whereas GPT2 has input Embeddings of shape $(50257\times 768)$ and $(1024\times 768)$, respectively. 
%
The OpenAI GPT2 (English) models are: \nred{gpt-small, gpt-medium, gpt-large, and gpt-xl}, \
having include $12, 24, 36, \text{and }48$ layers, respectively, with increasingly larger weight matrices.
The model card for GPT2 is published on github.\footnote{\url{https://github.com/openai/gpt-2/blob/master/model_card.md}}.

%        &  Num.  & $\Vert\mathbf{W}\Vert_{F}$ & $\Vert\mathbf{W}\Vert_{2}$ & $\hat{\alpha}$ & $\Vert\mathbf{X}\Vert^{\alpha}_{\alpha}$ \\
% Series & Models & Metric                     & Metric                     & Metric         & Metric                                   \\

\begin{table}[t]
\small
\begin{center}
%\begin{tabular}{|p{1in}|c|c|c|c|c|}
\begin{tabular}{|p{0.75in}|c|c|c|c|c|}
\hline
%   &    & Frobenius Norm & Spectral Norm & Weighted Alpha & Alpha Norm \\
% Series & \#Layers   & $\Vert\mathbf{W}\Vert_{F}$ & $\Vert\mathbf{W}\Vert_{\infty}$ & $\hat{\alpha}=\alpha\log\lambda_{max}$ & $\Vert\mathbf{X}\Vert^{\alpha}_{\alpha}$ \\
        &        & Log                   & Log                        & Weighted       & Log                                 \\
        & Num.   & $\Vert\cdot\Vert_{F}$ & $\Vert\cdot\Vert_{\infty}$ & $\hat{\alpha}$ & $\Vert\cdot\Vert^{\alpha}_{\alpha}$ \\
 Series & Layers & Metric                & Metric                     & Metric         & Metric                              \\
\hline
GPT & 49 & 1.64  & 1.72 & 7.01 & 7.28 \\
GPT (small) & 49 & 2.04  & 2.54& 9.62 & 9.87 \\
GPT2 medium & 98 & 2.08 & 2.58& 9.74 & 10.01 \\
GPT2 large & 146 & 1.85 & 1.99& 7.67 & 7.94 \\
GPT2 xl & 194 & 1.86 & 1.92 & 7.17 & 7.51 \\
\hline
\end{tabular}
\end{center}
\caption{Average value for the Average Log Norm metrics and the Weighted Alpha metric for pretrainnd OpenAI GPT and GPT2 models.  }
\label{table:nlp}
\end{table}


\paragraph{Coarse Analysis: Empirical Quality Metrics for GPT and GPT2.}

We have analyzed the four quality metrics described in Section~\ref{sxn:methods} for the OpenAI GPT and GPT2 pretrained models.
See Table \ref{table:nlp} for a summary of results.
We start by examining the PL exponents $\alpha$ for GPT and GPT2 (small).
Observe that all four metrics increase when going from GPT to GPT (small), i.e., they are smaller for the higher-quality model (higher quality since GPT was trained to better data), when the number of layers is held fixed.
Observe also that (with one minor exception involving the Log Frobenius norm metric) all four metrics decrease as one goes from GPT2 medium to GPT2 large to GPT2 xl, indicating that the larger models are indeed better than the smaller models.
%(The one minor exception is the Log Frobenius norm increasing from $1.85$ to $1.86$, for GPT2 large to GPT2 xl.)
\michael{We seem to have typos(s), so I'm not going to flesh out all conclusions I see yet.  The comments I just mad may be wrong, if GPT (small) is really GPT2 (small).  FIX TYPOS AND CLARIFY.}
These results are broadly consistent with the results in Section~\ref{sxn:cv} on series of CV models. 

Figure~\ref{fig:GPT-alpha-hist} shows the histogram (empirical density), for all layers, of $\alpha$ for GPT (blue) and this GPT2 (red) model.  
These two histograms are very different, with GPT2 having both a notably smaller mean $\alpha$, and far fewer unusually-large outlying $\alpha$ values.

Observe that these results indicate that $\alpha$ provides a good quality metric for comparing these two models, one of which which was designed to be of low quality.
The deficient GPT model has numerous weight matrices with unsually large fitted exponents (indicating that they are not HT at all).  
Indeed, we may expect that a poorly trained model will not exhibit consistent Heavy Tailed behavior in all layers.
On the other hand, as expected, the improved GPT2 model has, on average, smaller $\alpha$ than the older GPT, with all $\alpha\le6$.  
\michael{XXX.  FLESH OUT.}


\begin{figure}
    \centering
    \subfigure[PL exponent ($\alpha$)]{
        %\includegraphics[width=5cm]{img/GPT-alpha-hist.png}
        \includegraphics[width=4.0cm]{img/GPT-alpha-hist.png}
        \label{fig:GPT-alpha-hist}
       }
    %\qquad
    \subfigure[Log Spectral Norm]{
        %\includegraphics[width=5cm]{img/GPT-snorm-hist.png}
        \includegraphics[width=4.0cm]{img/GPT-snorm-hist.png}
        \label{fig:GPT-snorm-hist}
       }
   \caption{Comparison of PL exponents, $\alpha$, and log Spectral Norms, $\log\Vert\mathbf{W}\Vert_{\infty}$, for the OpenAI GPT and GPT2 (small) pretrained models.}
\label{fig:GPT-hist}
\end{figure}

However, as seen in Figure \ref{fig:GPT-snorm-hist},
 the poorly trained GPT model has many smaller (log) Spectral Norms $\log\Vert\mathbf{W}\Vert_{\infty}$
than the GPT2, which would be inconsistent with a belief that smaller Spectral Norm is always better.
Indeed, because there are so many anamously small $\Vert\mathbf{W}\Vert_{\infty}$,
it appears that the GPT model may be exhibiting a kind of rank collapse.
This is an extremely important observation because it demonstrates that while the Spectral Norm
may correlate well with predicted test error, it is not a good indicator of the overall quality of a model,
and using it as an empirical metric may give spurious results when applied to poorly trained
or otherwise deficient models.  


Note that Figure \ref{fig:GPT-snorm-hist} also shows a couple anomalously  large Spectral Norms.
From Figure \ref{fig:gpt-snorm-layer} (below), we see that these correspond to the first embedding layer(s).
These layers appear to have a normalization, and therefore a differenc scale.
For example, in GPT, most layers, the maximum eigenvalue $\lambda_{max}\sim\mathcal{O}(10-100)$,
but in the first embedding layer, the maximum is of order N (the number of words in the embedding), or
 $\lambda_{max}\sim\mathcal{O}(10^{5})$.  For GPT and GPT2, we layer all layers as-is (although one may to normalize
the first 2 layers by  $\mathbf{X}$ by $\frac{1}{N}$, or to treat them as an outlier).
Here, we do not include them in our computed average metrics in Table \ref{table:nlp},
and do not include them in the histogram plot in Figure \ref{fig:GPT-snorm-hist}.


\paragraph{Layer Analysis: Correlation Flow in GPT and GPT2.} 

also differs significantly between GPT and GPT2.
Figure \ref{fig:gpt-alpha-layer} plots $\alpha$ vs the depth (i.e. a layer id) for each model.

\charles{Discuss Spectral Norm, alpha-Norm}

\begin{figure}[t]
    \centering

    \subfigure[PL exponent ($\alpha$)]{
        %\includegraphics[width=4.5cm]{img/GPT-alpha-depth.png}
        \includegraphics[width=4.0cm]{img/GPT-alpha-depth.png}
        \label{fig:gpt-alpha-layer}
    }
    %\qquad
    \subfigure[Log Spectral Norm]{
        %\includegraphics[width=4.5cm]{img/GPT-snorm-depth.png}
        \includegraphics[width=4.0cm]{img/GPT-snorm-depth.png}
        \label{fig:gpt-snorm-layer}
    }
    %\qquad
    \subfigure[Log Alpha Norm]{
        %\includegraphics[width=4.5cm]{img/GPT-pnorm-depth.png}
        \includegraphics[width=4.0cm]{img/GPT-pnorm-depth.png}
        \label{fig:gpt-pnorm-layer}
    }
    \caption{Comparison of Correlation Flow and Spectral Norm for OpenAI GPT and GPT2   }
    \label{fig:gpt-alpha-layers}
\end{figure}




\paragraph{GPT2: small, medium, large, xl} 

We now look across the series of increasingly improving GPT2 models, examining both the PL exponent $\alpha$ as well as the various Log Norm metrics.
In general, as we move from small to xl, the histograms for both $\alpha$ exponents and the log Norm metrics shift from larger to smaller values. 
See Figure \ref{fig:gpt2-histograms}, which shows the histograms over the layer weight matrics
for fitted $\alpha$, and the log Spectral Norm
 $\log\Vert\mathbf{W}\Vert_{\infty}$  
and log Alpha Norm
 $\log\Vert\mathbf{W}\Vert_{\alpha}^{\alpha}$ 
metrics.

We see that the average $\alpha$ decreases with increasing model size, although the differences are less noticible between different GTP2 models than between the GPT and GPT2 models.
Unlike GPT, however, the Layer (log) Spectral Norms $\log\vert\mathbf{W}\Vert_{\infty}$ 
and (log) Alpha Norms $\log\vert\mathbf{W}\Vert_{\alpha}^{\alpha}$
behave more as expected for GPT2 layers, with the larger models  consistently having  smaller norms. 
\michael{Clarify.}
Likewise, Figure \ref{fig:gpt2-snorm-hist} shows decreasing average log Spectral Norms with the larger models.  
As we have seen in the trends of other well trained models.

We do notice, however, that while the peaks of the $\alpha$ is getting smaller, 
towards $2.0$,
the tails of the distribution shifts right, with larger GPT2 models having more
usually large $\alpha$.  
We suspect this indicates that these larger GPT2 models are over-parameterized/under-optimized and 
that they have capacity to support datasets even larger than the recent XL $1.5B$ release~\cite{gpt2-xl}.

\begin{figure}[t]
    \centering

    \subfigure[PL exponent ($\alpha$)]{
        %\includegraphics[width=4.5cm]{img/GPT2_all_alpha_hist.png}
        \includegraphics[width=4.0cm]{img/GPT2_all_alpha_hist.png}
        \label{fig:gpt2-alpha-hist}
    }
    %\qquad
    \subfigure[Log Spectral Norm]{
        %\includegraphics[width=4.5cm]{img/GPT2*_all_spectralnorm_hist.png}
        \includegraphics[width=4.0cm]{img/GPT2*_all_spectralnorm_hist.png}
        \label{fig:gpt2-snorm-hist}
    }
    %\qquad
    \subfigure[Log Alpha Norm]{
        %\includegraphics[width=4.5cm]{img/GPT2_all_logpnorm_hist.png}
        \includegraphics[width=4.0cm]{img/GPT2_all_logpnorm_hist.png}
        \label{fig:gpt2-pnorm-hist}
    }
    \caption{PL exponents ($\alpha$), Log Spectral norm, and Log Alpha norm for different size models in the GPT2 architecture series.  (The Log Spectral norm histogram omits the first 2 layers, corresponding to the embedding layer, as these are normalized differently and have anamolously large values.)}
    \label{fig:gpt2-histograms}
\end{figure}




